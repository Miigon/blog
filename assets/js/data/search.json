[ { "title": "随笔：Golang 循环变量引用问题以及官方语义修复", "url": "/posts/golang-loop-var-reference-problems-and-semantic-fix/", "categories": "Language, Golang", "tags": "discussion", "date": "2023-03-15 17:05:00 +0800", "snippet": "这篇文章谈一个已经在 Golang 中存在多年的，几乎每一个新手都要被坑一遍的设计：引用捕获了循环变量，且逃逸出循环迭代范围而造成的逻辑错误。以及重点是讨论了目前对这个常见问题的解决办法的探索（静态分析存在的不足，以及2022年10月 Golang 官方提出的直接更改 for 循环变量语义，从语言设计上根本地消除这个问题的 proposal）background这个问题是一个 Golang 从很早版本就一直存在的设计选择造成的。简单地讲就是 for 循环中，由于 func 捕获，或者显式/隐式的取引用，对循环变量产生了引用并且这个引用逃逸出了当前循环迭代（iteration）的生命周期范围。而由于 Golang 一开始决定将将循环变量（i、k、v）的生命周期定义为整个循环，而不是每个迭代都有新一份的循环变量，导致了每一轮迭代产生的引用实际上都指向同一个值，而不是指向每一轮各自对应的值。常见的场景有以下： function literal captures loop variablearr := []int{1,2,3,4,5}for _, v := range arr {\tgo func() { fmt.Println(v) }() // v is a implicit reference}// prints 5 5 5 5 5 implicit reference due to receiver mismatch（更难发现）type MyInt intfunc (mi *MyInt) Show() {\tfmt.Println(*mi)}func main() {\tms := []MyInt{1, 2, 3, 4, 5}\tfor _, m := range ms {\t\tgo m.Show()\t\t// implicitly converted to `go (&amp;m).Show()`\t\t// thus creating a reference to loop variable.\t\t// but you would never know this without more context.\t}\ttime.Sleep(100 * time.Millisecond)}// prints 5 5 5 5 5 appending &amp;v to a slicearr := []int{1,2,3,4,5}for _, v := range arr {\tarr2 = append(arr2, &amp;v) // all new elements &amp;v are the same.}// arr2 == {v_arr, v_arr, v_arr, v_arr, v_arr}// *v_arr == 5golang 的循环变量是 per loop 的，而不是 per iteration 的。如果对循环变量产生了引用（比如闭包 capture，或者取指针），不同次迭代取到的指针都是同一个。如果这个指针/引用被逃逸出了一次迭代的范围内（比如 append 到了一个数组里，或者被go/defer后面的闭包capture了），因为所有 iteration 里取到的指针都是同一个，指向的对象也都会是同一个（最后一轮iteration的结果）。workaround \tfor _, a := range alarms {+\t\ta := a \t\tgo a.Monitor(b) \t}一个 workaround 是加一句创建同名新变量 shadow 掉原来的循环变量，强制拷贝变量，把 per loop 的循环变量变成 per iteration的。问题是很多时候很难知道某个循环是否需要写这么一行拷贝，导致很容易因为遗漏而产生bug。另一个极端是有的开发者因为担心遗漏，选择过度矫正，把所有的循环都写上这一句拷贝，使得代码可读性降低。vet? static analysis?go vet 或其他 static analysis 方案虽然能帮助找到很明显的错误场景，但是由于静态分析并不能完全100%理解程序逻辑，在没有 proof 某个循环变量指针一定会超出 iteration 范围的前提下，会出现 false positive。 并不能单用逃逸分析，根据引用是否逃逸来判定是否会出问题。 例子：循环体和 goroutine 之间可能使用了 waitgroup 进行了同步，从而使得虽然循环变量引用逃逸到了 goroutine 中，但是每一个 goroutine 的执行时机实际上都不会超过对应 iteration 的生命周期而 go vet 中的 loopclosure 则是采取了保守的方案，只有十足把握是错误情况才报告，会有false negative。静态分析的问题是分析无法透过一些运行时功能，比如 interface 方法，比如 reflection。只能理解相对简单的代码。sematics fix问题的本质是 golang 设计之初，决定将循环变量设定为 per loop 的而不是 per iteration 的。想要根除这个问题，需要在语义层面修复。即将循环变量设定为 per iteration。Russ Cox（rsc）在2022年10月的时候，重新提出了这个话题：https://github.com/golang/go/discussions/56010 A decade of experience shows the cost of the current semantics …… Since then, I suspect every Go programmer in the world has made this mistake in one program or another. I certainly have done it repeatedly over the past decade, despite being the one who argued for the current semantics and then implemented them. (Sorry!)The current cures for this problem are worse than the disease.rsc 提到了，在Golang进入Go1版本之前就已经讨论过这个问题，当时的结论是：虽然很烦但是问题没有大到要改。但是过去这个 decade 已经展示出来当前语义设计的后果。rsc 本人也常常被这个问题坑到。更重要的是，目前对这个问题的解决方法，比问题本身还糟糕。 \tfor _, informer := range c.informerMap {+\t\tinformer := informer \t\tgo informer.Run(stopCh) \t} \tfor _, a := range alarms {+\t\ta := a \t\tgo a.Monitor(b) \t}光看这两份代码，都有上述提到的 workaround。但实际上一个是真正的 bugfix，另一个是没有作用的。在没有上下文的前提下，没有任何办法区分。实际上其中一个是 interface 类型，创建拷贝变量并没有任何效果。另一个则是 struct 类型调用了 pointer receiver 方法，是真正的 bugfix。并且，还有一些代码，不论上下文是什么，添加的拷贝都是没必要的拷贝（没有任何隐式引用循环变量的可能）： \tfor _, scheme := range artifact.Schemes {+\t\tscheme := scheme \t\tRuntime.artifactByScheme[scheme.ID] = id \t\tRuntime.schemesByID[scheme.ID] = scheme \t}像这样的迷惑性和二义性，恰恰违反了 Go 可读的设计目标。 This kind of confusion and ambiguity is the exact opposite of the readability we are aiming for in Go.之前 Golang 社区尝试通过文档和工具的方式，尝试防止用户因为这个语义而写出 bug 代码。但是实际实践中已经证明了，这种方式并不是非常有效，即使是语言的老手也经常不经意写出问题代码。the official fix在 https://github.com/golang/go/discussions/56010 中提到的，从根本上彻底解决这个问题的方案是，将循环变量（三段式循环以及range循环）改为 per iteration。概念上等同于，在每个 iteration 开始时，将原本 per loop 的循环变量拷贝一份，并且在每个 iteration 结束时拷贝回去。这样，每一轮 iteration 取到的地址都会是不同的地址。而这个 sematic change 会通过 go.mod 来判断是否启用，旧的项目的行为照旧完全不变。effects of migrating to new semanticsGoogle’s Go Tests 目前（2023-03）这个提案还没有被正式确定引入 Golang 中。这里提到的结果都是 Golang 团队测试/实验的结果。 原文：Changing the semantics is usually a no-op, and when it’s not, it fixes buggy code far more often than it breaks correct code大多数情况下，这个语义变更没有影响。在有影响的情况下，常常产生的影响都是修复了有bug的代码，而不是让更多代码出问题。他们（rsc）测试了 Google 内所有 Go 测试的一个子集。在变更语义后的新失败率大约是1/2000，但是几乎所有失败的测试都是之前没有发现的真实的bug。而原本正确的代码被这个更改影响坏的比率是1/50000。10w 个测试（包含 130w 个 for loop） 中，只有 58 个测试出现了失败。其中 36 个（62%）测试是由于和 t.Parallel 错误的交互而导致的不正确的无效测试，而在 for 循环变量语义更改后反而更正了这些测试了（指的是：测试失败的原因，是原本错误的测试在语义更改后变得正确了，然后测试从无效变成了有效，并且帮助找到了代码里确实存在的 bug，所以报告了失败）。第二大常见的错误是每一轮迭代将 &amp;v append 给了一个 slice，从而产生一个有 N 个相同指针的 slice。在这 10w 个测试的 58 个失败的测试中，只找到了 2 个是真的依赖了 per-loop 循环变量的语义，并且真的因为语义变更而导致失败的： One involved a handler registered using once.Do that needed access to the current iteration’s values on each invocation. The other involved low-level code running in a context when allocation is disallowed, and the variable escaped the loop (but not the function), so that the old semantics did not allocate while the new semantics did.两个都非常地容易修复。perspective: csharp’s migration to per-iteration loop varsC# 团队中的 @jaredpar （负责处理 customer feedback 的主要人物）提供了视角：C#5 的时候也做过类似的更改，将 foreach 的循环变量从 per-loop 改为 per-iteration。当时由于 C# 没有类似 go.mod 的版本指定机制，所以唯一的选项就是要么无条件地改掉并且 break 一些东西，要么永远忍受现状。循环变量的生命周期问题，在语言引入 lambda 表达式之后变成了一个痛点（闭包捕获）。随着语言对 lambda 表达式的使用越来越广泛，问题也越来越明显。严重到 C# 团队决定，无差别地全盘修改是值得的。相比给每个新的用户都解释一遍这个非常 tricky 的行为，相比之下给（但愿）数量较少的受影响的客户解释显得更容易一些。最终的结果是：受这个更改所影响的客户的数量，比想象中的少。并且对于那些被影响到的客户，相应都是积极的，并且都接受了所提议的代码修复。C# 作出这个更改已经10年了，jaredpar 的原话：”I’m honestly struggling to remember the last time I worked with a customer hitting this.“ （this 指更改语义造成的代码 break）more本文摘选以及翻译/总结自 Golang 官方仓库 Discussion 中对于该话题的帖子 https://github.com/golang/go/discussions/56010 。完整的讨论请前往原链接。（原讨论帖 17 天内收到了 241条回复，由于社区反馈已经比较充足，新回复基本上也是旧回复的重复，原讨论帖已被 lock）以下是原讨论帖的大致内容： 原 discussion 下方的评论几乎一致地支持这个新语义变更的发生。即使是多年的 Golang 语言使用者也承认会被这个语义坑到。一致同意 per-loop 的循环变量语义是 Golang 的一个 foot gun，是问题和 bug 的一个持续来源。 主要的讨论点在于CI/工具链和现有代码/依赖如何平滑迁移到新的语义上，以及是否有依赖旧语义才能正确工作的合法代码会被break（不多。只找到了极其稀有的 case 是依赖 per-loop 循环变量的旧行为的，而且这个 case 本身代码就不是特别清晰）。 另外一个问题就是 go generate 必须生成新旧版本都能正确编译的代码，不过同样的，依赖旧语义的代码极其稀有，大多数代码生成器不会被 break。 一部分用户对把三段式 for loop （for i := 1; i &lt; n; i++）中的循环变量也修改成 per iteration 这个提议提出了质疑，主要理由是会和其他语言（主要是C以及一众类C语言）的行为不一致，其他语言背景的用户迁移过来也会被坑到。（C# 迁移到 per-iteration 循环变量作用域的时候就只迁移了 foreach，而没更改三段式 for loop 的循环变量作用域） 一些 practical 的问题：如何在用户升级的时候告知用户这一变动？如何检测升级前后是否会 break 用户的具体代码？这个变更应该是在 minor 版本发布还是在 major 版本（Go2）中发布？" }, { "title": "[随笔]文件系统上存储哈希对象：哈希算法以及目录结构对性能的影响", "url": "/posts/storing-hash-objects-on-filesystem/", "categories": "Misc, Unix", "tags": "filesystem, hash", "date": "2022-12-30 17:16:00 +0800", "snippet": " 从文件系统的实现原理角度讨论 /77/e1/77e1cccccc... 模式的 hash object 命名的优点以及必要/不必要性，以及算法选择。原贴这是我在 0xffff 论坛的回帖的备档：https://0xffff.one/d/1395-wen-jian-xi-tong-zuo-wei-huan-cun一个常见的业务场景，需要实现一个 Key-Value Cache Storage，除了 Redis 等外，有一个方向是用设备本身的文件系统来落地。大概操作是，用 Key 生成 Path，再基于这个 Path 去读写文件，再将结果返回给业务，这个操作通常是 Key 经过 hash 函数算出一个值 取这个值的前四位，做一个二级目录，如 77e1ba46ee3a2b2d1558d7c5d07c4c0caa46c7bf，生成一个 77/e1/77e1ba46ee3a2b2d1558d7c5d07c4c0caa46c7bf 的路径 基于生成的路径读写有俩个考虑的点： hash 函数如何选择（sha256？还是古老的 sha1 / md5） 路径的划分，大量 key 下，对性能的影响哈希算法哈希算法，作为一个将大数据映射到一个固定范围内的值的算法，有几个主要的因素要考虑： 速度 碰撞概率，在期望的数据集上，计算出来的哈希分布是否均匀 安全性，从某个已知哈希，恶意构建哈希值一致的数据的难度不同用途的哈希算法当然用于不同用途的哈希，权衡的点也不同： Cryptographic Hash：用于密码学用途，比如密码哈希，碰撞概率和安全性要求都比较高。逆向构造难度大，没有已知快速（within reasonable time）破解的方法，速度相对要求不高，甚至要求速度不能太快，以抵御暴力攻击。常见：SHA-2（SHA256、SHA512）、SHA-3。 Checksum/Digest/Non-cryptographic Hash：用于校验消息传输的完整度，或者大致打乱数据，不要求十分随机，也不要求抗恶意攻击。一般都是优化计算速度。比较常见：取模、TCP/UDP的补码和checksum、CRC32、MD4、MD5、SHA1、xxhash、murmurhash。其中MD4、MD5、SHA1属于比较典型的原本作为 Cryptographic Hash 被使用，但是由于计算机算力发展/捷径攻击方式被发现，而被认为不再安全，从而退化到只被作为 Checksum 使用。一般来说，Checksum 使用的 Hash 算法的速度要比 Cryptographic Hash 要快。当然，使用 Cryptographic Hash 来算 Checksum 是完全可行的（虽然 overkill）。而反过来使用 Checksum 用的算法去当做 Cryptographic Hash 用就是认为是不安全的了。key hash 场景下的算法选择KV存储场景下 key 的哈希算法，属于比较介于两种用途之间，既不是完全不担心出现碰撞，但也不需要很高程度的密码学安全性。如果 key 是用户自定义的输入的话，可能需要考虑稍微好一点点的防碰撞特性，可以考虑使用 SHA1。如果想要追求极致的性能，而不担心 hash collision 或者有额外的机制用来处理 hash collision 的话，可以考虑用 xxhash 或 murmurhash 等性能较高的非密码学 hash 算法。当然，不同 hash 算法在不同的数据特性下表现也不同，数据内容/长短都可能会影响 hash 算法的相对快慢。特别是 kv 数据的 key 一般较短，需要对 key 常见的字符组成以及长度进行具体测试才能知道哪一个更快。大文件上跑得飞快的算法不一定在几个字节的 key 上也能打赢其他算法。（当然如果哈希计算不是瓶颈，就无所谓了，KV存储场景下估计存储才是瓶颈）碰撞概率关于 SHA1，以及其他几种常见的 non-cryptographic hash 算法的碰撞概率，可以参考：https://crypto.stackexchange.com/a/2600https://softwareengineering.stackexchange.com/a/145633/414841https://stackoverflow.com/questions/9392365/how-would-git-handle-a-sha-1-collision-on-a-blob如果这对你的信心能有任何帮助，基本上： 对于 SHA-1 来说，只要不是天文数字数量的 key，一台服务器生命周期内出现碰撞的概率比主板或硬盘烧掉的概率还小得多得多。 git 也是使用 SHA-1 算法，并且并没有对哈希碰撞做特殊处理，因为它实在是太稀有了。 如下直观感受（https://stackoverflow.com/a/9392518/7509248）： Here’s an example to give you an idea of what it would take to get a SHA-1 collision. If all 6.5 billion humans on Earth were programming, and every second, each one was producing code that was the equivalent of the entire Linux kernel history (1 million Git objects) and pushing it into one enormous Git repository, it would take 5 years until that repository contained enough objects to have a 50% probability of a single SHA-1 object collision. A higher probability exists that every member of your programming team will be attacked and killed by wolves in unrelated incidents on the same night. 所以，其实不做专门的碰撞处理也没有太大影响。当然如果 key 是用户输入，并且哈希算法比较弱的话，可能需要考虑用户恶意攻击的可能性。文件存储 这里假设 Linux 系统，并且假设 ext4 文件系统这里解释为何会出现 /77/e1/77e1ba46ee3a2b2d1558d7c5d07c4c0caa46c7bf 这样的多级目录的索引方案，以及我们如何需要（或不需要）这样做。背景知识1.树状文件系统ext 文件系统中（暂时忽略 ext3 加入的 htree，后面会提到），整个文件系统的结构是一颗 B Tree，每一个目录实际上也是一个特殊的文件。从根目录开始，每个目录文件的块数据，记录着该目录下直接包含（只包括直接相邻的一层，不包括子目录中间接包含的文件）的所有文件的索引信息（每一个称为一个 entry，内容包括文件名、inode号）。2.块与块大小文件系统上的文件的数据，并不是完全连续存储的，而是以块为单位存储。块是一个在单个文件系统内大小固定的最小空间分配单元，即即使文件只有1个字节，也需要占用至少一个块的空间来存储。而对于大文件，可以通过为该文件分配多个块，并将这些块的块编号存储在inode 中（同样，这里暂时忽略 ext4 extent tree 的细节，不影响后面讨论）。而对于 ext4 文件系统，块的大小一般是4KiB （可自定义），即小于等于 4KiB 大小的文件就可以只用一个块来存储实际数据，而大于 4KiB 大小的文件则要用多个块。对于线性目录结构，一个目录 entry 的大小为 4+2+2+n 字节，n=文件名长度。3.磁盘与内存与CPU对于文件系统来说，磁盘的访问速度在大多数情况下几乎一直是瓶颈（相比内存和 CPU 操作），一般磁盘和内存的速度差距都是很多倍的（20GB/s vs 100MB/s），延迟更是差了好几个数量级。所以对于文件系统来说，减少磁盘访问次数是很重要的优化手段。4.每个目录块能存多少个 key？根据背景知识2，假设我们使用 sha1 算法进行 key 哈希，并且 key 使用 hex 文本形式作为文件名，则每个文件的文件名长度为 40 字节，存到目录中，一个 entry 就需要占用 4+2+2+40=48 个字节，即一个线性目录块可以存 floor(4096/48)=85 个 key。方案1：所有 key 文件存储在同个目录下由背景知识1可知：（忽略 htree）在某个目录下查找某个文件，相当于要遍历这个目录的所有 entry，直到找到匹配的文件名，即 O(n) 操作，n=目录内的直接文件数。这意味着，假如我们直接把所有的哈希 key 作为文件名，全部塞到同一个目录 foobar 下，访问的时候（最坏情况下）我们可能要遍历整个 foobar 目录的所有目录 entry 才能找到我们想要的 key。性能要评估这个方案的性能，根据背景知识3，我们只需要考虑整个查询操作读入了几个块，不考虑在这个块已经读入内存后CPU运算所需要的时间（因为相比读入时间来说几乎可以忽略）。（不考虑 htree，只考虑线性目录）根据背景知识4，这个方案在 85 个 key 的时候恒定只需要读入1个目录块就可以完成查询，但是由于查询的本质是遍历目录，需要读入的目录块的数量是随着 key 的数量增加而线性增加的：当文件数量为 1000 的时候，最坏情况就已经需要读入 ceil(1000/85)=12 个块了；当文件数量为 1w，最坏需要读入 ceil(10000/85)=118 个块，可以看到文件的数量一旦多起来，性能下降得特别快。 一个粗略数据提供prespective：假设是磁盘，一次读取延迟10ms的话，118个块就已经要1秒多了。注意注意注意：这里得出的所有结论，都是【忽略 htree 的存在】的，可以理解为 ext2 及之前文件系统的做法，不代表 ext4 的表现！请阅读下方 htree 部分。这里提供这个方案只是为了作为分层目录方案的背景，以及性能比较的基准参考。方案2：分多层目录，最内层目录存储文件使用提到的，取哈希文本前两字节作为目录名创建目录，将开头两字节相同的所有 key 都放到这个目录中的方法，即 /77/77e1ba46ee3a2b2d1558d7c5d07c4c0caa46c7bf 的形式，先考虑只加一层：根据背景知识2，计算第一层（即根目录）下的每个子目录的 entry 大小，得到每个目录 entry 需要 4+2+2+2=10 字节，则第一层目录可以存储 4096/10 = 409 个子目录，而我们取的哈希hex 文本前两字节的取值最多只有 256 个，所以第一层目录（根目录）是恒定只需要一个目录块的，查询第一层的时候不会跨多个块。而第二层目录，由于已经在第一层里面分过一次类了，每个目录的文件数量会明显下降，假设 key 均匀分布的话（一个好的哈希算法应该尽量保证这一点，所以这个假设在现实中通常也是成立的），每个二级目录只需要存储 n/256 个 key 即可。性能已知查询时，第一层目录恒定会需要 1 个块的访问，同样假设 key 数量为 1w，第二层平均每个目录存储 1w/256 = 39 个 key，远远小于背景知识4中算得的一个目录块能存储 85 个 key 的上限，所以第二层的任意目录也能在一个块内完成查询，即 key 总数量即使为 1w，也只需要稳定读取 2 个块就能完成一次 key 查询。（相比方案1的 118 个块）当然，如果 key 的数量继续增加，第二层目录也可能超过 1 个块，导致在第二级查找的时候也出现耗时的多块遍历。解决方法也很简单：加第三第四层，每次都相当于以每次访问稳定多 1 次块读取为代价，将 key 的承载能力扩大 256 倍。一些粗略的数字，用作 rule of thumb，对于一次查询： 1 层目录，85 个 key 以内稳定 1 次块读取，超过则开始出现需要 2 次块读取的 key，170 个 key 以上则所有 key 退化为平均 3 次块读取。 2 层目录，21760 个 key 以内稳定 2 次块读取，超过则开始出现需要 3 次块读取的 key，43520 个 key 以上则所有 key 退化为平均 3 次块读取。 3 层目录，557w 个 key 以内稳定 3 次块读取，超过则开始出现需要 4 次块读取的 key，1114w 个 key 以上则所有 key 退化为平均 4 次块读取。 n 层目录，a = 85*(256^(n-1)) 个 key 以内稳定 n 次块读取，超过则开始出现需要 (n+1) 次块读取的 key，2*a 个 key 以上则所有key 退化为平均 (n+1) 次块读取。 以此类推…本质上就是，在 n 层下，a = 85*(256^(n-1)) 个 key 以内是树查找，而超过 a = 85*(256^(n-1)) 个 key 则退化为顺序查找。根据实际场景的数量级，选择合适的层数就行了。如果需要适应宽范围的 key 数量，也可以支持不同的目录使用不同的层数，当目录过大的时候自动增加层数。当然设计上就会复杂许多。Elephant in the Room.ext3、ext4 的 htree？前面我们都假设了，文件系统中的目录是 Linear Directory 线性目录，即目录下的所有文件，以一个数组的形式存储在（一个或多个）目录块下。而每一次在目录中查找文件，都需要遍历目录的（最坏情况下所有）目录块才能找到目标文件对应的 entry。但是，文件系统的设计者们，也很早就意识到了这种方案的局限性，即在单个目录下文件过多的时候性能下降明显。所以早在 ext3，就加入了一种新的目录类型：Hash Tree Directory 哈希树目录 。原理实际上就和我们方案2做的事情几乎一模一样：对于需要访问的文件名，计算一个哈希（没错，文件系统内部其实又算了一次哈希）。根目录块中实际上存储的是一颗 htree 的根节点（以 hash 为 key 的 btree 的意思），也是同样的使用 hash 去查询第一层目录块，得到第二层的块号，如果读取第二层块，发现不是叶子块（即这个块内的结构，不是传统的线性目录，而是仍然是 htree directory）的话，就继续用 hash 查询这第二层目录块，得到第三层的块号，如此下去直到最后找到一个线性目录块，想要找的文件就在这个线性目录块中。（注意到这些块都属于同一个目录文件，而不是不同目录。该目录的所有目录块一同组成了一颗 btree）也就是，我们的方案2实际上是 htree 在软件层面的一个复刻版本，而且很有可能是更拙劣的复刻版本。 因为多级目录下，实际上每一级目录中，”00”到”ff”这256个目录的 entry，并没有完全填满一个目录块，存在空间浪费。并且 htree 的每一段哈希段到下一级目录块的映射 entry 不需要消耗完整的一个目录文件，只是记录一个简单的 &lt;hash, blockno&gt; 二元组就可以了，更节约空间。（8字节 vs 10字节+一个inode块256字节）。当然，还有另一个显而易见的好处，就是 htree 是操作系统提供的功能，对用户程序完全透明，代码上只需要把所有文件都丢到同一个文件夹中就行。而且自带了前面所提到的“支持不同的目录使用不同的层数”的能力，更灵活。htree 也有一定局限性，比如其使用的文件名 hash 算法（Legacy、半MD4、TEA）长度只有 64bit，而每一层哈希使用 32bit 作为 hash code，就限制了 btree 树高最多只能有 2 层哈希转跳，第三层必须是叶子节点（即和方案2中的三层目录方案的跳转次数相同）。当然，htree 每一层能存储的哈希桶数量也比我们自己实现的多级目录要多，根据这个 Source，一个目录块能存储的 dx_entry （8字节长的 &lt;hash, blockno&gt; 二元组）的数量是 508个。即 branching factor 是 508 而不是我们自己实现的方案的 256。性能假设「一个目录块能存储的 dx_entry 的数量是 508个」的信息准确（个人未进行验证，请阅读上述来源），可以估算出，2层转跳的 htree，在小于 85*508*508 = 2193w 个 key 文件的情况下最坏读取 3 个块。 相比方案2三层目录时，只能在 557w 个 key 以内保证稳定 3 个块读取结论多层目录的做法，是在比较旧的文件系统上，用于解决单个目录下文件数量过多时的访问效率低问题，而出现的用户软件级 workaround。实际上是将多级目录当做 btree 来使用，每一级目录就是 btree 的一层，而每一个具体目录就是一个 btree 节点。而在新的文件系统下（ext3 以上），文件系统将这个功能（目录内使用 btree 索引文件）直接集成到了目录的实现中（即 htree），所以用户程序再实现一遍多级目录的意义并不大。htree 即方便，性能和承载能力也强，那如何创建一个使用 htree 索引文件的目录呢？答案是：什么都不用干 :D，在任何现代的 ext3/ext4 文件系统实现上（Linux kernel 2.6.23 以上），htree 是默认打开的。只要目录的文件 entry 数量超过了一个目录块可以存储的范畴，就会直接将目录切换到 htree 的形式对目录文件进行树状索引。NTFS 上的目录内索引也有类似的机制，但是使用的是 btree 而不是 htree（即 key 是文件名本身，而不是文件名的哈希）。所以，实际上没有必要再手动实现一遍方案2，除非需要支持一些很老的文件系统（ext2），或者文件数量超过了 htree 三层树的承受能力（2193w个以上）。否则，直接把所有文件丢到同一个目录下即可。" }, { "title": "Linux 是否有 zombie thread？从glibc和内核源码探究", "url": "/posts/does-linux-has-zombie-thread/", "categories": "Linux", "tags": "Chinese, linux", "date": "2022-11-23 17:47:00 +0800", "snippet": "系统编程课上遇到的一个问题：Linux下，如果一个 pthread_create 创建的线程没有被 pthread_join 回收，是否会和僵尸进程一样，产生“僵尸线程”，并一直占用一个 pid/tid？猜想僵尸进程对于进程与子进程来说，如果子进程退出了，但是父进程不对子进程进行 reap （即使用 wait/waitpid 对子进程进行回收），则子进程的 PCB（内核中的 task_struct）依然会保留，用于记录返回状态直到父进程获取，并且状态将被设置成 ZOMBIE，即产生“僵尸线程”。#include &lt;unistd.h&gt;int main(){ if(fork() == 0) { return 0; // child exits immediately } while(1); // parent loops}运行后可以看到子进程 607727 的状态为 Zombie，并且在最后有 &lt;defunct&gt; 标志。USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMANDmiigon 607726 97.5 0.0 2640 988 pts/0 R+ 21:55 0:28 ./childmiigon 607727 0.0 0.0 0 0 pts/0 Z+ 21:55 0:00 [child] &lt;defunct&gt;僵尸线程？#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;void *child_thread(void *args) { // child thread exits immediately. return (void*)666;}int main() { pthread_t t1; pthread_create(&amp;t1, NULL, child_thread, NULL); while(1); // parent and child never join. // pthread_join(t1, NULL);}子线程启动后马上返回，而父线程无限循环，并且不 join 子线程，不检查其返回状态。Linux 内核中（至少在调度上）并不区分线程和进程，都视为 task，故合理猜想：可能这里的 pthread_create 和 pthread_join 也可以类比 fork 和 wait，如果一个线程被创建后，不进行 pthread_join，那在子线程执行结束后，可能子线程也会进入 Zombie 状态，直至被父线程回收？（猜想）验证对上述猜想进行验证，编译运行上述线程代码：$ gcc pt.c -o pt -lpthread -g$ ./pt如果我们的猜想正确，当查看 pt 的所有线程的时候，理论上应该可以看到一个主线程，还有一个 defunct 状态的子线程 task。使用 ps 检查 pt 的所有线程：$ ps -T -C pt PID SPID TTY TIME CMD 610281 610281 pts/1 00:00:09 pt发现只有一个主线程（PID == SPID），没有观察到 defunct 状态的子线程，子线程退出后虽然主线程没有 pthread_join 读取其返回值，但是子线程 pid/tid 依然被回收了，并没有进入僵尸状态。验证一下如果把子线程函数换成死循环，运行后可以观察到子线程存在，说明测试方法没有问题，排除子线程没有创建成功或者观测方法有误的可能性：void *child_thread(void *args) { while(1);}$ ps -T -C pt PID SPID TTY TIME CMD 610762 610762 pts/1 00:00:05 pt 610762 610763 pts/1 00:00:05 pt说明我们的猜想是不准确的，并没有观察到子线程退出后变为僵尸状态。探究由于已知在 Linux 上，创建线程和创建进程实际上走的是同一套机制，本质上都是 fork/clone，只是调用者指定的资源共享程度不同，所以差异出现的诱因只能是位于 fork/clone 的调用者，即位于 pthread 的代码中。pthread 在 Linux 上一般是由 libc 实现的，最常见的 libc 是 glibc（另一个 Linux 上常用的 libc 的例子是 musl，更轻量，不展开）。glibc 的 pthread 实现叫做 NPTL（替换掉之前的远古实现叫 LinuxThreads，也不展开），可以在 https://codebrowser.dev/glibc/glibc/nptl/ 很方便地在线浏览相关代码。 本文环境 ubuntuserver 22.04.1 + linux5.15.0 + glibc2.35；所有源代码文件以这些版本为准。如果发现 glibc/NPTL 部分代码的锁进很乱，那是由于原来的代码就是这么锁进的，不是文章格式化错误。线程等待 pthread_join()首先检查 pthread_join 的源码，因为根据我们的猜想，如果是会产生“僵尸线程”的话，pthread_join 要回收这个“僵尸线程”，必然要调用 wait/waitpid 系的系统调用。https://codebrowser.dev/glibc/glibc/nptl/pthread_join.c.htmlhttps://codebrowser.dev/glibc/glibc/nptl/pthread_join_common.c.html// pthread_join.c:21int___pthread_join (pthread_t threadid, void **thread_return){ return __pthread_clockjoin_ex (threadid, thread_return, 0 /* Ignored */,\t\t\t\t NULL, true);}核心部分：// pthread_join_common.c:35int__pthread_clockjoin_ex (pthread_t threadid, void **thread_return, clockid_t clockid, const struct __timespec64 *abstime, bool block){ struct pthread *pd = (struct pthread *) threadid; // ...... if (block) // true {\t // 等待线程执行完成 pthread_cleanup_push (cleanup, &amp;pd-&gt;joinid); pid_t tid; while ((tid = atomic_load_acquire (&amp;pd-&gt;tid)) != 0) // 获取锁 {\t int ret = __futex_abstimed_wait_cancelable64 ( // 通过等待一个 futex 来等待线程执行完成，只是 futex syscall 的封装，内部并没有调用 wait/waitpid\t (unsigned int *) &amp;pd-&gt;tid, tid, clockid, abstime, LLL_SHARED);\t if (ret == ETIMEDOUT || ret == EOVERFLOW)\t {\t result = ret;\t break;\t }\t} pthread_cleanup_pop (0); } void *pd_result = pd-&gt;result; // 获取线程的返回值，说明线程已经执行完成 if (__glibc_likely (result == 0)) // 等待成功 { /* We mark the thread as terminated and as joined. */ pd-&gt;tid = -1; /* Store the return value if the caller is interested. */ if (thread_return != NULL)\t*thread_return = pd_result; /* Free the TCB. */ __nptl_free_tcb (pd); // 释放 TCB，即 pthread 结构体 } // ......}通过 JOIN 的这部分关键代码，可以推测出这几个重要信息： glibc 上 pthread_join 等待子线程完成，并不是通过传统的 wait/waitpid 实现的，而是由 pthread 自己再维护了一个 futex （在这里作为「线程执行完毕」的条件变量 condition variable），通过等待这个 futex 实现。 通过 __nptl_free_tcb(pd) 可以知道，所谓的 “TCB” 这个概念实际上就是 pthread 结构体本身（pthread_t 是指向其的指针），并且是存储在用户态的，由 glibc/nptl 管理，而不是在内核态管理。 pthread_join 只负责释放用户态 pthread 结构体（pd），而和释放线程在内核中占用的资源没有关系。综合「pthread_join 不负责回收（reap）内核态线程」以及「观察到子线程在执行完成后，在主线程什么都没有做的情况下自己消失了」这两个信息，进一步猜测子线程是退出后被内核自动 reap 掉了。但是按照正常进程来说，除非是父进程设置了 signal(SIGCHLD, SIG_IGN);，否则操作系统是不会自动 reap 掉子进程的，假设内核不区分进程和线程，对线程而言应该也是这个行为（需要等待父进程 reap，否则就处于 ZOMBIE 状态）才对。由此猜测有可能是两种可能性中的一种： 内核可能对线程 task 有一定的特殊照顾/特殊处理，使得线程的 task 会在退出时自动 reap，而进程则等待父进程回收。 也有一种可能性是 pthread 自己在子线程执行末尾做了特殊处理，让操作系统 reap 掉自己（真的可能做到吗？）后面的内容和探究都是围绕尝试检验这两个猜想展开的。线程创建 pthread_create()由于已知线程 task 不是由 pthread_join 回收的，必然是内核或者 pthread 在什么其他地方进行了回收，故追踪整个线程 pthread 从创建开始的生命周期：https://codebrowser.dev/glibc/glibc/nptl/pthread_create.c.html// pthread_create.c:619int__pthread_create_2_1 (pthread_t *newthread, const pthread_attr_t *attr,\t\t void *(*start_routine) (void *), void *arg){ void *stackaddr = NULL; size_t stacksize = 0; // ...... // 分配 TCB （pthread 结构体）和线程栈空间 struct pthread *pd = NULL; int err = allocate_stack (iattr, &amp;pd, &amp;stackaddr, &amp;stacksize); int retval = 0; // ...... // 初始化 TCB pd-&gt;start_routine = start_routine; // 子线程入口 pd-&gt;arg = arg; // 子线程入口参数 pd-&gt;c11 = c11; // ...... /* Setup tcbhead. */ tls_setup_tcbhead (pd); /* Pass the descriptor to the caller. */ *newthread = (pthread_t) pd; // ...... // .......some more signal related stuff /* Start the thread. */ if (__glibc_unlikely (report_thread_creation (pd))) // 如果需要报告 TD_CREATE 事件 { // ......在我们的例子中不会走到这里，忽略 // event 是 debug 时用的，例如用来通知 gdb 线程已经创建 } else // ！！创建内核态线程 task！！ retval = create_thread (pd, iattr, &amp;stopped_start, stackaddr,\t\t\t stacksize, &amp;thread_ran); // ...... if (__glibc_unlikely (retval != 0)) { // ......错误处理，忽略 } else {\t // 放开 pd 上的锁，让子线程自由运行 if (stopped_start) lll_unlock (pd-&gt;lock, LLL_PRIVATE); // ...... } out: if (destroy_default_attr) __pthread_attr_destroy (&amp;default_attr.external); return retval;}pthread_create 做的事情并不复杂： 为子线程分配栈空间和 TCB（pd） 准备/配置好 TCB 各项参数，包括子线程入口和入口参数 调用 create_thread() 启动子线程 task线程 task 创建 create_thread()这个函数在 pthread_create() 中负责为子线程创建实际的内核态 task，通过调用 clone 实现（__clone_internal() 是 clone/clone2/clone3 的简单封装）。// pthread_create.c:231static int create_thread (struct pthread *pd, const struct pthread_attr *attr,\t\t\t bool *stopped_start, void *stackaddr,\t\t\t size_t stacksize, bool *thread_ran){ // ...... /* We rely heavily on various flags the CLONE function understands: CLONE_VM, CLONE_FS, CLONE_FILES\tThese flags select semantics with shared address space and\tfile descriptors according to what POSIX requires. ...... 篇幅原因缩略，查看原始文件或 `man clone` 查询每个 flag 作用 The termination signal is chosen to be zero which means no signal is sent. */ const int clone_flags = (CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SYSVSEM\t\t\t | CLONE_SIGHAND | CLONE_THREAD // ！！注意到这个 CLONE_THREAD flag\t\t\t | CLONE_SETTLS | CLONE_PARENT_SETTID\t\t\t | CLONE_CHILD_CLEARTID\t\t\t | 0); TLS_DEFINE_INIT_TP (tp, pd); struct clone_args args = { .flags = clone_flags, // clone flags .pidfd = (uintptr_t) &amp;pd-&gt;tid, .parent_tid = (uintptr_t) &amp;pd-&gt;tid, // CLONE_PARENT_SETTID .child_tid = (uintptr_t) &amp;pd-&gt;tid, .stack = (uintptr_t) stackaddr, .stack_size = stacksize, .tls = (uintptr_t) tp, // CLONE_SETTLS }; // clone，子线程从 start_thread() 开始执行 int ret = __clone_internal (&amp;args, &amp;start_thread, pd); if (__glibc_unlikely (ret == -1)) return errno; // ...... return 0;}这里注意到，调用 clone 的时候，传递给内核的 flags 中含有 CLONE_THREAD 这个 flag。这个 flag 意味着用户态显式地告知了内核，克隆出来的 task 应该被作为一个线程看待。先不管这个 flag 的具体影响是什么，传递这个 flag 这件事情本身足以说明，内核实际上对普通进程 task 和线程 task 还是有专门的区分的，并不是除了资源共享程度不同以外其他都完全一模一样。我们前面针对子线程的 task 会被自动 reap 掉这件事，做出了两种猜想：可能是内核特殊处理了线程 task，也可能是 pthread 自己在子线程末尾回收了子线程。CLONE_THREAD 这个 flag 的存在加大了第一种猜想正确的可能性，不过并不完全排除第二种猜想，要排除第二种猜想，需要看子线程的用户例程执行完毕后，在 pthread 中都做了什么，有没有回收掉子线程 task。子线程 task 执行入口 start_thread()create_thread() 创建的子线程的执行入口固定为 start_thread()，这个函数再从 pd-&gt;start_routine 和 pd-&gt;args 获得用户函数的地址和参数，并跳转到用户函数开始执行。/* Local function to start thread and handle cleanup. */static int _Noreturnstart_thread (void *arg){ struct pthread *pd = arg; // ...... // ......thread local storage and stuff // ......unwinders and stuff, for cancellation and/or exception handling // ...... if (__glibc_likely (! not_first_call)) { /* Store the new cleanup handler info. */ THREAD_SETMEM (pd, cleanup_jmp_buf, &amp;unwind_buf); __libc_signal_restore_set (&amp;pd-&gt;sigmask); LIBC_PROBE (pthread_start, 3, (pthread_t) pd, pd-&gt;start_routine, pd-&gt;arg); /* Run the code the user provided. */ void *ret; if (pd-&gt;c11)\t{\t // ......c11 标准下多了一个类型转换问题，我们不使用 c11 标准，不走到这里，略\t} else\tret = pd-&gt;start_routine (pd-&gt;arg); // 使用用户提供的参数，调用用户函数，得到返回值 ret THREAD_SETMEM (pd, result, ret); // 用户函数返回值 ret 存入到 pd-&gt;result } // ======= 到这里，用户函数已经执行完毕，子线程任务完成，进入结束阶段 ====== // thread local storage、thread local data 析构#ifndef SHARED if (&amp;__call_tls_dtors != NULL)#endif __call_tls_dtors (); /* Run the destructor for the thread-local data. */ __nptl_deallocate_tsd (); /* Clean up any state libc stored in thread-local variables. */ __libc_thread_freeres (); /* Report the death of the thread if this is wanted. */ if (__glibc_unlikely (pd-&gt;report_events)) { // ......在需要时，报告 TD_DEATH 事件，略 // event 是 debug 时用的，例如用来通知 gdb 线程已经退出 } // ...... if (__glibc_unlikely (atomic_decrement_and_test (&amp;__nptl_nthreads))) /* This was the last thread. */ exit (0); // 如果这个线程是最后一个线程，退出整个线程组（thread group） // ......signal and mutex and stuff // ......如果栈空间是 pthread 分配的（而不是用户创建线程时提供的），则回收栈空间 // ...... // 如果线程被 pthread_detach 过，则顺便回收 TCB（注意 TCB 不是 task_struct，是用户态 pthread 结构体） // 默认行为是不在这里回收 TCB，我们的例子中也是默认情况，即不会在这里执行 `__nptl_free_tcb(pd)`，而是需要等到 pthread_join() 时才会回收 TCB // 这是由于 pd 结构体中含有用户态返回值 pd-&gt;result 可能会被父线程需要；pthread_detach 则意味着父线程不关心子线程的执行结果。 if (IS_DETACHED (pd)) /* Free the TCB. */ __nptl_free_tcb (pd);out: // 子线程完全结束，最后一步：调用 sys_exit 系统调用，结束【子线程】（不是整个进程） while (1) INTERNAL_SYSCALL_CALL (exit, 0); /* NOTREACHED */}注意到最后一步 sys_exit 和常见的 exit()/_exit() 不同，前者是系统调用，后者是由 libc 提供的用户态包装方法。两者的作用效果也不一样： sys_exit 是退出当前【调度单元】，即 task，在这里是指当前【线程】 而 exit()/_exit() 实际上包装的是 sys_exit_group 系统调用，代表退出整个【线程组】，即整个进程的所有线程。 命名的混乱是历史原因，由于一开始 Linux 只支持多进程，最初 exit()/_exit() 也的确封装的是 sys_exit。 而后来加入多线程后，Linux 在内核态内引入了一个新概念：thread group。原来的进程变成了 task，task_struct-&gt;pid 变成了线程 id（gettid() 返回 task_struct-&gt;pid），而现在常说的进程 id，则是新的task_struct-&gt;tgid（thread group id，getpid() 返回 task_struct-&gt;tgid ）。同时，libc 的 exit()/_exit() 也被改为调用新的 sys_exit_group，即结束整个线程组。原来的 sys_exit 的作用不变，依然是结束一个调度单元 task，只是「调度单元」的概念改变了而已。 这个比较 hack 的方式，使得 linux 以较小的改动，实现了对线程的支持，坏处就是导致用户态的 “pid” 的概念和内核态中的 “pid” 的含义不一致，不留意的话容易混淆。可以看到，子线程的入口函数 start_thread()，在执行完用户函数后，销毁了栈和 thread local storage，然后执行了 sys_exit 结束子线程 task。这个过程和多进程模型中一个子进程调用 exit() 退出线程是类似的，并不保证一定清理掉 task_struct，而是理论上有可能使 task 进入 ZOMBIE 状态。而且这里可以看到，pthread 也没有让子进程做（诸如自己 wait 自己？）之类的魔法来显式清理掉子进程的 task。这说明我们之前的猜想2是不正确的，子线程的内核 task_struct 并不是 pthread 在子线程退出后进行特殊处理 reap 回收的。只能是 sys_exit 系统调用中，退出子线程 task 的时候，内核自己决定要直接 reap 掉这个 task。 实际上猜想2本身也不可能，一个 task 不可能自己回收自己的资源，因为只有已经结束的 task 才能被回收，但是已经结束的 task 就无法执行任何代码了，也就没法回收自己。退出子线程 sys_exit 系统调用前面通过排除，将我们子线程的 task 被 reap 的准确位置定位到了 sys_exit 中了。我们从用户态的 glibc 以及 pthread，继续进入到内核态代码的范围中。前一小节结尾提到，我们发现子线程的 task 在用户态是正常 sys_exit 退出的，但是 sys_exit 后 pid 以及 task_struct 被马上回收掉，而不是像普通进程一样进入僵尸状态，这里看到内核 do_exit() 方法（sys_exit 系统调用的内核态处理函数）：https://elixir.bootlin.com/linux/v5.15/source/kernel/exit.c// kernel/exit.c:727// 不用仔细看这个函数的每一步，这里全放出来只是为了体现步骤有多么多而已void __noreturn do_exit(long code){\tstruct task_struct *tsk = current;\tint group_dead;\t// ......\tprofile_task_exit(tsk);\tkcov_task_exit(tsk);\tptrace_event(PTRACE_EVENT_EXIT, code);\tvalidate_creds_for_do_exit(tsk);\t// ......\t\tio_uring_files_cancel();\texit_signals(tsk); /* sets PF_EXITING */\t/* sync mm's RSS info before statistics gathering */\tif (tsk-&gt;mm)\t\tsync_mm_rss(tsk-&gt;mm);\tacct_update_integrals(tsk);\tgroup_dead = atomic_dec_and_test(&amp;tsk-&gt;signal-&gt;live);\tif (group_dead) {\t\t/*\t\t * If the last thread of global init has exited, panic\t\t * immediately to get a useable coredump.\t\t */\t\tif (unlikely(is_global_init(tsk)))\t\t\tpanic(\"Attempted to kill init! exitcode=0x%08x\\n\",\t\t\t\ttsk-&gt;signal-&gt;group_exit_code ?: (int)code);#ifdef CONFIG_POSIX_TIMERS\t\thrtimer_cancel(&amp;tsk-&gt;signal-&gt;real_timer);\t\texit_itimers(tsk-&gt;signal);#endif\t\tif (tsk-&gt;mm)\t\t\tsetmax_mm_hiwater_rss(&amp;tsk-&gt;signal-&gt;maxrss, tsk-&gt;mm);\t}\tacct_collect(code, group_dead);\tif (group_dead)\t\ttty_audit_exit();\taudit_free(tsk);\ttsk-&gt;exit_code = code;\ttaskstats_exit(tsk, group_dead);\texit_mm();\tif (group_dead)\t\tacct_process();\ttrace_sched_process_exit(tsk);\texit_sem(tsk);\texit_shm(tsk);\texit_files(tsk);\texit_fs(tsk);\tif (group_dead)\t\tdisassociate_ctty(1);\texit_task_namespaces(tsk);\texit_task_work(tsk);\texit_thread(tsk);\tperf_event_exit_task(tsk);\tsched_autogroup_exit_task(tsk);\tcgroup_exit(tsk);\tflush_ptrace_hw_breakpoint(tsk);\texit_tasks_rcu_start();\texit_notify(tsk, group_dead);\tproc_exit_connector(tsk);\tmpol_put_task_policy(tsk);#ifdef CONFIG_FUTEX\tif (unlikely(current-&gt;pi_state_cache))\t\tkfree(current-&gt;pi_state_cache);#endif\t/*\t * Make sure we are holding no locks:\t */\tdebug_check_no_locks_held();\tif (tsk-&gt;io_context)\t\texit_io_context(tsk);\tif (tsk-&gt;splice_pipe)\t\tfree_pipe_info(tsk-&gt;splice_pipe);\tif (tsk-&gt;task_frag.page)\t\tput_page(tsk-&gt;task_frag.page);\tvalidate_creds_for_do_exit(tsk);\tcheck_stack_usage();\tpreempt_disable();\tif (tsk-&gt;nr_dirtied)\t\t__this_cpu_add(dirty_throttle_leaks, tsk-&gt;nr_dirtied);\texit_rcu();\texit_tasks_rcu_finish();\tlockdep_free_task(tsk);\tdo_task_dead();}EXPORT_SYMBOL_GPL(do_exit);可以看到一个 task 退出时需要清理/释放的资源种类非常之多，主流程do_exit()里的子流程函数调用就有好几十个了。我们想要从中找到这个逻辑：exit 的时候，内核根据什么决定是否直接 reap 掉 task。利用已知知识帮助快速定位到想要找的逻辑的代码：已知对于一般进程来说，如果父进程设置了忽略 SIGCHLD 信号（signal(SIGCHLD, SIG_IGN)），则子进程 exit 的时候会【reap 掉 task】，否则子进程会【进入 ZOMBIE 状态】。这实际上正是我们要找的「exit 决定是否直接 reap 掉 task」的决策过程的一部分。猜测对于线程 task 是否自动 reap 的决策逻辑也是在相同的位置或附近。故以【进入 ZOMBIE 状态】为线索反查，直接搜索 ` = EXIT_ZOMBIE 尝试找所有将 task 状态设置为 ZOMBIE 的地方，快速定位到 exit_notify()` 中：/* * Send signals to all our closest relatives so that they know * to properly mourn us.. */static void exit_notify(struct task_struct *tsk, int group_dead){\tbool autoreap; // 是否自动 reap 掉 task\t// ......\ttsk-&gt;exit_state = EXIT_ZOMBIE; // 默认将 task 置入 EXIT_ZOMBIE 状态\tif (unlikely(tsk-&gt;ptrace)) { // 如果启用了 ptrace（一般用于 debug）\t\tint sig = // ......\t\tautoreap = do_notify_parent(tsk, sig);\t} else if (thread_group_leader(tsk)) { // 如果是线程组组长，即主线程\t // 并且整个线程组（进程）中所有线程都已经退出\t // 则发送 SIGCHLD 给 parent，如果父进程 SIG_IGN 掉了 SIGCHLD，则自动 reap\t\tautoreap = thread_group_empty(tsk) &amp;&amp;\t\t\tdo_notify_parent(tsk, tsk-&gt;exit_signal);\t} else { // 其他任何情况，即：不是线程组组长（例如子线程）\t\tautoreap = true; // 则均自动 reap\t}\tif (autoreap) { // 自动 reap 的进程直接进入 EXIT_DEAD 状态\t\ttsk-&gt;exit_state = EXIT_DEAD;\t\tlist_add(&amp;tsk-&gt;ptrace_entry, &amp;dead);\t}\t// ......}这里证实了猜想1：当一个 task 不是一个线程组的组长的时候，内核会在 exit 的时候直接 reap 掉子线程的 task。所以在子线程执行完毕但是未 join 之间，ps 会看不到子线程，因为子线程 task 已经被内核回收了。也就是说，只有一个线程组（也就是进程）的主线程可以进入僵尸状态，所有的子线程都不可能会有僵尸状态。子线程的 task 在用户程序代码执行完毕后，就马上退出并被回收了。而子线程的执行结果（返回值等）则保存在用户态 pthread 结构体中，供后续可能的 JOIN 使用。结论对于 Linux 平台上的 pthread 线程，在子线程比父线程先退出且没被 JOIN 的情况下，不会产生和传统意义上的僵尸进程类似的“僵尸线程”（即 ps 不会看到有 defunct 的线程 task，子线程 task 会在 exit 时被内核直接回收掉，不等父进程 JOIN）。但是并不意味着没有被 pthread_join 的线程完全不会占用资源。没被回收的线程虽然不会占用内核的 task 资源，但是会在用户态留下 pthread 结构体（TCB）以及线程的栈（因为 pthread 结构体和线程栈是一起分配一起释放的），如果未被 JOIN 的线程累积过多，仍然可能会导致用户态资源耗尽而导致该进程无法创建新的线程。与僵尸进程不同的是，“僵尸线程”堆积的影响只限制在一个进程之内，理论上不会导致系统上其他进程创建失败（因为不占用 task_struct 和 pid/tid）。 注意到该结论只适用于 Linux，因为 Linux 实现线程的方式为内核轻改动，大多数线程相关的功能实现都在用户态中实现（glibc）。不排除其他 POSIX 系统可能内核级原生支持 pthread 线程。pthread_detach() 过的线程，则 pthread 会在线程执行完成后自动释放 pthread 结构体以及栈，所以对不关心执行结果的线程，应当使用 pthread_detach() 进行脱离。若不进行脱离，则必须确保所有线程在合适的时间都能被 pthread_join() 回收。 Reference:https://manpages.debian.org/bullseye/manpages-dev/clone.2.en.html#CLONE_THREADhttps://codebrowser.dev/glibc/glibc/nptl/https://elixir.bootlin.com/linux/v5.15/source/kernel/exit.c" }, { "title": "MySQL Prepare后语句查询性能降低 内核源码bug排查分析", "url": "/posts/mysql-prepare-slower-query-bug-analyze/", "categories": "Backend, Database, MySQL", "tags": "Chinese, MySQL", "date": "2022-11-18 18:00:00 +0800", "snippet": "源自于业务上遇到的一个先将某个语句Prepare再Execute查询效率很低的问题，而将查询中的参数直接嵌入到SQL语句内并以文本形式执行，则执行反而变得很快。测试环境：腾讯云 MySQL 服务（txsql8.0.22）、MySQL 源码编译（refs/tags/mysql-8.0.22）问题描述背景MySQL 中，语句执行有两种方式，分别是 Text Protocol 和 Prepared Statement。两者主要的差别是传参方式的不同（返回包格式也不同，这里不展开）。 Text Protocol 是直接将语句中的参数嵌入到 SQL 语句中，以文本的形式整个语句直接传递到数据库。（后面称为「文本SQL模式」） Prepared Statement 则是先 COM_STMT_PREPARE 一个查询语句的模板，如 SELECT * FROM t1 WHERE a = ? AND b = ?，然后再执行 COM_STMT_EXECUTE，将实际的参数传入，替换掉占位符 ? 并执行。（后面称为「Prepare/Execute模式」）问题定位并简化问题查询，得到最小可复现样例：表结构：CREATE TABLE t1 (\tcol1 BIGINT NOT NULL,\t\tKEY idx_t1_col1(col1));查询：SELECT * FROM t1 WHERE col1=10036 ORDER BY col1 ASC LIMIT 5;可以看到，这个查询限定结果集中 col1 = 10036，然后要求结果按照 col1 排序实际上 ORDER BY col1 ASC 是冗余的，因为结果中所有的 col1 的值都是一样的（10036）。问题在于：这个查询，在表数据量较大（200w）时，Prepare/Execute模式下的速度非常慢（~100ms），而文本SQL模式则很快（0.7ms）。这个是比较反直觉的，因为 Prepare/Execute模式的优势之一就是传递数据更高效，并且可以避免重复解析语句，每次执行只需要插入参数（Prepared_statement::insert_params()）并优化即可执行，理论上应该比文本SQL模式更快。 相比之下 SELECT * FROM t1 ORDER BY col1 ASC LIMIT 5（删掉Where，只保留 Order By）只要0.7ms，也就是 Where 和 Order By 同时存在的时候，同一个语句，Prepare/Execute模式的性能出现了明显降低，而文本SQL模式不受影响。问题分析由于执行的是同一个语句，排除了两种模式的RTT差异因素后，初步怀疑是「Prepare/Execute模式」和「文本SQL模式」下，优化器生成的执行计划出现了差异导致。使用 SET SESSION OPTIMIZER_TRACE='enabled=on';，打开 optimizer trace，分别以prepare/execute和文本SQL形式执行该SQL，得到优化器 trace：可以看到 trace 中大多数优化流程和结果是完全一致的（符合预期，因为毕竟是同一个语句），但是在 optimizing_distinct_group_by_order_by ，即 ORDER BY 优化部分出现了差异。问题1 Prepare/Execute模式没有剔除冗余 ORDER BY前面提到在我们制定的查询条件 col1 = 10036 下，ORDER BY col1 ASC 实际上是冗余的，因为 col1 是常量，理论上该阶段优化器会剔除掉这个 ORDER BY。 文本SQL模式执行时，优化器成功地发现了这一点（图中1），即\"equals_constant_in_where\": true，并正确地将该冗余 order by clause 优化掉，resulting_clause 为空。 Prepare/Execute模式下，优化器由于某些原因，并没有发现 col1 实际上恒等于一个常量，没有在这一阶段优化掉冗余的 order by（图中2）（最后结论是为 MySQL 8.22 的一个 bug）问题2 ORDER BY 没有利用索引但是这不是效率低的全部原因，因为即使优化器没有剔除掉冗余 ORDER BY，由于 col1 上有索引，对 col1 的 ORDER BY 也应该是不会产生排序操作的，理论上可以直接访问索引得到有序 col1，相比直接剔除 ORDER BY 也不应该会慢这么多（百倍）。这里继续往下可以看到，Prepare/Execute模式下，优化器出于某种原因，在考虑 order by 的执行路径的时候，认为 col1 上的索引（KEY idx_t1_col1(col1)）无法满足我们要进行的排序条件（\"index_provides_order\": false）（图中3），这显然是错误的，导致了执行器不从索引上直接获得有序的 col1，而是自己又对所有数据进行了一次排序，导致效率低下。这里注意到 order_direction 为 undefined，在 MySQL 8.0.22 源码上验证，没有成功复现，原版 MySQL 中 trace 出来的这个 key 的 order_direction 为 asc，并且能够正确使用索引来避免对 col1 进行排序。原因调查问题1 冗余 ORDER BY 剔除源码编译 mysql 8.0.22，并挂载 gdb，由于 optimizer trace 中体现出来的差异是 equals_constant_in_where （where条件等于常量）是否为 true，全局搜索这个关键字，发现是在 JOIN 语句优化部分的 JOIN::remove_const 函数中，该函数被 JOIN::optimize_distinct_group_order 函数调用，即这部分逻辑是在判断 ORDER BY 中是否存在按常量列排序的 clause，如果存在则剔除掉（符合预期）。部分调用栈#0 const_expression_in_where at sql/sql_select.cc:3798#1 JOIN::remove_const at sql/sql_optimizer.cc:9865#2 JOIN::optimize_distinct_group_order at sql/sql_optimizer.cc:1261#3 JOIN::optimize at sql/sql_optimizer.cc:614#4 SELECT_LEX::optimize at sql/sql_select.cc:1905#5 SELECT_LEX_UNIT::optimize at sql/sql_union.cc:680#6 Sql_cmd_dml::execute_inner at sql/sql_select.cc:924#7 Sql_cmd_dml::execute at sql/sql_select.cc:725#8 mysql_execute_command at sql/sql_parse.cc:4344#9 Prepared_statement::execute at sql/sql_prepare.cc:3487#10 Prepared_statement::execute_loop at sql/sql_prepare.cc:2994#11 mysqld_stmt_execute at sql/sql_prepare.cc:1891# ......JOIN::remove_const() 函数 优化器JOIN语句优化阶段，移除常量的 order by 或 group by 条件// sql/sql_optimizer.cc:9774ORDER *JOIN::remove_const(ORDER *first_order, Item *cond, bool change_list, bool *simple_order, bool group_by) { // ...... // line 9812 // 对于每一个 order_by 条件 for (order = first_order; order; order = order-&gt;next) { Opt_trace_object trace_one_item(trace); trace_one_item.add(\"item\", order-&gt;item[0]); table_map order_tables = order-&gt;item[0]-&gt;used_tables(); if (/* ......aggregation检查 */) // ...... else if (/* ......子查询优化 */) { // ...... } else if (/* ......重复排序检查 */) { // ...... } else if (/* ......检测子查询 */) // ...... else { // line 9861 if (order_tables &amp; (RAND_TABLE_BIT | OUTER_REF_TABLE_BIT)) *simple_order = false; else { // ！！检查排序的依据列是否是常数值，即所有行这一列的值都相等 if (cond &amp;&amp; const_expression_in_where(cond, order-&gt;item[0])) { trace_one_item.add(\"equals_constant_in_where\", true); continue; // 如果是，则跳过，忽略该 ORDER BY } if ((ref = order_tables &amp; (not_const_tables ^ first_table))) { // ...... } } } if (change_list) *prev_ptr = order; // use this entry prev_ptr = &amp;order-&gt;next; } // ...... return first_order;}这里调用了 const_expression_in_where(cond, order-&gt;item[0]) 函数，来判断 cond （也就是 WHERE 中指定的筛选条件）中是否有使得排序条件 order-&gt;item[0] 恒等于某个值的条件，从而决定是否剔除 ORDER BY order-&gt;item[0] 的排序条件 （比如 col1 = 10036 这样的条件）const_expression_in_where() 函数 8.0.24 后叫 check_field_is_const() 判断 WHERE 条件 cond 内是否有能使得结果集中 comp_item 恒为某个常数的条件。如果有，则可以断言 ORDER BY comp_item 为冗余操作，可以剔除。// sql/sql_select.cc:3773bool const_expression_in_where(Item *cond, Item *comp_item, const Field *comp_field, Item **const_item) { // ...... if (cond-&gt;type() == Item::COND_ITEM) { // `a = ?` 这样的条件实际上是一个布尔比较函数，所以不是走到这个分支 // ...... } else if (cond-&gt;eq_cmp_result() != Item::COND_OK) { // 布尔比较函数 Item_func *func = (Item_func *)cond; if (func-&gt;functype() != Item_func::EQUAL_FUNC &amp;&amp; func-&gt;functype() != Item_func::EQ_FUNC) return false; // 如果是等于比较（left = right） Item *left_item = ((Item_func *)cond)-&gt;arguments()[0]; Item *right_item = ((Item_func *)cond)-&gt;arguments()[1]; // 且 left 或 right 其中有一个是要检查的目标 comp_item // 则检查 `left = right` 这个条件是否足够保证 comp_item 的值的唯一性 if (equal(left_item, comp_item, comp_field)) { if (test_if_equality_guarantees_uniqueness(left_item, right_item)) { if (*const_item) return right_item-&gt;eq(*const_item, true); *const_item = right_item; return true; } } else if (equal(right_item, comp_item, comp_field)) { if (test_if_equality_guarantees_uniqueness(right_item, left_item)) { if (*const_item) return left_item-&gt;eq(*const_item, true); *const_item = left_item; return true; } } } return false;}原理是对于每一个 WHERE col = xxx 的条件，检查 col = xxx 条件成立能否保证 col 在结果集中的值唯一（test_if_equality_guarantees_uniqueness），这里需要检查： 右侧的量 xxx 是否是个常量（不能是引用其他的列，也不能是一个子查询） 是否类型一致，如果是字符串，编码是否一致 注（与主问题无关）：仅仅满足 xxx 是常量并不足以保证结果集中的 col 的值唯一，因为在 col 的类型和 xxx 不一致的时候，会出现 type cast 自动类型转换。比如当 col 的类型是 string，而 xxx 是个 int 的时候，可能会出现如下情况： mysql&gt; insert into t2 values(\"123\"),(\"123a\"),(\"123bbbc\");Query OK, 3 rows affected (0.01 sec)mysql&gt; select * from t2 where col = 123;+---------+| col |+---------+| 123 || 123a || 123bbbc |+---------+3 rows in set (0.03 sec) 这是由于 col 的值在和 123 做比较的时候，会将两者都 typecast 成 double，然后再进行比较，string cast 到 double 的时候，会丢弃掉尾部无效字符，123bbbc 会只剩下 123，导致\"123bbbc\" = 123 结果为 true。这里的条件 col = 123 就是一个等号右侧为常量，但是还是无法保证结果集中该列的「值唯一」的例子。https://dev.mysql.com/doc/refman/8.0/en/type-conversion.htmltest_if_equality_guarantees_uniqueness() 函数 8.0.24 后叫 check_field_is_const()检查 「l = r 条件成立」能否推出「结果集中 l 的值唯一」需要检查： r 是否是个常量（r-&gt;const_item() 是否为 true） 是否类型一致，如果是字符串，编码是否一致// sql/sql_select.cc:3730static bool test_if_equality_guarantees_uniqueness(const Item *l, const Item *r) { return r-&gt;const_item() &amp;&amp; /* elements must be compared as dates */ (Arg_comparator::can_compare_as_dates(l, r) || /* or of the same result type */ (r-&gt;result_type() == l-&gt;result_type() &amp;&amp; /* and must have the same collation if compared as strings */ (l-&gt;result_type() != STRING_RESULT || l-&gt;collation.collation == r-&gt;collation.collation)));}这里检查「r 是否是个常量」的方法是 r-&gt;const_item()，继续跟踪发现，这个方法判断的是 r 在整个 Prepared_statement 中是否恒定为常量。Item::const_item() 函数 该 item 是否是个常量（要求在整个表达式中自始至终都是常量，不管执行状态如何）这里是问题所在。回到一开始的例子中：SELECT * FROM t1 WHERE col1 = 10036 ORDER BY col1 ASC LIMIT 5;Prepare/Execute 模式这个查询中的条件 col1 = 10036，Prepare/Execute模式下，在优化器眼里其实是 col1 = ?，使用 gdb 打出 left_item 和 right_item 可以验证这一点：而占位符 ?，由于实际的值需要 execute 阶段才传入，prepare 阶段自然是不把它标记为常量的。占位符对于这个表达式本身来说也的确不算常量（理由是每一次 execute 传入的实际值可能都不一样），最多只能说它在「某一次具体的执行过程」之中，才可以被认为是常量：(gdb) p left_item-&gt;item_name.m_str # 左 item 名称（col1）$76 = 0x7f25a4024ef0 \"col1\"(gdb) p right_item-&gt;item_name.m_str # 右 item 名称（?）$77 = 0x5b6de48 \"?\"(gdb) p ((Item_param*)right_item)-&gt;value.integer # 右 item 代入值$78 = 10036(gdb) p right_item-&gt;const_item() # 右 item 在表达式中是否恒为常量$79 = false即对于 col1 = 10036 条件，在 Prepare/Execute 模式下，实际上优化器看到的是 col1 = ? ，right_item-&gt;const_item() 会返回 false。这一点在 Item::const_item() 中也有提到：/** Returns true if item is constant, regardless of query evaluation state. An expression is constant if it: - refers no tables. - refers no subqueries that refers any tables. - refers no non-deterministic functions. - refers no statement parameters. -- 即 `?` - contains no group expression under rollup*/bool const_item() const { return (used_tables() == 0); }从而导致 test_if_equality_guarantees_uniqueness() 认为结果集中 col1 的值可能不唯一，所以认为需要保留（实际上是冗余的）ORDER BY col1 条件。文本SQL模式相比之下，如果我们用文本SQL方式执行，即 col1 = 10036，则这个条件的右参数 10036 在表达式解析 Prepare 的时候，值就已经确定下来了，所以 right_item-&gt;const_item() 为 true：(gdb) p left_item-&gt;item_name.m_str # 左 item 名称（col1）$80 = 0x7f25a4024de0 \"col1\"(gdb) p right_item-&gt;item_name.m_str # 右 item 名称（10036）$81 = 0x7f25900074b0 \"10036\"(gdb) p right_item-&gt;const_item() # 右 item 在表达式中是否恒为常量$82 = true所以该模式下 col1 = 10036 能够通过 test_if_equality_guarantees_uniqueness 检查，优化器能够剔除掉冗余的 ORDER BY col1。从而出现了同一个语句在「Prepare/Execute 模式」和「文本SQL模式」下产生了不同的执行计划的现象。分析 &amp; 结论test_if_equality_guarantees_uniqueness() 要做的，实际上是检查【在这次执行中】 某个 WHERE 条件能否确保结果集中的某一列唯一。所以其检查的第一个条件 r-&gt;const_item()（参数是否在整个表达式构造的时候就是 constant 的，无论执行状态）实际上是 overkill。（因为 Prepare/Execute 模式下每一次 Execute 都会用当次传入的参数重新跑一遍 optimize）对于优化器来说，判断某个占位符 item 是否为常量，实际上并不需要关心这一次 Execute 的时候这个占位符的值是不是永远和之前每一次 Execute 的时候相同（即r-&gt;const_item()为 true），而只需要知道【同一次 Execute 过程内】该占位符 item 的值是常量就足够了。Item类实际上也提供了一个方法用来判断某个 item 是不是同一次 execution 内是常量，这个就包括了 ? 占位符的情况：/** Returns true if item is constant during one query execution. If const_for_execution() is true but const_item() is false, value is not available before tables have been locked and parameters have been assigned values. This applies to - statement parameters &lt;---重点 - non-dependent subqueries - deterministic stored functions that contain SQL code. For items where the default implementation of used_tables() and const_item() are effective, const_item() will always return true.*/bool const_for_execution() const { return !(used_tables() &amp; ~INNER_TABLE_BIT);}使用 gdb 验证，col1 = ? 中的参数?（代入参数值10036）：(gdb) p right_item-&gt;item_name.m_str # item 名称$77 = 0x5b6de48 \"?\"(gdb) p ((Item_param*)right_item)-&gt;value.integer # 代入值$78 = 10036(gdb) p right_item-&gt;const_item() # 在表达式中是否恒为常量$79 = false (gdb) p right_item-&gt;const_for_execution() # 在*某次具体执行*中是否是常量$80 = true解决方法就是用 const_for_execution() 替换 test_if_equality_guarantees_uniqueness() 中的 const_item() 判断：static bool test_if_equality_guarantees_uniqueness(const Item *l, const Item *r) { return /* r-&gt;const_item() is overkill */ r-&gt;const_for_execution() &amp;&amp; /* elements must be compared as dates */ (Arg_comparator::can_compare_as_dates(l, r) || /* or of the same result type */ (r-&gt;result_type() == l-&gt;result_type() &amp;&amp; /* and must have the same collation if compared as strings */ (l-&gt;result_type() != STRING_RESULT || l-&gt;collation.collation == r-&gt;collation.collation)));}幸运（不幸）的是，追踪代码库中这一行的提交历史，可以看到，这个 bug 在 mysql-8.0.24 中已经被修复了，修复方法就是使用 const_for_execution() 替换 const_item()，具体的 patch 是 443384454fdbb365a20bfbf86f552ab914d1ea92。changelog: Filesort was used for a query having an ORDER BY … DESC clause, even when an index on the descending column was available and used. This happened because an ORDER BY sub-clause was not removed due to matching a field in an equality predicate, even though it should have, so that the optimizer did not match the query with the descending index, leading to suboptimal performance. (Bug #101220, Bug #32038406)The problem here is that an ORDER BY sub-clause was not removeddue to matching a field in an equality predicate, even though it should.Because of this, the optimizer fails to match the query with adescending index, causing non-optimal performance.The essence of the problem is that the functiontest_if_equality_guarantees_uniqueness() returns false if collation ofleft and right hand side of equality predicate are different.However, this should not make a difference as long as the collation ofthe column in the index matches the collation of the equality operation.And these should be the same since they are both derived from thecollation of the column.Thus, the main part of this bugfix is to pass down the actual operationto the function, and perform an improved collation check.The logic of the function was also further enhanced:- It now accepts const values and const-for-execution values such as user variables and dynamic parameters.- It is more specific on rejected predicates involving temporals and strings (it rejects predicates where the column is a string and the const value is a temporal).Regression tests were added for these changes as well.经过验证这个 bug 在 mysql-8.0.22 到 mysql-8.0.23 中存在 。腾讯云线上使用的是基于 mysql-8.0.22 的修改版本，所以存在这个缺陷。 edit: 了解到这个 bug 是在 mysql 8.0.22 官方实现 prepare once 功能时引入的众多 bug 之一： https://dev.mysql.com/worklog/task/?id=9384总结（TLDR）这个问题总结起来就是：优化器在优化 ORDER BY col1 的时候，有一个检查「WHERE 条件是否使得结果集中 col1 的值唯一」的检查，作用是如果有存在 WHERE col1 = 10036 这样的条件，则意味着结果中所有 col1 的值都会是 10036，那 ORDER BY col1 排序与否就没有任何差别了，可以直接优化掉。这个检查的其中一个步骤，是检查 col1 = xxx 后面的 xxx 部分是不是一个常量，因为如果假设条件是 col1 = RAND() 之类，就不能保证结果集中 col1 的值还是唯一的了。问题在于，这个检查在 Prepare/Execute 模式下，MySQL检查过严了，将 col1 = ? 中的占位符 ? 认为不满足常量条件，不是一个常量，从而导致没能剔除冗余的 ORDER BY col1，使得执行计划非最优。而文本模式下， col1 = 10036 中的 10036 是满足常量检查条件的，所以优化器成功剔除了冗余的 ORDER BY，产生了比 Prepare/Execute 模式更高效的执行计划。问题2 ORDER BY 没有利用索引问题1解释了为何 「Prepare/Execute 模式」下产生的执行计划会比「文本SQL模式」多一个排序的步骤，但是这并不能完整解释为何性能会差那么多，[[#问题分析]]中提到了，Prepare/Execute 模式下，不仅多出来了 ORDER BY 步骤，而且这个步骤本该能利用 col1 上的索引的，但是实际上并没有使用索引，而是进行了一次外排。由于这个问题在 MySQL 8.0.22 上没能复现，认定为可能是 TXSQL 8.0.22 的问题，由于没有 TXSQL 8.0.22 的源码，联系内核的维护者协助调查。（WIP）解决方案对于项目中遇到的场景，在指定 WHERE col1 = xxx 条件的时候，在构造SQL时就不添加 ORDER BY col1 条件，防止触发bug。并且选择用文本SQL模式来执行请求，规避潜在的问题。" }, { "title": "MIT6.830 Database Systems | 数据库系统", "url": "/posts/mit6830-database-systems/", "categories": "Course Notes, MIT6.830", "tags": "system design, database, course recommendation", "date": "2022-08-24 18:24:00 +0800", "snippet": "MIT6.830 Database Systems (Spring 2021) 前置课程： 6.033 Computer System Design 计算机系统设计 这门课与MIT 6.814是同一门课程，两者区别在于Final Project在6.814中由Lab5以及Lab6替代。虽然这门课是研究生课程，但是在MIT里，这门课大概1/3的学生是本科生。课程介绍MIT6.830 Database Systems 数据库系统课程为麻省理工学院的研究生课程，主要通过来自数据库社区的阅读材料（论文），向学生介绍数据库系统的基础，重点关注基本概念，如实现关系代数和数据模型、schema 范式化、查询优化、事务等。课程不假设学生有任何先前的数据库经验。涉及话题与数据库系统的设计有关的话题，包括：数据模型、数据库和 schema 设计、schema 范式化和完整性限制（integrity constraints）、查询处理、查询优化与开销预估、事务、恢复、并行控制、隔离与一致性、分布式/并行/多样数据库、自适应数据库、trigger系统、键值存储、对象-关系映射、流式数据库、服务化数据库。来自原创研究论文的lecture和阅读。课程架构完整的课程结构参考课程Assignment和 Lecture 1 PDF。Lecture主要完成视频、PPT、PDF。节约大家挑选课程的时间，这里将 Lecture 1 中提到的课程核心概念内容摘抄出来： Data modeling &amp; layout 数据模型 &amp; 布局 Systematic approach to structuring / representing data Important for consistency, sharing, efficiency of access to persistent data Declarative Querying and Query Processing 声明式请求和请求处理 High level language for accessing data “Data Independence” Compiler that finds optimal plan for data access Many low-level techniques for efficiently getting at data Consistency / Transactions + Concurrency Control 一致性 / 事务 + 并行控制 Atomicity – Complex operations can be thought of as a single atomic operation that either completes or fails; partial state is never exposed Consistency and Isolation – Semantics of concurrent operations are well defined –equivalent to a serial execution, respecting invariants over time Durability – Completed operations persist after a failure Makes programming applications MUCH easier, since you don’t have to reason about arbitrary interleavings of concurrent code, and you know that the database will always be in a consistent state Distributed data processing 分布式数据处理 一点点其他领域的东西 A bit of many fields: systems, algorithms and data structures, languages + language design, more recently AI + learning, distributed systems…. 一些最新的论文 This course will look in detail at these areas, as well as a number of papers current in DBMSresearch, e.g., streaming, large scale data processing (Spark etc)Reading AssignmentLecture前后读论文，都是与Lecture内容相关的或者Lecture围绕的论文。读论文也是这门课的主题之一。LEC 1: Introduction to DatabasesLEC 2: The Relational ModelLEC 3: Schema DesignLEC 4: Intro to Database InternalsLEC 5: Database Operators and Query ProcessingLEC 6: Indexing and Access MethodsLEC 7: Database Layout for Analytic DatabasesLEC 8: Join AlgorithmsLEC 9: Query OptimizationLEC 10: Query Execution in Main Memory DBMSLEC 11: Learned IndexingLEC 12: Transactions And LockingLEC 13: Optimistic Concurrency Control and Snapshot IsolationLEC 14: RecoveryLEC 15: Recovery (cont.)LEC 16: Distributed DatabasesLEC 17: Distributed TransactionsLEC 18: Modern TransactionsLEC 19: Eventual ConsistencyLEC 20: MapReduce / HadoopLEC 21: Cluster ComputingLEC 22: Project/Lab DiscussionsLEC 23: Final Project Presentations这些链接都指向Reading Assignment，关于Lecture本身的信息请查看课程Assignment。Lab一个Java实现的基本教学数据库SimpleDB，以代码留空+自动化单元测试的形式，让学生接触数据库的基本机制的代码实现：存取数据、查询操作符、事务、锁、并行查询、索引等等。LAB 1: SimpleDBLAB 2: SimpleDB OperatorsLAB 3: Query OptimizationLAB 4: SimpleDB TransactionsLAB 5: B+ Tree IndexLAB 6: Rollback and Recovery另外，原课程还包含了一个团队的自选主题Final Project、以及可以替代Final Project的Programming Contest组队比赛。以及三个Problem Set练习和Quiz小测，感兴趣以及有时间的话也推荐完成一下。同样的，最全的信息在课程Assignment中。课程资源课本： “Readings in Database Systems” (The Red Book, 4th ed!) “Database Management Systems” (Ramakrishnan and Gehrke)课程主页：http://db.lcs.mit.edu/6.5830/2021/index.php课程介绍：http://db.lcs.mit.edu/6.5830/2021/syllabus.php课程时间表（含有Reading Assignment）：http://db.lcs.mit.edu/6.5830/2021/sched.php课程Assignment（课程视频、PPT、Project、Lab）：http://db.lcs.mit.edu/6.5830/2021/assign.php部分课程公网链接（2014）：YouTube B站搬运" }, { "title": "git的前世，和BitKeeper", "url": "/posts/origin-of-git-feat-bitkeeper/", "categories": "Misc, Unix", "tags": "git", "date": "2022-04-02 22:10:00 +0800", "snippet": "很多人应该都知道git的开发，最早是用来管理linux的内核源码的。在git之前，linux用的是一个叫做BitKeeper的商业软件进行源码管理和patch merge。最早最早，Linux采用的其实是和unix一样的邮件互发patch的方式来共享更改，随着内核开发者群体的不断增长，这样的贡献管理方式所需要的人力劳动已经开始明显影响内核的开发进展了。BitKeeper（BK）软件的作者/公司CEO–Larry McVoy向Linus提出，如果使用一个源代码管理软件，可以解决Linux日益严重的项目管理问题，使得整个流程再一次顺畅起来。Larry是Linux的极早期贡献者（1992年，Linux 0.97阶段），他本身是很支持自由软件的发展的，所以Larry希望，和开源社区达成一个协议，让开源社区可以免费使用他公司的商业软件BK。2002年，在Larry成功说服Linus相信BK能做他所需要做的一切后，Linus决定试一试BK，从此就是一条不归路。 Linus使用BK作为源代码管理工具的决定也在社区中掀起了风波，原因很简单，Linux是历史上最重要的开源软件之一，而这么重要的开源项目，居然由一个商业软件做源码管理。Linus表示自己不是开源软件狂热信奉者，在开源方案比商业方案好的时候自己会选择开源方案，而在商业方案比开源方案好的时候，自己也不会抗拒。这在当时激怒了许多社区中的开源软件狂热者。 详细阅读：https://www.linuxjournal.com/content/git-origin-story BitKeeper enabled a work method and patch flow which naturally supported the kernel’s development model.BK的设计，与Linux内核的协作开发流十分的契合，解决了Linux开发中愈发严重的scale问题和内核开发者一直以来的痛，内核社区的开发效率得到明显提升（这跟Larry本身是内核社区的一员并且一直和开源社区紧密合作有脱不开的干系）。但是问题在于，BK终究是一个商业软件，背后有一个商业公司和团队要养，这就导致了Larry在授权BK给开源社区的时候，加入了严格的限制。并且由于必须考虑BK的商业利益，Larry需要在「给开源社区使用BK」和「不能让社区出现可能对BK造成威胁的东西」之间做出艰难平衡。许可协议常常出于一些Larry所「认为存在」的威胁为理由而收严（Larry需要保证开源社区不会开发出会让BK式微的替代品）。以至于最后协议被戏称为“别惹毛Larry协议”（”don’t piss off Larry license.”）。最后一根稻草，是协议中一条不允许社区对BK进行反编译的条例，被一个为OSDL（Open Source Development Labs）工作的开发者（Andrew Tridgell，samba和rsync的作者）公然高调违反（社区对这一事情有很两极化的看法，详情可以读这篇文章的评论区）。Larry一气之下决定撤销BK对开源社区的授权。时间是2005年，这一下没了BK，内核社区需要一个替代品作为源代码管理软件，于是有了这个邮件列表里Linus表示决定bite the bullet，找BK的替代品，并且表明自己会尝试一段时间不用BK会是怎么样子（原话“至今为止感觉是晦暗荒芜的”） So I just wanted to say that I’m personally very happy with BK, and with Larry. It didn’t work out, but it sure as hell made a big difference to kernel development. And we’ll work out the temporary problem of having to figure out a set of tools to allow us to continue to do the things that BK allowed us to do.Linus在邮件列表里告诉大家不要去恨BK的公司BitMover，并认可了Larry将商业软件BK和内核开发结合的努力，虽然最后没能促成，但Larry和BK对内核开发的影响是巨大的。后续的历史也证实了BK的影响深远。在讨论了现有源代码管理工具之后，Linus在一封邮件里首次提到了git。一开始的git的目标是一个分布式的代码存档工具，可以commit、pull、push，但是没有考虑merge，因为一开始的定位并不是一个完整的SCM源代码管理工具，而只是一个在不同的开发者之间同步代码历史的方式。git的设计和BK有很多相似之处：如分布式（没有一个“中心仓库”，所有人的副本的地位平等。相比SVN的集中式，Linus在邮件列表中多次提到自己不喜欢集中式的源码管理方式），如不实现cherry-pick以保证历史不变性。文中也提到了可以将git看成更像一个「内容寻址文件系统」（即git的hash object系统，相比键寻址的传统文件系统）。但也有其独特之处，比如相比一个源代码管理系统，git的设计更像是一个低层文件系统；相比保存patch，git保存了完整的内容并动态按需生成patch，这使得git达到了当时其他尝试替代BK的开源替代品（arch, darcs, monotone…）不可企及的速度。“速度”是git的关键，Linux内核代码之庞大和开发者社区之活跃，使得SCM的速度变得至关重要。 https://lkml.org/lkml/2005/4/7/150插曲：不实现cherry-pick这个话题也蛮有意思的，一开始BK没有cherry-pick这件事Linus也是十分讨厌的，Larry说服了他历史只增加而不改变能使复制更简单。 而第二个使得Linus决定说cherry-pick是错误的的理由是一个非技术理由：cherry-picking隐含了一个「“食物链上游”的用户可以修改其“下游”的人所做的工作」的逻辑。因为进行cherry-pick的理由往往是想要修复“别人的错误”，即你不同意的东西。 问题是，这种做法往往导致整个系统里面出现错误的气氛和心理（results in the wrong dynamics and psychology in the system）。首先它假设了“食物链上下游”的存在，Linus认为这是错误的。内核开发正逐步变成一个“网络”，而Linus认为自己相比是“顶部的人”，更不过仅仅是”网络里比较中心的人”而已，而且Linus认为就应该是这样。内核的开发就应该是一个随意的网络而不是一个层级的结构。 另外一个是，支持cherry-picking相当于隐式地将质量控制的职责放到了上游（“我会从你的提交树里面挑出好的东西”），而不支持cherry-picking则使得双方都有义务来保证提交树的干净。至于开发树的干净如何保证，Linus提到不同的人有不同的做法，有人有很多颗树，然后用自己开发的工具合并；有人在开发树上尽情开发，当满意后，把开发树丢掉，重新在主树上提交。（这有点像现在git分支的雏形） 这里可以看出Linus对社区治理的态度。git吸收了很多BK的设计：分布式的仓库，提交，同步，多树（“分支”），合并等等。Linus从1991年以来，第一次暂停了参与Linux的开发，转而投入了git这个新工具的开发上：经过Linus短短几天的开发，git在2005年4月8日迎来了第一个commit（commit message: “git 第一版，地狱而来的内容管理器”），并实现了自己托管自己的源代码。几周内，git便已经有托管linux完整源代码的能力了。// 很Linus风的第一版READMEGIT - the stupid content tracker\"git\" can mean anything, depending on your mood. - random three-letter combination that is pronounceable, and not actually used by any common UNIX command. The fact that it is a mispronounciation of \"get\" may or may not be relevant. - stupid. contemptible and despicable. simple. Take your pick from the dictionary of slang. - \"global information tracker\": you're in a good mood, and it actually works for you. Angels sing, and a light suddenly fills the room. - \"goddamn idiotic truckload of sh*t\": when it breaksThis is a stupid (but extremely fast) directory content manager. Itdoesn't do a whole lot, but what it _does_ do is track directorycontents efficiently. 看git的第一个提交可以看到，git的早期代码极其简单，只有不到一千行的c代码，git的核心设计分为两部分：内容索引的对象数据库，以及当前目录结构缓存，简单有效且快速。第一个git源代码的提交也是通过git自身完成的。从Linux-2.6.12-rc2开始，Linux内核正式转移到了git上托管，后续的历史也证明了基于git协作的简单有效，BK最终还是迎来了它一直以来最担心的东西：一个会威胁到他生存的开源替代品。不过此时BK已经放弃开源社区了，愤世嫉俗地看，也可以说BK在linux社区已经吸收了足够的利益，并决定已经可以离开了罢了。从此git席卷世界的时代开始了。 Author: Miigon（禁止转载）Sources: https://lwn.net/Articles/130746/ https://lkml.org/lkml/2005/4/7/121 https://lkml.iu.edu/hypermail/linux/kernel/0504.0/2022.html https://www.linuxjournal.com/content/git-origin-story https://github.com/git/git https://github.com/torvalds/linux https://en.wikipedia.org/wiki/BitKeeper Sources中的邮件列表链接有远比这篇文章更详细的讨论，已经是众多历史爱好者的考古现场了，本文只是对这些资料的粗糙概括，强烈建议对这段辛酸史感兴趣的朋友读一下Sources中的1，以及2中邮件列表所有Linus Torvalds的邮件。 后记：bitKeeper在2016年选择了以Apache协议开源，并目目前已不再继续维护(来源请求)。截止2020年3月，仅仅GitHub上就已经有超过1.28亿公有git repo。" }, { "title": "记一次 C++ 核心语言标准中一个 issue 的发现和提交经历", "url": "/posts/cpp-core-language-issue-for-enum-const-redefinition/", "categories": "Language, C++", "tags": "c++ standard", "date": "2022-04-02 22:10:00 +0800", "snippet": " 该文章记录自己的一次发现一个 C++ 核心语言标准规定中，关于枚举量重定义的一个规则缺陷（defect）并提交的经历。所有对标准的引用以 N4901 草案为准（当时的较新版本）。C++ 核心语言标准 N4901 草案引言问题本身是关于 enum 中枚举值 (enumerator) 的重复定义问题的。例子如下：enum {\tee, ee};上面的代码，无论是在 gcc/clang 还是 g++/clang++ 上，编译都是不能通过的，报错如下：enumtest.cpp:3:2: error: redefinition of enumerator 'A' A, ^enumtest.cpp:2:2: note: previous definition is here A, ^1 error generated.也就是常用的两个编译器实现上，无论 C 还是 C++ 都不允许枚举值的重复定义（注意区分枚举类型和枚举值）。C99 草案中对该行为作出了明确规定： From the draft of C99, Section 6.7:6.7.5. A declaration specifies the interpretation and attributes of a set of identifiers. A definition of an identifier is a declaration for that identifier that:— for an object, causes storage to be reserved for that object;— for a function, includes the function body; — for an enumeration constant or typedef name, is the (only) declaration of the identifier.enumeration constant 即枚举常量，也就是上述代码中的 ee。C 语言标准的这一规定，就避免了同一个标识符被多次用于定义枚举值。在实际的使用中这一行为也符合逻辑，因为每一个枚举值在未指定具体常数值的情况下，是递增分配整形常数值的，如果允许枚举值 enumerator 同名可能导致一个枚举值名字对应多个常数值，造成歧义。问题按理来说，C++ 在大多数情况下都可以认为是 C 的超集，C 标准明确规定不能通过编译的代码，在 C++ 中应该也不能通过。由于枚举类型定义的时候，会顺带定义其中的所有枚举值，又因为定义是一种特殊的声明，那么 C++ 标准中就必然存在一定的规则，要么阻止枚举量的重复定义，要么阻止枚举量的重复声明，使得上述代码非法。One-definition rule 不阻止枚举量的重复定义出于好奇，查找了一下 C++ 关于这方面的规定，了解到 C++ 中，有一个单独列出的 One-definition rule 条目（6.3 [basic.def.odr]），其中第一条规定如下： No translation unit shall contain more than one definition of any variable, function, class type, enumeration type, template, default argument for a parameter (for a function in a given scope), or default template argument.即：所有的翻译单元都不可以包含多于一个的任何变量、函数、类、枚举类型、模版、参数默认值或默认模版参数的定义。这里特别注意到，One-definition rule 限制了「枚举类型」的重复定义，但是没有限制「枚举量」的重复定义。当然 One-definition rule 相当于是在原来的声明/定义规则上打的一个“补丁”，直接用特殊规则来限制一些种类实体的重复定义。并不代表标准中的其他规则就不会限制重复定义的枚举值的存在（这在后续与委员会的邮件交流中也涉及到了），所以这里没有限制并不足以作为允许枚举量重复定义的充分条件。由于定义是一种特殊的声明，虽然定义 definition 相关的规则没能阻止例子中的代码通过编译，但是仍然有可能在声明 declaration 中阻止了这样重复声明枚举量的情况出现，故继续探寻，发现：declaration 声明规则允许枚举量重复声明两次 ee 声明的是同一实体继续浏览标准中关于声明 declaration 和定义 definition 的章节，可以看到如下描述：6.6 [basic.link] paragraph 8: Two declarations of entities declare the same entity if, considering declarations of unnamed types to introduce their names for linkage purposes, if any (9.2.4, 9.7.1), they correspond (6.4.1), have the same target scope that is not a function or template parameter scope, and either they appear in the same translation unit, or……. （后续几种情况与问题无关，故没有列出） 即两个实体声明（在这里指两次枚举量定义 ee 和 ee，定义也是一种声明）如果它们满足： 相互「对应」（例子满足） 在同一个作用域（例子满足） 且出现在同一个翻译单元（例子满足）则他们声明的是同一个实体。「对应 correspond」的意思，在6.4.1 [basic.scope.scope], paragraph 4 中指出： Two declarations correspond if they (re)introduce the same name, both declare constructors, or both declare destructors, unless……（后面的排除情况比较复杂，但都不适用本文中的例子，故省略）在这里，两个枚举量声明由于引入了同一个名字，所以说它们「对应」。也就是说，他们满足了声明同一个实体的三个条件，两次 ee 声明的是同一实体。 这个规则一般是服务于函数声明、变量声明或者类型声明的，即多次声明同一个函数，声明的其实都是同一个函数： // 例子：此代码是合法C++程序，能通过编译void foobar();// 声明void foobar();// 再次声明void foobar() {} // 再次声明，并定义，前面所有的声明都指向这里定义的实体int main() {\tfoobar();} 但是在这一套规则下，我们一开始例子中的枚举量定义 ee 和 ee 也恰好符合这里的要求，即两次指向同一个实体。两次 ee 声明的是同一实体为什么重要呢？因为声明还有一种「冲突」的情况如下：两次 ee 声明不满足「可能冲突」的条件，不造成冲突6.4.1 [basic.scope.scope]: Two declarations potentially conflict if they correspond and cause their shared name to denote different entities (6.6). The program is ill-formed if, in any scope, a name is bound to two declarations that potentially conflict and one precedes the other (6.5).当两个声明「对应」（满足）并使得他们共有的名字指向两个不同实体时，两个声明即「可能冲突」。而前面一段已经说明了，两次 ee 声明，指向的是同一实体，也就是说这里「可能冲突」的规则并不适用，两次声明不冲突。结论：枚举量重复定义不违反 C++ 标准！由于 One-definition rule 并不限制枚举量的重复定义，而两次枚举量声明指向的是同一个实体，不会造成声明冲突，意味着一开始的示例代码：enum {\tee, ee};其中重复的 ee，无论是从实体定义的角度，还是从实体声明的角度，都不违反 C++ 标准中的规则，也就是说当前 C++ 标准没有像 C 标准一样成功阻止该有歧义的程序通过编译。总结当然，对同一个名字进行多次枚举量定义肯定在逻辑上是错误的，每个枚举量都必须对应「一个」整型常量，每一个枚举量定义又会使得枚举量对应的常量相比上一个枚举量定义增1，允许同个名字定义两次枚举量的话，这两个规则就产生矛盾了。（例如上面的例子里，ee 同时对应 0 和 1）在这里，合理的解决方法应该是将枚举值 (enumerator、enumeration constant) 加入到 One-definition rule 中，阻止枚举值的重复定义。我也将相关的信息提交给了 C++ 标准委员会相关人员，并经过几轮邮件来回解释，该问题已经被接受并成为 C++ 核心语言议题 #2530。应该会在下一次委员会会议中讨论并可能在未来草案中修复。That's a good point that I had overlooked; it seemed clear that two enumerators (that, in general, have different values) cannot declare the same entity, which in the case of an enumerator, is only a value. However, this wording contradicts that reasoning, so you've convinced me. This will be issue 2530 in revision 108 of the core language issues list. -- William M. (Mike) Miller最奇妙的是，这个疏忽虽然看起来很微不足道，但它至少从 C++03 之前就存在了（C++03 标准文档有版权问题，这里链接是2005年的工作稿），一直存在了20来年，期间的编译器作者也都默认按照 C 的规则去处理了，也没有人提交过相关 defect。甚至有理由怀疑是 C++ 标准里存在时间最久的 bug 之一，果然最简单的问题藏在眼皮底下反而不容易被发现 😃 。 我的博客即将同步至腾讯云开发者社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=h98n0pl8nhui" }, { "title": "[SZU] 数据库内核课程 PostgreSQL 12.5 源码安装避坑 guide", "url": "/posts/postgresql-source-compilation-guide/", "categories": "Backend, Database, SZUCourse", "tags": "guide", "date": "2022-03-04 00:08:11 +0800", "snippet": " 课程：2022 年下学期，秦建斌老师的《数据库内核原理与实现》课程。示例环境：Ubuntu 20.04 LTSPostgreSQL 版本： 12.51. 准备 linux 环境/虚拟机/或Windows下使用wslLinux/Mac 用户可直接编译，Windows 用户自行搜索 wsl 教程配置后，剩余流程同 Linux 用户。Mac 用户需要用其他方式（homebrew 或 macports）安装 configure 过程中缺少的包。2. 登陆机器，拷贝 PGDev.zip 到用户文件夹~中使用 ssh/scp，或执行指令现场下载：wget https://raw.fastgit.org/Miigon/pgdev-zip/master/PGDev.zip -O PGDev.zip3. 安装 unzip 和 libreadline-dev：sudo apt install unzip libreadline-dev4. 解压 PGDev.zipunzip PGDev.zip解压完成后，出现 PGDev 文件夹：5. 进入 PGDev 文件夹，创建 pghome 与 data 文件夹，用于存储 PostgreSQL 本身以及 PostgreSQL 所产生的数据cd PGDev/mkdir pghomemkdir data6. 使用 nano (或 vim/gedit 等其他编辑器) 编辑 env-debug 文件： 方向键移动光标 将第一行的 PGHOME=/Users/jqin/PGDev/target-debug 整行改为 PGHOME=~/PGDev/pghome 将第三行的 export PGHOST=$PGDATA 改为 export PGHOST=127.0.0.1修改后：按 Control + X，下方提示 Save modified buffer?，按 Y，出现 File Name To Write: env-debug 直接按回车，保存退出。7. 执行 sourcesource env-debug这一步将前面配置好的路径导入当前 shell 的环境变量中，没有任何输出则表示执行成功8. 执行 configure（autoconf）cd 进入 postgresql-12.5，执行：./configure --prefix=$PGHOME --enable-debug UPDATE: 2022-07 增加 –enable-debug 选项，方便后续 gdb 调试（确保执行时路径正确）如果一开始有装 libreadline-dev，这里应该能执行成功：9. 构建 &amp; 安装执行：make -j4 这里假设机器 4 核心，如果不是可自行更改。写错了也不会影响构建结果等待构建完成，看到这一句代表构建完成：将编译好的 PostgreSQL 安装到 pghome 中：make install安装成功的提示：10. 运行 执行 initdb 初始化数据库： 执行 pg_ctl -D $PGDATA -l $PGDATA/logfile start 启动 PostgreSQL 服务：（注意这里的指令和上图提示的指令不同） 执行 createdb 创建数据库，再执行 psql 进行连接：（这两个指令后面都可带参数来指定数据库名，不带则默认同用户名，建议不带参数。） 此时应该就可以正常使用了：后记 这样安装后，PostgreSQL 本体会在 ~/PGDev/pghome 中（PGHOME），数据会在 ~/PGDev/data 中（PGDATA）。 后续如果对源代码作出修改，只需要在 ~/PGDev/postgresql-12.5 中再次执行 make 与 make install 即可。 建议使用 git 对 postgresql-12.5 文件夹进行版本管理，方便后续修改回退。 如果关闭了当前终端，打开新终端后需要先执行一次 source ~/PGDev/env-debug，否则会提示找不到 psql 等错误。" }, { "title": "[mit6.033] 第二部分 LEC 7-12 Networking 笔记", "url": "/posts/mit6033-lec7-to-12-networking/", "categories": "Course Notes, MIT6.033", "tags": "system design", "date": "2021-12-01 21:31:00 +0800", "snippet": " 这是我自学 MIT6.033 课程的第二部分：Networking 的笔记。该课程共分为 4 部分：Operating Systems、Networking、Distributed Systems、Security 课程详细介绍可以查阅：https://blog.miigon.net/posts/mit6033-computer-system-design/ 笔记为学习过程中随笔记录以及思考，未经整理，所以可能会比较零散。这一部分 Lecture 讲的内容，与计算机网络有一定重合，这里笔记偏向于在系统设计中需要考虑的概念，而至于具体的协议、网络通信的具体细节等，即使在 lecture 视频中提到了，也不会记录到笔记中。这一部分细节可以通过计算机网络课程学习。LEC7 Intro to networking and layeringHow do modules of a system communicate if they’re on separate machines? 网络。网络是一个系统的重要部分，许多系统的错误来源都来自于网络错误。不同机器上模块的通信 - 网络网络：包含 end point，以及将这些 end point 连接在一起的中间节点 switch 以及边 link 形成的图随着系统增长，需要考虑如何将这些链接转化成一个「网络」网络层级： link：两个直接相连的节点之间的通信 例子：以太网、蓝牙、802.11 network: 多个 link 以及节点形成的网络 这一层需要考虑的问题：节点的命名naming、寻址addressing、路由routing 不仅是简单的图最短路问题，网络需要考虑节点的加入、离开，网络不会有一个中心的权威者来规划一切 节点需要完全靠自己，去知道自己的邻居、以及离自己较远的网络的其他地方的情况（分布式的系统） transport: 共享网络；保证可靠（或不保证） 这一层所面对的问题：如何公平地在节点之间共享网络，如何应对网络的不可靠性 application: 实际产生网络流量 因特网历史与演变 这部分的配图：https://web.mit.edu/6.033/www/lec/s07-partial.pdf 早期因特网，Pre-1993 1970s：ARPAnet，起初很小，使用 hosts.txt 对节点进行命名（难以scale，但是在早期节点数少的情况下足够），使用「距离向量路由distance-vector routing」，使用滑动窗口收发。 1978：灵活性与分层结构出现，常见的模型： 应用层：产生网络流量 运输层：可靠运输，带宽共享 网络层-IP：寻址/路由（图上寻径） 链路层：相邻节点间的点到点链路 理想状态下，分层架构使得我们可以方便地更换协议，而不影响其他层 1983：TCP可靠传输。应用不再需要自己实现可靠传输。增长 =&gt; 改变 1978-79: 链路状态路由 link-state routing，EGP（分层的路由方式），更可扩张(scalable)的路由方式。 1982：DNS：更可扩张的命名方式。分布式管理带来的增长（网站管理者可以自己管理自己的DNS 服务器，可自己添加新的主机与命名，即子域名）增长 =&gt; 问题what can go wrong when things get big? 80年代中期：拥塞倒塌 congestion collapse。网络被无用数据包/重传包堵塞。TCP加入拥塞控制机制。 90年代早期：策略路由policy routing。互联网开始商业化，互联网的一些组成部分（如大学、政府机构、大公司）并不想为商业流量提供传输。策略路由（BGP）提供了解决方法。 寻址。按照不同大小的块来分配地址。（A类、B类、C类）=&gt; CIDR 协议英特网，Post-19931993: 商业化。改变停止。“新技术基本上都是由于恐惧或者贪婪的原因才被部署的”现在所需要面临的问题 DDoS：难以避免，互联网设计之初并没有考虑责任性accountability 安全：DNS、BGP等完全没有安全措施，可以说谎。 移动性：根本没人会想到这一点 地址空间枯竭：IPv4 -&gt; IPv6 拥塞控制：也许需要改变反复出现的主题 分层 layering 与分级 hierarchy 可扩张性 scalability：如果确保模块性 modularity 是 6.033 第一部分的主题，则可扩张性则是第二部分的主题。 效率/性能：性能需求影响网络的设计 应用的多样性：有的关心吞吐（如下载）、有的关心延迟（如游戏），有的都在乎，有的需要可靠性，有的不需要……等等。我们需要构建一个可以支撑所有这些需求的网络 端到端争议：互联网的什么功能应该由那些部分来实现？（端点？交换机？etc.）LEC8 Routing在因特网或任何网络上的routing，以及think about how well they scale.路由表routing table 目标：让每个交换机知道，任何两节点之间的最低开销(minimum-cost)路径 cost：可以是任意指标或多指标的综合，如延迟、吞吐、拥塞、流量价格。可动态变化。 路径path：整个从起点A到终点D的完整路径 路由route：当前点A到终点D的路径中，作出的一次hop称为一个route（在A到D的例子中，就是第一步A-&gt;C的这个hop，即route只关心当前节点以及下一节点，不关心整条链路） routing table：包含了指引当前packet前往下个节点的映射表（一般为IP段映射到该router的某个interface，即相邻链路节点）路由协议routing protocol中心化的路由表构建协议无法scale，只考虑分布式，即节点独立获取自己所需要的信息，然后独自构建它们自己的路由表。 Goal: For every node X, after the routing protocol is run, X’s routing table should contain a minimum-cost route to every other reachable node. 节点通过 HELLO 协议探测邻居节点的存在（类似TCP三次握手），该协议不断执行（以检测failure） 节点通过某种advertisement协议，从其他节点提供的信息探测到整个网络的所有其他可达节点 此时每个节点都有一份完整的AS内的网络拓扑结构，可以使用如最短路算法等来计算从自己到每个节点的最短路，并生成路由表（什么IP段应该向什么端口发送）。因为网络可能变化，节点可以加入或离开网络，链路开销也可能随时变化，所以这个过程需要不断运行，以保证节点的路由表反映了最新的网络状况。link state routing (eg. OSPF)节点A在探测到自己的所有邻居后，将尝试使用flooding洪流式公告，尝试将自己的邻居信息告诉网络中的所有其他节点。每一个节点收到A的公告后，都会进一步地转发给自己的邻居节点，所以称作“flooding”，洪水一样地在网络里不断转发（like 广度优先搜索）。等到所有的节点都进行过flooding后，所有节点都知道了所有其他节点的邻居，也就构建出来网络的拓扑结构了。可以进一步在网络上跑最短路算法。flooding的一个副作用是，由于有充分的冗余redundancy，现实中advertisement丢失的可能性很小。但这些冗余也带来了更大的overhead开销。（这是link state routing 无法 scale 到整个因特网级别的主要原因）distance vector routing (eg. RIP)在 link state routing 中，节点计算出了完整的最短路径。但实际上生成路由表，只需要知道最短路径上的下一跳（route路由，路牌/指向）即可。distance vector routing 利用了这一点。节点A在探测到自己的所有邻居后，将自己的所有已知节点以及对应开销（包括邻居与非邻居）告诉自己的相邻节点（约等于把自己的路由表发送出去）（每次只发给所有邻居，不flood，邻居无需进一步转发这个信息）。整合阶段：每一次一个节点收到相邻节点的adversitement，看是否能通过这个节点找到一条通向每一个目标节点dst的更好路径，若能，则更新自己的路由表：为dst采用这一节点作为新的路由（下一hop）。多次运行后，每个节点就都能知道网络中要到达每个点的路由了（只知道路由，也就是最佳的下一跳是谁，而不知道整条链路）由于不依赖flooding来传输信息，所以overhead开销更小。问题：timing matters。每个节点发出公告的时机，尤其在链路失效发生时，会严重影响最终结果。最差情况下一个节点的断连，会导致关于这个节点的路由开销会在两个路由器之间来回广播累加直到达到infinity。（看LEC视频时间戳41:30到48:00）由于failure发生时“累加到infinity”问题不可避免，以及failure的普遍性，一般infinity需要设置为较小的数字以减轻累加到infinity的影响，在实际网络中常常是16或32之类的小数字。这也就意味着，在使用该路由协议的网络中，最长的链路不能超过16或32（infinity），这是限制 distance vector routing scale 到整个因特网的主要因素。what do we do这两种算法都不能scale到因特网的量级，也都不支持策略路由policy routing。那么因特网上实际的路由是如何实现的？小网络内独立建立路由+BGP粘合小网络成为因特网。LEC9 BGP互联网的极速增长 =&gt; 重新设计路由协议以适应规模，以及实行策略路由（policy routing）。[[IP数据包从出发到目的节点全过程 &amp; 路由与路由表 &amp; 自治网络 &amp; BGP]]如何应对在互联网上的scale问题分级路由（routing hierarchy） 互联网被分割为许多自治系统（Autonomous Systems, AS）。自治系统可以是一个大学、一个运营商、政府分支等等。每个AS都有自己独特的ID。互联网目前共有上万个这样的AS。 AS之内的路由以及AS与AS之间的路由采用不同的路由协议。 在每一个自治网络AS的边缘，有设备可以在这两种不同的协议之间相互“翻译”。 AS之间使用的路由指向协议是BGPpath vector routing 和 distance vector routing 相似，但是advertisement中包含了整个完整路径。overhead增大，但仍然比link state routing要小。汇聚时间减小（由于整合的时候有完整路径，能避免routing loop，可以避免在出现节点故障时某些路由来回广播直到infinity的问题）distance vector routing 和 link state routing 的折中。overhead开销介于两者之间BGP 是一个 path vector routing 协议。拓扑寻址（topological addressing） 即使在不同AS之间，BGP依然路由到IP地址（实际上是地址段） 为AS分配连续的地址段（CIDR表示法，192.168.1.0/24格式），使得BGP可以用地址段作为路由目的地，保持每个advertisement相对较小（相比为AS内的每一个IP地址都advertise一个路径而言）policy routingAS想要实现“策略路由”，交换机基于人类设置的策略进行决策。方式：选择性公告。只在能获得经济利益的前提下才对一个节点公告一条线路。关于这个话题的更多阅读资料：https://web.mit.edu/6.033/www/papers/InterdomainRouting.pdf(see LEC9 21:30)常见的AS关系 客户/运营商：客户付费获得网络服务（home） peers：互相提供给对方自己路由表的子集，增加互联。互利行为。（比如电信和移动网的互联。早期互联网的peer互联比较匮乏，稳定性也比较差，所以游戏会有不同线路的服务器）BGPBGP 是一个应用层协议 application layer protocol。BGP 策略一般由人设置，所以经常可以看到误配置导致的大规模中断（如2021年10月的 facebook outage）BGP basically relies on the honor system（BGP并不安全，BGP spoofing，AS对 BGP 公告的绝对信任）（2008年巴基斯坦ISP公告伪造的Youtube线路导致世界Youtube断连）LEC10 Reliable Transport and Congestion Control (TCP) 目标：提供可靠传输，防止拥塞 更大的问题：我们如何可 scale 地实现这个目的？如何将网络高效而又公平的共享？基于滑动窗口的可靠传输 目标：接收程序能接收到一个完整的、无重复的有序字节流。即有序的每个packet包的一个拷贝。 为什么需要做？网络不可靠。包可能丢失，也可能乱序。TCP：端到端协议，smart端系统，dumb交换机（交换机只负责在拥塞时进行丢包，而不会提供任何帮助）（LEC接下来的内容是演示了TCP滑动窗口的工作示例以及拥塞控制，具体细节不展开，可参考计算机网络课程）拥塞控制：通过控制滑动窗口大小，方式：AIMD（线性增加，倍数回退）额外机制 slow start（实际上是快启动，启动阶段指数放大window size，直到出现丢包为止开始回退。相比一开始就跑大窗口来说是“slow start”） 快速重传/快速恢复（有时通过重复ACK可以直接推断某个包丢失，则可以直接开始重传而不需要等待超时）回顾 TCP 是巨大的成功，不需要对互联网的基础架构做任何更改。端点可自由参与使用，使得网络可以在大量用户之间分布式地共享。 BUT: TCP 只能相应已经正在发生的拥塞。有更好的办法吗？LEC11 In-network Resource Management网络链路共享，but with 交换机的帮助拥塞控制：需要一个cue作为backoff的信号，在TCP中这个信号是「丢包」。也就是TCP必须等到出现问题之后才会开始相应。交换机中的队列如何决定何时丢包丢尾DropTail最基本的队列管理策略，如果包到达时队列已满，则丢弃；否则则将其加入队列。主动丢包概率p=(queue_full ? 1 : 0); 优点：简单 缺点：来源有可能会被「同步」（即同时检测到丢包，同时回退，导致链路从过载突然变成欠载，导致链路利用率下降）RED（Random Early Drop，随机提前丢包）主动队列管理（active queue management）方案，在队列未完全满之前就开始随机丢包，以提前给TCP发送「拥塞正在形成」的信号。主动丢包概率与一段时间内的平均队列长度正相关。平均队列长度计算公式：q_avg = a*q_instant + (1-a)*q_avg ; 0 &lt; a &lt;&lt; 1（低通滤波，消除高频波动。q_instant是采样瞬间的队列长度，越往后的采样点贡献越大）优点： 队列波动减小 由于拥塞而造成的丢包率变化更加平稳 数据流去同步（分散开始回退，避免同时回退）可能的缺点：依然依赖丢包，源头依然需要重新传输。ECN（Explicit Congestion Notification，显式拥塞通知）与RED类似，但相比丢包，只在包头上做标记。发送者通过ACK包上的标记来检测拥塞。ECN的有效施行，要求发送者对标记做出相应。主动队列管理的问题这些主动队列管理模式的一个问题是，相比被动拥塞控制，它们更加复杂。包含了许多与网络状态相关的需要设置的参数，而网络是不断变化的。这些参数如果设置不当，有时候会使得效果更差。流量区分将不同类型的流量放到不同的队列中，并在队列上做特殊处理。（流量调度）流量调度基于延迟的调度多级优先级队列：不同优先级拥有单独的队列，只有高优先级的队列为空时才会开始发送低优先级的队列。（调度问题）问题：大量的高优先级流量（如游戏流量）会 starve out 低优先级流量（如email）基于带宽的调度为每一个队列分配一定量的带宽轮询、带权轮询（Weighted Round-robin）轮流准许每个队列发送数据。带权轮询可以用于手动指定优先级（或者在按包轮询的实现中，可以使用权来将其转换为等数据量）。（细节略，LEC11 33:30）缺点：如果需要实现按数据量分配的话，需要提前知道或者测量计算出某队列的平均包大小，才能生成适合的权用于确定每轮每个队列应该传输多少个包。deficit round-robin赤字轮询不同的队列可以积累“积分”用于下一轮的发送。每一轮：for each queue q:\tq.credit += q.quantum // 每轮累积一定数量的积分\twhile q.credit &gt;= size of next packet p: // 当积分足够发送下一个包时\t\tq.credit -= size of p // 消耗积分\t\tsend p // 发送包ps. 为每个队列加上最大令牌数量限制，就几乎是令牌桶算法了好处：能实现限流以及几乎绝对公平的带宽分配，不需要提前知道平均包大小。且允许变化的包大小以及一部分的突发流量（靠积分累积）。讨论 流量区分：是一个好主意吗？理论上是。但是： 难以确定合适的隔离粒度（按app？按flow？） 按app流量区分同时需要深层的包内容监测。昂贵、受加密通信阻挠。 按flow=状态过多 公平队列：「哪些流量重要，哪些流量不重要」应该由谁来决定？gov？isp？you？ 这里涉及到了一个很有意思的话题叫做Net neutrality互联网中立性原则。引用维基百科：要求互联网服务供应商及政府应平等处理所有互联网上的资料，不差别对待或依不同用户、内容、网站、平台、应用、接取设备类型或通信模式而差别收费。其中有包含 dumb pipe 和 traffic shaping 的讨论，以及正反双方在“所有网络流量地位平等”这一问题上的立场，是有意思而且很有现实影响的一个话题。Who should make decisions? Should we allow traffic to be prioritized at all? LEC12 Application Layer, P2P Networks + CDNs在因特网上分发内容content delivery。（最早期的互联网应用！）文件分发的可选方案 Client/Server eg：FTP/HTTP 缺点：单点故障（single point of failure）；不能 scale。 CDNs 买多个服务器，放到客户附近降低延迟（分布式） 没有单点故障，更好的 scale P2P点对点网络 极端地分布式架构 理论上无限 scale（从上到下越来越分布式） P2P 基础（以 BitTorrent 为例） .torrent 种子文件，包含关于文件的元信息（文件名，长度，构成文件的各部分的信息，tracker服务器的 URL） tracker服务器，知道所有与该文件传输有关的peer的信息的一个服务器 下载流程：联系tracker，tracker发送参与文件传输的所有peer。连接到这些peer，开始传输数据块。 一部分peer是做种者（seeder）：已经有整个文件。可能是服务器或已经下好文件的下载者。什么激励用户进行上传，而不是只下载？ 高层面的：用户在上传给另一个用户之前不可以从对方下载数据。 peer想要互利关系：A必须有B要的块，反之亦然。 协议分成了轮：第n轮的时候，一些peer上传块给 X。在第n+1轮的时候，peer X 会发送块给第n轮中上传块最多的peer。 peers如何开始整个流程（避免鸡蛋问题）？每个peer保留了一小部分可以免费分发的带宽。问题1：tracker服务器可能成为「单点故障」大多数的BT客户端现在是“无tracker”的。改用分布式哈希表Distributed Hash Table（DHT）。问题2：网络容量这样的网络的容量受限于用户的上传速度，而用户上传速度通常远小于用户的下载速度。CDN 基础CDN 可以帮助我们解决容量问题。一个公司拥有CDN网络，其他公司付费使用CDN将内容传输给用户。围绕世界设置上千服务器，用户通常从“最优”服务器下载（一般是距离它们很近的一个）overlay network问题： 在哪里设置这些服务器？ 如何保持内容最新（6.033下一部分的话题） 如何将客户引导到“正确”的服务器？（eg. 利用DNS的indirection） 错误处理？如何将用户从故障的服务器引导到其他服务器" }, { "title": "[mit6.033] 第一部分 LEC 1-6 Operating Systems 笔记", "url": "/posts/mit6033-lec1-to-6-operating-systems/", "categories": "Course Notes, MIT6.033", "tags": "system design", "date": "2021-11-11 04:45:00 +0800", "snippet": " 这是我自学 MIT6.033 课程的第一部分：Operating Systems 的笔记。该课程共分为 4 部分：Operating Systems、Networking、Distributed Systems、Security 课程详细介绍可以查阅：https://blog.miigon.net/posts/mit6033-computer-system-design/ 笔记为学习过程中随笔记录以及思考，未经整理，所以可能会比较零散。LEC1 Enforced Modularity via Client/Server Organization复杂度复杂度使得构建系统变得困难。使用模块化和抽象的方式，来解决复杂度。 模块化：模块化的系统更容易理解、管理、改变、改进 模块化减少了「命运共享」(fate-sharing) 抽象使我们可以不指定具体实现的情况下进行交互（隐藏细节） 好的抽象设计应当减少模块之间的连接（低耦合）「强制实施(Enforced)」的模块性 「软模块化」并不足够（eg. 面向对象，但是运行在同一程序内，即错误或崩溃会在模块间传播） 一种强制实施模块化的方法是客户端/服务器模型（使得模块可以运行在物理隔离的机器上。或者多进程，此时模块化与错误隔离能力由操作系统提供。）系统设计的其他目标 除了降低复杂度，我们可能还希望有：可伸缩性（scalability）、容错性（fault-tolerance）、安全性（security）、性能（performance）等。 好的模块化设计可以帮助我们实现这些特性。 难以全部得到，多个之间有取舍。LEC2 Naming对模块的「命名」使得模块之间可以互相交互。使用命名的好处 方便检索 共享（多个用户访问同一个名称） 用户友好 寻径（eg.IP地址、域名） 隐藏具体细节（以及访问控制） Indirection（间接，系统可以更换一个名字所指向的具体对象，而不需改动或通知客户端）命名的 scheme 体现了我们希望系统通过命名来得到什么功能。（addressing, indirection, etc）Naming Schemes namespace: 所有可能的名称集合 names 所有可能的值集合 values 从 name -&gt; values 的映射/查找算法（name resolution）需要思考的问题： 名称的语法是什么？ 名称是否含有内部结构？（例如IP地址、域名） 名称是全局名称还是依赖上下文的局部名称？ 系统的那一部分有权利来做「将一个名称绑定到一个值」这件事？（eg. nameserver） 一个名称可以对应多个值吗？（负载均衡） 名字对应关系是否可以随时间而改变？ 名称解析在哪里发生？取舍：eg. 分布性、可伸缩性、委托（delegation）Example: DNSbenefit（与上面的需要思考的问题对应）: 用户友好 负载均衡（一个名称对应多个值） 单机多网站（一个值对应多个名称） 名字对应关系可以随时间而改变bad design 1初期，在每一台主机都维护了一份 hosts 映射表，当新的主机加入网络的时候，新主机向所有现有主机发送更新：问题：难以 scale，一旦主机多起来，难以使所有机器上的映射表保持最新。（也许还有 proofing attack 风险？）bad design 2用中央的服务器维护唯一的一份 hosts 映射表，每次其他主机访问域名时，都与中央服务器请求对应的 ip问题：解决了更新一致性问题，但是依然有 scale 问题，中央服务器需要处理成吨的请求分布式的解决方法（DNS）DNS 中，没有任何一个机器包含所有的映射关系。映射关系被分布在许多不同的机器上。新问题：如何知道要访问的域名的映射关系存在哪一台机器上？方案：DNS 名称的「结构本身」思想：分布式（distributed）+分层架构（hierarchical）apple、google、mit 等自己管理自己的名称服务器（nameserver）。顶级域名 com、net、edu 管理的名称服务器只解析 apple.com、mit.edu 这种二级域名以及它们的名称服务器，而类似 eecs.mit.edu 这种三级域名的解析，由 mit.edu 自己的名称服务器提供。请求解析的时候，向根服务器发出请求，请求一层层代理委托至最终的名称服务器，得到 ip 地址。解析 blog.miigon.net 的过程：root nameserver-&gt; NS of net is at 192.14.171.193net nameserver (192.14.171.193)-&gt; NS of miigon.net is at 58.247.212.48miigon.net nameserver (58.247.212.48)-&gt; blog.miigon.net A-record 185.199.108.153blog.miigon.net is at 185.199.108.153每一级向上一级/下一级请求的过程，如果由客户端完成，则为 iterative DNS，如果由每一级服务器代为完成，则为 recursive DNS。 root nameserver 的地址直接硬编码在机器中。 每一层解析，更接近最终的结果：这里体现了 delegation 委托，即根服务器不能直接给出查询结果，但是可以将请求委托给下一级名称服务器去完成。 类比：文件系统路径的分层结构对比：对象存储服务，所有文件路径都只是扁平的键，没有分层结构（体现的是另一种设计选择与取舍：低复杂度，高访问速度，但无法做目录遍历，也就是难以实现类似 ls 指令的功能）依然有问题： 性能问题：解析需要许多请求，尤其是对根服务器 可靠性问题：名称服务器失效或（安全问题）被攻击总结 「模块化」与「抽象」能降低复杂度。 「客户/服务器模型」使得我们可以通过将模块放在物理隔离的机器上，从而增加模块性 「命名」使得模块之间可以交互，同时提供了如间接性、用户友好等其他特性 DNS 是一个很好的命名实例，同时展示了如分层（hierarchy）、可伸缩（scalability）、代理委托（delegation）、去中心化（decentralization）REC3: DNS 递归（recursive）请求的好处是什么？ 相比 iterative，简化客户端设计，只需要请求一个名称服务器，名称服务器代为完成整个请求过程。 容易实现 cache，如果请求的某一级服务器已经有完整记录，则可直接返回，而对于客户端来说是无感知的。 DNS 分层架构的好处是什么？有什么缺点吗？ 每一层只需管理/维护该层的映射关系，将工作量分摊到不同的层上的不同主机，避免了单主机处理所有请求 由每个名称服务器管理自己的域内的名称，无需担心更新时需要向某个中央服务器汇报导致难以 scale 的问题 依然不是完全分布式，依然有 single point of faliure（根服务器） 应该由谁控制 DNS 根服务器？LEC3 Virtual Memory操作系统操作系统通过「虚拟化（Virtualization）」和「抽象（Abstraction）」，保证单机上的模块性为了实现单机器上的模块性，操作系统需要实现的目标： 程序不应该可以访问和修改其他程序的内存（隔离） =&gt; virtual memory（今天话题） 程序之间应该可以互相通信（通信机制） 程序应该可以和其他程序共享 CPU主要实现这些目标的手段：虚拟化操作系统对硬件的不同部分进行虚拟化，让程序以为自己访问的是完整的物理硬件。虚拟内存让程序寻址整个地址空间，MMU 负责将虚拟地址转换为物理地址。 Virtual Memory 也是一种 Naming Scheme，为我们提供了隐藏（隐藏其他程序的内存）、间接性（虚拟地址指向的实际地址可以随时改变，而不需要显式地告知程序）、访问控制（页表控制位，R/W）等特性。Ideas 储存所有物理地址对应关系，使用整个虚拟地址作为 index 问题：表太大 页表：分页映射，虚拟地址前 p 位作为页 index，后 q 位作为页内偏移 体积小很多 虚拟内存是否真的能阻止程序访问其他程序的内存？（只有系统可以修改页表，+权限位，当程序尝试破坏这一模块性的时候，系统捕获exception，防止发生） 在这里会出现什么性能问题？（每次内存访问都有查表开销，缓解：TLB 快表作为 cache，潜在问题 consistency）详细见S081。多级页表（Hierarchical）单级页表的设计依然需要使用许多空间，解决方法：多级页表，每一级存储指向下一级的地址，仅在使用到时分配。tradeoff： 优点：更离散化的存储（利用零散内存页），占用更小空间 缺点：查询速度（多级查找而不是一级），更多的缺页异常抽象（abstraction）对于不可虚拟化的资源（磁盘、网络）（ps其实还是可以虚拟化的hhhhhh），系统提供抽象（系统调用），使得这些东西更加可移植。LEC4 Bounded buffers and locks操作系统（again）操作系统通过「虚拟化（Virtualization）」和「抽象（Abstraction）」，保证单机上的模块性为了实现单机器上的模块性，操作系统需要实现的目标： 程序不应该可以访问和修改其他程序的内存（隔离） =&gt; virtual memory 程序之间应该可以互相通信（通信机制） =&gt; 有界缓冲区与锁同步（今日话题） 程序应该可以和其他程序共享 CPUBounded buffers &amp; locks (concurrency)例子：unix pipe（consumer/producer）考虑 consumer/producer 问题，如果可以用一个整数数值来描述资源剩余数量，比如剩余的 bounded buffers 空间，则可以用「信号量Semaphore」来同步。如果资源可用条件较为复杂，无法使用整数描述，则应该使用「条件变量Condition variable」来同步（利用其较高的自由度，自定义唤醒条件）[[唤醒丢失问题 &amp; 条件变量 vs 信号量 the lost wakeup problem &amp; condition variable vs semaphore]]本质：都是利用互斥锁同步。Atomic actions原子操作的粒度？ 过粗：性能变低（根本原因是并行性受到了限制，体现为“锁竞争严重”） 过细：意外行为tip：将锁的作用理解为「保护不变量」。在不变量被暂时破坏时保护数据结构，以不被外部察觉。在释放锁之前，operation 应该恢复该不变量。[[锁的作用 Ways to think about what locks achieve]]例子：文件系统中的锁文件移动： 粗粒度锁：锁整个文件系统，问题：性能低，不能同时移动两个文件 细粒度锁：更好的性能，但是更难正确 例子：将 file1.txt 从 A 移动到 B，同时将 file2.txt 从 B 移动到 A （死锁，拿A锁等B锁，拿B锁等A锁） 例子解决方法：确保获取锁顺序，阻止环路形成。（例如强行要求先拿小inode号的目录的锁） 结论：一些系统中的锁问题以及锁设计，需要有全局观地去思考锁与锁之间的交互，而不能只考虑锁对局部代码的影响。LEC5 Threads操作系统（again again）操作系统通过「虚拟化（Virtualization）」和「抽象（Abstraction）」，保证单机上的模块性为了实现单机器上的模块性，操作系统需要实现的目标： 程序不应该可以访问和修改其他程序的内存（隔离） =&gt; virtual memory 程序之间应该可以互相通信（通信机制） =&gt; bounded buffers &amp; locks 程序应该可以和其他程序共享 CPU =&gt; 虚拟化CPU（抢占式多线程 threads）线程thread：一个虚拟的处理器（包装了一个程序的运行状态）线程切换：将当前程序的运行状态换出，将另一个程序的运行状态换入（suspend/resume，或context switching） 线程需要包含的信息：所有的寄存器、所有的内存（利用页表切换实现） 何时挂起/恢复一个线程？（定期，yield()）线程主动放弃CPU：yield()放弃当前线程的CPU执行权，保存当前线程的信息（栈、页表寄存器、callee-saved registers，不需要保存pc，因为在栈中的return address已经有），找到另一个可以运行的线程，并恢复新线程的运行状态（恢复栈指针、页表、callee-saved registers，ret指令返回到新线程之前yield后的位置）这一部分在s081中有更详细展开：https://blog.miigon.net/posts/s081-lab7-multithreading/在 bounded buffer 的 send 遇到缓冲区不足时，进程调用 yield 来主动放弃 CPU，但是进程可能在缓冲区出现空闲之前被再次调度唤醒，然后立刻又进入睡眠。我们希望能够实现「yield，然后直到 buffer 中有空闲位置之前都不要再次唤醒该线程」（减小无意义唤醒）——操作系统提供条件变量机制来实现这一功能（condition variable）条件变量（condition variable）程序可以等待（wait）一个条件（condition）的发生，并且在该条件发生的时候收到通知。另一进程可以在一个条件变为真的时候，使用 notify() 通知所有正在等待该条件的进程。more on [[唤醒丢失问题 &amp; 条件变量 vs 信号量 the lost wakeup problem &amp; condition variable vs semaphore]]（https://blog.miigon.net/posts/the-lost-wakeup-problem-cond-var-vs-semaphore/）问题：lost wakeup 唤醒丢失进程 A 在完成 release(bb.lock) 之后，但是进入 wait(bb.has_space) 之前，另一个进程 B 可能会执行 receive，从而尝试唤醒正在等待 has_space 条件变量的进程。而此时唤醒的进程中不会包括进程 A。这时候进程 A 继续执行，然后才进入到 wait(bb.has_space) 中，而不会收到来自进程 B 的唤醒。从而，进程 A “丢失” 了进程 B 发出的这一次唤醒。唤醒丢失的后果轻者可能是使得进程 A 需要等待下一次条件为真时才能被唤醒，对于一些十分关键的条件变量，A 可能永远都不会再收到通知，从而陷入永久的沉睡。解决方法：wait() 进入等待的同时，原子性地放弃锁。在被唤醒之后，原子性地重新获得锁，然后再从 wait 返回到用户代码。即：# pseudo-codedef wait(cond_var, mutex):\trelease(mutex)\tcurrentthread.state = WAITING\tcurrentthread.waiting_on = cond_var\tyield_wait()\tacquire(mutex)注意 wait 整个操作必须是原子的，这一原子性可以通过给 wait 加一个额外的锁 tlock 实现。问题：yield_wait 死锁这里的 yield_wait 有一个很细微的死锁问题：这里寻找可运行的线程的循环，由于 yield_wait 被调用时持有 t_lock，假设当所有其他线程都处于等待/不可运行状态，则该循环不会找到可运行的线程，并且更糟糕的是由于该循环执行时持有 t_lock，而其他 CPU 需要首先进行 yield 才能运行线程，而 yield 所要求的 t_lock 被 yield_wait 所在 CPU 持有着，则其他 CPU 无法推进任何线程的状态，该循环永远找不到可运行线程，整个系统进入死锁。这种情况在普通的 yield 中是不会出现的，因为普通的 yield 无论如何都至少会有切换前的原线程是 RUNNABLE 的，所以循环必然会终止（而对于 yield_wait 来说不是这样）。这里是一个 CPU 持有 t_lock 去等待「有可运行线程出现」这一条件，而可运行的线程在出现前又需要获取 t_lock，造成环路等待。解决方法比较简单，循环中需要间断性主动放弃 t_lock，破坏环路等待，使得其他 CPU 可以推进，从而使得「可运行线程出现」，解除死锁。问题：两个 CPU 共用同一线程栈导致栈损坏这里还有一个小细节，由于 yield_wait 此时使用的还是原线程的栈，此时放弃 t_lock 允许其他线程执行的时候，其他 CPU 可能会执行到该线程，而此时 yield_wait 的循环依然可能需要用到栈。从而导致有两个 CPU 使用同一个线程的栈，导致栈内容损坏。这一情况比较稀有，但是依然可能出现，解决方法就是在进入 yield_wait 循环之前，将栈切换到其他地方，比如 CPU-specific 的栈，从而避免两个 CPU 共用一个线程栈。如果线程不主动放弃 CPU 呢？抢占（preemption）。使用时钟中断，定期中断 CPU 执行流，并强行调用 yield。def timer_interrupt():\tpush PC\tpush ALL registers \t\tyield() # will further save stack pointer,\t\t\t# page table register and callee-saved registers\t# since this is an interrupt, must restore all registers,\t# instead of just the callee-saved ones.\tpop ALL registers\tpop pc总结 「线程」虚拟化了 CPU，使得一个 CPU 可以在多个程序之间共享。 「条件变量」为线程提供了一个更高效的等待其他线程事件发生的 API 「抢占」使得一个进程能被中断并强制其放弃CPU，使得不需要依赖程序员主动调用 yield 来放弃 CPU（抢占式多线程以及协作式多线程） 操作系统中为用户程序提供模块化的三个不同方面：内存、通信、CPU；机制看起来不同，但都是虚拟化思想的应用（虚拟化环境，使得上层应用/模块可以运行在自己独立的环境中而与其他模块互不干扰）。 单机上的模块化（模块之间互相隔离）的保证，需要来自操作系统以及硬件的支持。 我们成功的利用操作系统的虚拟内存机制、通信机制、虚拟化CPU机制，保证了单机上的模块性。LEC6 OS structure, Virtual Machines另一种 virtualization 的应用：虚拟机目标：在同一机器上同时运行多个系统限制：兼容性。因为我们不希望需要修改内核的代码（否则称为半虚拟化）。操作系统考虑的是如何将程序与程序之间隔离。虚拟机的任务是退后一步，也就是如何将操作系统与操作系统之间隔离。内核中负责运行虚拟机的模块，称为 virtual machine monitor VMM（Linux kernel 中的实现为 KVM，windows 下为 hyperv）。VMM 需要负责分配资源以及分发事件，这里重点考虑如何处理客户系统中执行的，需要与实际硬件交互的指令。虚拟化 CPU尝试方案1：模拟每一条客户系统执行的指令。问题：慢（内存比寄存器要慢100倍左右，仅仅是用内存去模拟寄存器这一步就会将性能降低两个数量级）尝试方案2：用户系统直接在真实 CPU 上运行指令。问题：特权指令的处理VMM 负责处理特权指令，当子系统执行特权指令的时候，会引发宿主系统的中断，并允许宿主系统处理这一特权指令。方式： 通过将客户系统运行在用户态使得特权操作触发中断（trap-and-emulate）。但是有一些特权操作（如写入 U/K 位切换用户/内核态）本身就不会触发中断，需要修改客户系统或进行指令替换； 通过硬件提供的支持，如 Intel VT-x虚拟化内存 方法1：将客户系统的页表内存设为只读，捕获每一个客户系统对页表的修改，并将其与宿主机的页表结合，生成一个合并页表，作为 CPU 实际使用的页表。 方法2：现代 CPU 含有对虚拟化的支持，CPU 同时知道客户系统以及宿主系统的页表的存在，并会在客户系统尝试访问内存时，依次查询两个页表。（拓展：嵌套虚拟化，CPU 会需要知道并查找 n 层页表）内核架构宏内核： Linux 内核，内核内部没有模块化与隔离，一个bug可以使整个系统崩溃。 由于复杂度，bug 容易出现微内核： 将子系统——文件系统、设备驱动——运行为用户态程序。 更好的模块性，更少的 bug，bug 更不容易将整个系统带垮。 模块间接口设计更复杂，模块间通信性能可能受影响。性能一个选择宏内核而不是微内核的理由是性能。测量性能的指标：吞吐量 throughput、延迟 latency。提升性能的通用手段：批处理 batching、缓存 caching、并发 concurrency、调度 scheduling会在后续的网络章节继续讲延迟与吞吐的话题。6.033 重点在于讨论系统设计中「通用的提升性能手段」，而不是如复杂度优化这一类场景相关的具体优化手段。总结在 Operating System 的 Lecture 结束后，已经理解了系统中单台机器上的模块化以及工作。后续讨论多机系统中机器之间互相交流必不可少的部分：网络。" }, { "title": "MIT6.033 Computer System Design | 计算机系统设计", "url": "/posts/mit6033-computer-system-design/", "categories": "Course Notes, MIT6.033", "tags": "system design, course recommendation", "date": "2021-11-01 02:19:00 +0800", "snippet": "MIT6.033 Computer System Design (Spring 2021) 前置课程： 6.004 Computation Structures 计算架构 6.006 Introduction to Algorithms 算法导论 ps. 本课 6.033 Computer System Design 和 6.S081 Operating System Engineering 共同作为 6.824 Distributed Systems 的前置课。课程介绍MIT6.033 Computer System Design 是麻省理工学院计算机科学本科的必修课程，课程涵盖了计算机软件系统与硬件系统的工程设计：控制复杂度的技巧、利用客户-服务设计保证强模块性、操作系统、性能、网络、命名、安全与隐私、容错系统、原子性以及并发活动协调、错误恢复等。MIT 6.033 包含了四个单元的技术内容：操作系统、网络、分布式系统、安全。来自文献所提供的现实系统的研究，为系统设计学习提供了参照与对比。课程包含一个学期长的合作设计项目（design project），学生参与全面的沟通交流能力练习。课程目标使得学生能设计自己的分布式系统来解决现实世界中的问题。论证自己的设计选择也是设计分布式系统能力的一部分。这一主要目标由其他这些目标所支撑： 学生将能分析并评价现有系统，以及自己的系统设计。作为该目标的一部分，学生将学会辨别现有系统中的设计选择。 学生将可以将课程中学习到的技术知识应用到新的系统组件中。这意味着需要学会辨别与描述： 计算机系统中，如何使用常见的设计模式（例如抽象与模块化）来限制复杂度。 操作系统如何使用虚拟化与抽象来确保模块性。 互联网是如何为大容量、多元化应用、互相竞争的经济利益而设计 如何在不可靠的网络上构建可靠、可用的分布式系统 计算机系统安全的常见陷阱，以及如何应对它们 课程架构LecturesLectures 将教学生设计他们自己的计算机系统所需要的技术细节，并将这些细节放在更大的图景中：包括具体领域系统的图景以及作为一个系统总体的图景。RecitationsRecitations 给学生一个练习系统分析与口语交流技能的机会。每一次 Recitation 围绕着一个具体的关于系统的 paper 展开。通过读这些 paper，学生能够更好的体会该领域内的沟通交流是如何完成的。Recitation 是基于讨论的，学生得以练习分析、评价以及交流系统的能力 Note: 这一部分是 MIT 学生每周必须要做的论文阅读复述与讨论环节，旨在锻炼学生的口语表达能力以及描述系统设计的能力。个人觉得这个是十分重要的技能，特别是在团队/开源社区等合作开发环境下，能够准确、清晰且简洁地将自己的系统设计、设计选择与取舍讲清楚，是十分重要的一项能力。（避免在沟通上内耗）这一部分内容可以作为扩展阅读使用，重点不仅是看一些实际系统是如何设计的，还要看作者是如何把自己的系统设计选择向别人讲清楚的。Writing Tutorials这些教程将教你关于这门课的沟通的理论以及实践，并帮助你为作业（尤其是 design project）做好准备。你将会流畅掌握不同的沟通流派，培养将技术概念展现给不同听众所需要的策略与技能，学习如何使用写作来培养以及加深你的技术理解。 Note: 主要面向 MIT 学生需要团队完成的学期大作业 design project。该作业要求学生三人组队设计一个自己的系统以解决一个现实世界中的问题。现实世界中的系统都不是由一个人独立构建的，总是团队协作的成果。design project 包含报告、演示、peer review 等环节，感兴趣的可以前往 Design Project 页面 查看。课程资源课程网站：https://web.mit.edu/6.033/2021/wwwdocs/课本：https://ocw.mit.edu/resources/res-6-004-principles-of-computer-system-design-an-introduction-spring-2009/ 很可惜，据我所知目前还没有中文课程翻译。下面整理的内容已经将非 MIT 学生无法访问的 assignment 以及 design project 的内容去掉了。只留下 lecture、recitation和 writing tutorials。LEC: lecturesREC: recitationsTUT: writing tutorialsOperating Systems 操作系统如何在「单主机」的范围内施行模块化。LecturesLEC 1: Enforced Modularity via Client/server OrganizationLEC 2: NamingLEC 3: Virtual memoryLEC 4: Bounded buffers and locksLEC 5: ThreadsLEC 6: OS structure, Virtual MachinesRecitationsREC 1: Introduction to 6.033 RecitationsREC 2: We Did Nothing WrongREC 3: DNSREC 4: UNIXREC 5: UNIXREC 6: DP DiscussionNetworking 多机系统的机器之间数据与请求是如何交换的。如何放大规模。LecturesLEC 7: Intro to networking and layering LEC 8: Network Layer: RoutingLEC 9: BGPLEC 10: Transport Layer: TCPLEC 11: In-network Resource ManagementLEC 12: Application LayerLEC 13: CanceledEXAM: MidtermRecitationsREC 7: EthernetREC 8: EncapsulationREC 9: Overlay Networks (RON)REC 10: Performance, Measurement, and Evaluation (RON)REC 11: DCTCPREC 12: End-to-end ArgumentsREC 13: CanceledDistributed Systems 通过网络连接的分布式系统中可能会遇到什么问题/错误，如何应对。LecturesLEC 14: ReliabilityLEC 15: TransactionsLEC 16: LoggingLEC 17: IsolationLEC 18: Distributed TransactionsLEC 19: ReplicationRecitationsREC 14: GFSREC 15: MapReduceREC 16: ZFSREC 17: CanceledREC 18: Consistency GuaranteesREC 19: RaftSecurityLecturesLEC 20: Intro to Security + AuthenticationLEC 21: Low-level attacksLEC 22: Secure ChannelsLEC 23: ToRLEC 24: Network AttacksLEC 25: Wrap-upEXAM: Final examRecitationsREC 20: RaftREC 21: MeltdownREC 22: DNSSECREC 23: CanceledREC 24: MiraiWriting TutorialsTUT 1: Intro to 6.033 CommunicationTUT 2: Consensus and Reasoning About SystemsTUT 3: Reading for Systems ConceptsTUT 4: Collaboration and Collaborative WritingTUT 5: Visual Design, Figures, and DiagramsTUT 6: Assembling the DPPRTUT 7: DP PresentationTUT 8: Responding to Feedback TUT 9: Analysis and EvaluationTUT 10: Peer ReviewTUT 11: Final DP Report" }, { "title": "杂记随笔：唤醒丢失问题 & 条件变量 vs 信号量", "url": "/posts/the-lost-wakeup-problem-cond-var-vs-semaphore/", "categories": "Misc, Notes", "tags": "operating system", "date": "2021-10-30 08:38:00 +0800", "snippet": "Scenario考虑 producer-consumer 同步模式中的 receiver:receive(bb):\tacquire(bb.lock)\twhile bb.out &gt;= bb.in:\t\trelease(bb.lock)\t\t# release lock before sleep\t\t\t\t\t\t\t\t# so other threads can run\t\twait(bb.has_message) \t# wait on conditional variable\t\tacquire(bb.lock)\t\t# reacquire\tmessage &lt;- bb.buf[bb.out mod N]\tbb.out &lt;= bb.out + 1\trelease(bb.lock)\tnotify(bb.has_space)\treturn message在没有新消息进入的时候，receiver 应该放弃共享缓冲区的锁，然后进入睡眠等待 sender 唤醒。然而上述代码的问题在于，「放弃缓冲区锁」和「进入睡眠」不是一步原子操作，而是独立的两步操作。Problem在 receiver 放弃共享缓冲区锁（release(bb.lock)）之后，但是在进入睡眠（wait(bb.has_message) ）之前，另一个 sender 有可能在这个间隙中发送消息。在 receiver 进入睡眠之前，sender 会看到没有接收者正在等待 has_message，于是该 receiver 并不会得到消息通知。sender 发送完成后，receiver 才进入睡眠，最好的情况下需要等待下一次 sender 唤醒才能被唤醒，最差情况下永远都不会被唤醒。（deadlock）同样在 receive 从睡眠中唤醒之后以及重新获取锁之前，并发的 sender 也同样可能发送消息，这一部分消息的通知也无法被 receiver 收到。该问题被称为 “lost wakeup problem” 或 “lost notify problem”，可译为“唤醒丢失” lost wakeup 检测方法：if(receiver 收到通知的次数 &lt; sender 发送通知的次数)Solution: atomic release-wait-reacquire问题出在 release(bb.lock) 与 wait(bb.has_message) 之间留出来的短暂间隙。解决方法是由操作系统提供一个「原子性释放锁与进入等待」机制，以及「唤醒后原子性重新获得锁」的机制（release-wait-acquire）。也就是，将这三行代码合成一个原子操作：release(bb.lock)\t\t# release lock before sleep\t\t\t\t\t\t# so other threads can runwait(bb.has_message) \t# wait on conditional variableacquire(bb.lock)\t\t# reacquiremonitor (condition variable)在 POSIX 上，这个机制为 pthread condition variable，包含如下操作：# atomic [release mutex `m`, sleep on `c`, reacquire mutex after wakeup]pthread_cond_wait(c, m)pthread_cond_signal(c)\t\t# wake up 1 thread sleeping on `c`pthread_cond_broadcast(c)\t# wake up all threads sleeping on `c`pthread_cond_wait 提供了原子性的「释放互斥锁—进入睡眠—在唤醒后重新获得锁」操作。即改为：receive(bb):\tacquire(bb.lock);\twhile bb.out &gt;= bb.in:\t\tpthread_cond_wait(bb.has_message, bb.lock)\tmessage &lt;- bb.buf[bb.out mod N]\tbb.out &lt;= bb.out + 1\trelease(bb.lock)\tpthread_cond_signal(bb.has_space)\treturn messagesemaphore另一个解决思路是使用信号量 semaphore。只适用于能够抽象为「某个整数是否大于 0」的 condition（例如剩余缓冲区数量、剩余消息数量）。sem_wait(s)sem_post(s)检查是否需要 sleep 的逻辑，和进入睡眠的逻辑一起被包含在了 P 原子操作中（相比于 monitor/condition variable 的由用户程序自己做检测）。P 原子操作包含了整个「获得互斥锁—判断资源数量—释放互斥锁—进入睡眠—在唤醒后重新获得锁」的过程，所以我们所需要的「释放互斥锁—进入睡眠—在唤醒后重新获得锁」过程自然也是原子性的。differences between the two可以将 semaphore 看作「condition 为某个整数 &gt; 0 的 condition variable」。semaphore 适合在等待条件可以用一个整数描述的时候使用。条件变量的维护工作由 P（wait）、V（signal） 原子操作完成。condition variable 则将判断等待条件的任务交给了用户程序，提供了更大的自由度和灵活性。可以用来等待一些不可以用「整数&gt;0」描述的条件变量，例如网络事件和同步屏障（需要等待整数 = 0 ，信号量为等待整数 &gt; 0）（s081-lab7-multithreading-barrier）。小细节： 对于 semaphore 来说，signal 操作在没有进程正在等待的时候，并不会丢失，而是会被记录为整数+1 对于 condition variable，signal 操作在没有进程正在等待的时候，会丢失。 semaphore 使用了内部的互斥锁保证原子性，condition variable 使用了外部传入的互斥锁保证原子性 可以使用「维护一个整数 i + 等待「i &gt; 0」的 condition variable」来实现 semaphoreReferences: https://en.wikipedia.org/wiki/Monitor_(synchronization)#Condition_variables https://en.wikipedia.org/wiki/Producer–consumer_problem" }, { "title": "MIT6.S081 Operating System Engineering 课程总结 & Lab 指北", "url": "/posts/s081-ending/", "categories": "Course Notes, MIT6.S081", "tags": "operating system, course recommendation", "date": "2021-10-24 16:51:00 +0800", "snippet": "课程介绍MIT6.S081 Operating System Engineering 是麻省理工学院计算机科学本科的中级课程，前身是 MIT6.828 研究生课程。课程基于 RISCV 架构以及类 unix 操作系统 xv6，介绍了操作系统最重要的几个基本概念，以及操作系统如何提供抽象、隔离、调度、资源管理，以及为上层应用程序提供服务。Lab 作业几乎都是利用所学到的理论知识，为 xv6 操作系统添加新功能与改进。每个 lab 都带有自动评测功能。课程资源时间表：https://pdos.csail.mit.edu/6.S081/2020/schedule.html为了方便后来的同学们，收集了一些中文资源：课程翻译：https://mit-public-courses-cn-translatio.gitbook.io/mit6-s081/中文字幕：https://www.bilibili.com/video/BV19k4y1C7kA/ (2021-10-29 update: 以上资源是 2020 秋学期的，2021 秋学期的 6.s081 已经出来了，在 https://pdos.csail.mit.edu/6.828/2021/schedule.html，但是粗看了一下变化不大，lab除了少了一个 lazy page allocation 以外只是调换了顺序，建议暂时可以学习2020年版的，资源较多)Lab 指引官方难度参考：🟩 easy：小于 1 小时。通常是为后续练习的热身🟧 moderate：1～2 小时。🟥 hard：大于 2 小时，通常这些练习不需要写很多行代码， 但是要把代码写对很难“个人耗时”是我个人在做这个 lab 的时候所用的时间，包含了研究、阅读代码、编码与调试全过程。 Lab1 - Unix utilities｜Unix 实用工具： 目标简述： 实现几个用户态程序及 unix 实用工具，熟悉 xv6 的开发环境以及系统调用的使用。 实验难度： 🟩🟩🟩 3 easy, 🟧🟧🟧 3 moderate 个人耗时： 4 小时 (2021-09-06) Lab2 - System calls｜系统调用： 目标简述： 添加 syscall 跟踪，以及添加新的系统调用 sysinfo，帮助加深对 xv6 内核的理解。 实验难度： 🟧🟧 2 moderate 个人耗时： 4 小时 (2021-09-09) Lab3 - Page tables｜页表： 目标简述： 探索页表，为每个进程维护独立的内核页表；修改页表以简化从用户态拷贝数据到内核态的方法。（难点：理解进程页表、内核页表概念） 实验难度： 🟩 1 easy, 🟥🟥 2 hard 个人耗时： 19 小时 (2021-09-13) Lab4 - Traps｜中断陷阱： 目标简述： 探索中断以及中断处理机制（trap、trampoline、函数调用、现场保存、页表/特权切换、调用栈、栈指针及返回指针）（注：本 lab 并不非常难，重要的是理解 lecture5 与 lecture6 中的概念） 实验难度： 🟩 1 easy, 🟧 1 moderate, 🟥 1 hard 个人耗时： 8小时 (2021-09-22) Lab5 - Lazy allocation｜内存页懒分配： 目标简述： 实现内存页懒分配机制，在调用 sbrk() 的时候，不立即分配内存，而是只作记录。在访问到这一部分内存页并触发缺页异常的时候才进行实际的物理内存分配。 实验难度： 🟩 1 easy, 🟧 2 moderate 个人耗时： 5小时 (2021-10-01) Lab6 - Copy-on-write fork｜fork 懒拷贝： 目标简述： 实现 fork 懒复制机制，在进程 fork 后，与父进程共享物理内存页。在任一方尝试对内存页进行修改时，才对内存页进行复制。 实验难度： 🟥 1 hard 个人耗时： 4小时 (2021-10-05) Lab7 - Multithreading｜多线程： 目标简述： 实现一个用户态的线程库；尝试使用线程来为程序提速；实现一个同步屏障 实验难度： 🟧🟧🟧 3 moderate 个人耗时： 3 小时 (2021-10-06) Lab8 - Parallelism/Locking｜并发与锁： 目标简述： 重新设计并发代码以降低锁竞争，提高在多核系统上的性能。 实验难度： 🟧 1 moderate, 🟥 1 hard 个人耗时： 14 小时 (2021-10-14)（在第二个实验做到无死锁无错误上构思花费了比较多时间，其实本 lab 测试没有那么强，测不出来这些边界情况） Lab9 - File System｜文件系统： 目标简述： 为 xv6 的文件系统添加大文件以及符号链接支持。 实验难度： 🟧🟧 2 moderate 个人耗时： 4 小时 (2021-10-15) Lab10 - Mmap | 文件内存映射： 目标简述： 实现 *nix 系统调用 mmap 的简单版：支持将文件映射到一片用户虚拟内存区域内，并且支持将对其的修改写回磁盘。 实验难度： 🟥 1 hard 个人耗时： 6 小时 (2021-10-22) Lab11 - Network stack： 目标简述： 熟悉系统驱动与外围设备的交互、内存映射寄存器与 DMA 数据传输，实现与 E1000 网卡交互的核心方法：transmit 与 recv。（本 lab 的难度主要在于阅读文档以及理解 CPU 与操作系统是如何与外围设备交互的。换言之，更重要的是理解概念以及 lab 已经写好的模版代码的作用。） 实验难度： 🟥 1 hard 个人耗时： 2 小时 (2021-10-24) 我在所有 Lab 上的笔记/参考解答，可以查看这个分类：https://blog.miigon.net/categories/mit6-s081/2021/09/06～2021/10/24，在 210 节结束了接近两个月的 MIT6.S081 旅途。不得不说 MIT 的课程以及作业设计真的很不错，目标明确但又充满探索的 xv6 lab，加上自动 grading，体验简直好到爆。推荐同样对操作系统有兴趣的同学们可以来刷一刷，亲自动手体验实现操作系统各项机能的乐趣。" }, { "title": "[mit6.s081] 笔记 Lab11: Networking | 网络", "url": "/posts/s081-lab11-network/", "categories": "Course Notes, MIT6.S081", "tags": "operating system", "date": "2021-10-24 09:51:00 +0800", "snippet": " 这是我自学 MIT6.S081 操作系统课程的 lab 代码笔记第十一篇：Networking（最后一篇）。此 lab 大致耗时：2小时。 课程地址：https://pdos.csail.mit.edu/6.S081/2020/schedule.htmlLab 地址：https://pdos.csail.mit.edu/6.S081/2020/labs/net.html我的代码地址：https://github.com/Miigon/my-xv6-labs-2020/tree/netCommits: https://github.com/Miigon/my-xv6-labs-2020/commits/net 本文中代码注释是编写博客的时候加入的，原仓库中的代码可能缺乏注释或代码不完全相同。Lab 11: Networking (hard)熟悉系统驱动与外围设备的交互、内存映射寄存器与 DMA 数据传输，实现与 E1000 网卡交互的核心方法：transmit 与 recv。本 lab 的难度主要在于阅读文档以及理解 CPU 与操作系统是如何与外围设备交互的。换言之，更重要的是理解概念以及 lab 已经写好的模版代码的作用。代码实现inte1000_transmit(struct mbuf *m){ acquire(&amp;e1000_lock); // 获取 E1000 的锁，防止多进程同时发送数据出现 race uint32 ind = regs[E1000_TDT]; // 下一个可用的 buffer 的下标 struct tx_desc *desc = &amp;tx_ring[ind]; // 获取 buffer 的描述符，其中存储了关于该 buffer 的各种信息 // 如果该 buffer 中的数据还未传输完，则代表我们已经将环形 buffer 列表全部用完，缓冲区不足，返回错误 if(!(desc-&gt;status &amp; E1000_TXD_STAT_DD)) { release(&amp;e1000_lock); return -1; } // 如果该下标仍有之前发送完毕但未释放的 mbuf，则释放 if(tx_mbufs[ind]) { mbuffree(tx_mbufs[ind]); tx_mbufs[ind] = 0; } // 将要发送的 mbuf 的内存地址与长度填写到发送描述符中 desc-&gt;addr = (uint64)m-&gt;head; desc-&gt;length = m-&gt;len; // 设置参数，EOP 表示该 buffer 含有一个完整的 packet // RS 告诉网卡在发送完成后，设置 status 中的 E1000_TXD_STAT_DD 位，表示发送完成。 desc-&gt;cmd = E1000_TXD_CMD_EOP | E1000_TXD_CMD_RS; // 保留新 mbuf 的指针，方便后续再次用到同一下标时释放。 tx_mbufs[ind] = m; // 环形缓冲区内下标增加一。 regs[E1000_TDT] = (regs[E1000_TDT] + 1) % TX_RING_SIZE; release(&amp;e1000_lock); return 0;}static voide1000_recv(void){ while(1) { // 每次 recv 可能接收多个包 uint32 ind = (regs[E1000_RDT] + 1) % RX_RING_SIZE; struct rx_desc *desc = &amp;rx_ring[ind]; // 如果需要接收的包都已经接收完毕，则退出 if(!(desc-&gt;status &amp; E1000_RXD_STAT_DD)) { return; } rx_mbufs[ind]-&gt;len = desc-&gt;length; net_rx(rx_mbufs[ind]); // 传递给上层网络栈。上层负责释放 mbuf // 分配并设置新的 mbuf，供给下一次轮到该下标时使用 rx_mbufs[ind] = mbufalloc(0); desc-&gt;addr = (uint64)rx_mbufs[ind]-&gt;head; desc-&gt;status = 0; regs[E1000_RDT] = ind; }}操作系统想要发送数据的时候，将数据放入环形缓冲区数组 tx_ring 内，然后递增 E1000_TDT，网卡会自动将数据发出。当网卡收到数据的时候，网卡首先使用 direct memory access，将数据放入 rx_ring 环形缓冲区数组中，然后向 CPU 发起一个硬件中断，CPU 在收到中断后，直接读取 rx_ring 中的数据即可。完结撒花本 lab 的完成，为接近两个月的 MIT6.S081 旅途画上了句号。不得不说 MIT 的课程以及作业设计真的很不错，目标明确但又充满探索的 xv6 lab，加上自动 grading，体验简直好到爆。推荐同样对操作系统有兴趣的同学们可以来刷一刷，亲自动手体验实现操作系统各项机能。同时，对计算机科学的探索依然没有穷尽，后面还会同步更新更多的刷课记录以及课程推荐。课程地址：https://pdos.csail.mit.edu/6.S081/2020/schedule.html" }, { "title": "[mit6.s081] 笔记 Lab10: Mmap | 文件内存映射", "url": "/posts/s081-lab10-mmap/", "categories": "Course Notes, MIT6.S081", "tags": "operating system", "date": "2021-10-24 06:51:00 +0800", "snippet": " 这是我自学 MIT6.S081 操作系统课程的 lab 代码笔记第十篇：Mmap。此 lab 大致耗时：6小时。 课程地址：https://pdos.csail.mit.edu/6.S081/2020/schedule.htmlLab 地址：https://pdos.csail.mit.edu/6.S081/2020/labs/mmap.html我的代码地址：https://github.com/Miigon/my-xv6-labs-2020/tree/mmapCommits: https://github.com/Miigon/my-xv6-labs-2020/commits/mmap 本文中代码注释是编写博客的时候加入的，原仓库中的代码可能缺乏注释或代码不完全相同。Lab 10: mmap (hard)实现 *nix 系统调用 mmap 的简单版：支持将文件映射到一片用户虚拟内存区域内，并且支持将对其的修改写回磁盘。这里涉及的操作系统基本概念是「虚存」，mmap 指令除了可以用来将文件映射到内存上，还可以用来将创建的进程间共享内存映射到当前进程的地址空间内。本 lab 只需实现前一功能即可。代码实现首先需要在用户的地址空间内，找到一片空闲的区域，用于映射 mmap 页。查阅 the xv6 book，可以看到 xv6 对用户的地址空间的分配中，heap 的范围一直从 stack 到 trapframe。由于进程本身所使用的内存空间是从低地址往高地址生长的（sbrk 调用）。为了尽量使得 map 的文件使用的地址空间不要和进程所使用的地址空间产生冲突，我们选择将 mmap 映射进来的文件 map 到尽可能高的位置，也就是刚好在 trapframe 下面。并且若有多个 mmap 的文件，则向下生长。// kernel/memlayout.h// map the trampoline page to the highest address,// in both user and kernel space.#define TRAMPOLINE (MAXVA - PGSIZE)// map kernel stacks beneath the trampoline,// each surrounded by invalid guard pages.#define KSTACK(p) (TRAMPOLINE - ((p)+1)* 2*PGSIZE)// User memory layout.// Address zero first:// text// original data and bss// fixed-size stack// expandable heap// ...// mmapped files// TRAPFRAME (p-&gt;trapframe, used by the trampoline)// TRAMPOLINE (the same page as in the kernel)#define TRAPFRAME (TRAMPOLINE - PGSIZE)// MMAP 所能使用的最后一个页+1#define MMAPEND TRAPFRAME接下来定义 vma 结构体，其中包含了 mmap 映射的内存区域的各种必要信息，比如开始地址、大小、所映射文件、文件内偏移以及权限等。并且在 proc 结构体末尾为每个进程加上 16 个 vma 空槽。// kernel/proc.hstruct vma { int valid; uint64 vastart; uint64 sz; struct file *f; int prot; int flags; uint64 offset;};#define NVMA 16// Per-process statestruct proc { struct spinlock lock; // p-&gt;lock must be held when using these: enum procstate state; // Process state struct proc *parent; // Parent process void *chan; // If non-zero, sleeping on chan int killed; // If non-zero, have been killed int xstate; // Exit status to be returned to parent's wait int pid; // Process ID // these are private to the process, so p-&gt;lock need not be held. uint64 kstack; // Virtual address of kernel stack uint64 sz; // Size of process memory (bytes) pagetable_t pagetable; // User page table struct trapframe *trapframe; // data page for trampoline.S struct context context; // swtch() here to run process struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory char name[16]; // Process name (debugging) struct vma vmas[NVMA]; // virtual memory areas};实现 mmap 系统调用。函数原型请参考 man mmap。函数的功能是在进程的 16 个 vma 槽中，找到可用的空槽，并且顺便计算所有 vma 中使用到的最低的虚拟地址（作为新 vma 的结尾地址 vaend，开区间），然后将当前文件映射到该最低地址下面的位置（vastart = vaend - sz）。最后记得使用 filedup(v-&gt;f);，将文件的引用计数增加一。// kernel/sysfile.cuint64sys_mmap(void){ uint64 addr, sz, offset; int prot, flags, fd; struct file *f; if(argaddr(0, &amp;addr) &lt; 0 || argaddr(1, &amp;sz) &lt; 0 || argint(2, &amp;prot) &lt; 0 || argint(3, &amp;flags) &lt; 0 || argfd(4, &amp;fd, &amp;f) &lt; 0 || argaddr(5, &amp;offset) &lt; 0 || sz == 0) return -1; if((!f-&gt;readable &amp;&amp; (prot &amp; (PROT_READ))) || (!f-&gt;writable &amp;&amp; (prot &amp; PROT_WRITE) &amp;&amp; !(flags &amp; MAP_PRIVATE))) return -1; sz = PGROUNDUP(sz); struct proc *p = myproc(); struct vma *v = 0; uint64 vaend = MMAPEND; // non-inclusive // mmaptest never passed a non-zero addr argument. // so addr here is ignored and a new unmapped va region is found to // map the file // our implementation maps file right below where the trapframe is, // from high addresses to low addresses. // Find a free vma, and calculate where to map the file along the way. for(int i=0;i&lt;NVMA;i++) { struct vma *vv = &amp;p-&gt;vmas[i]; if(vv-&gt;valid == 0) { if(v == 0) { v = &amp;p-&gt;vmas[i]; // found free vma; v-&gt;valid = 1; } } else if(vv-&gt;vastart &lt; vaend) { vaend = PGROUNDDOWN(vv-&gt;vastart); } } if(v == 0){ panic(\"mmap: no free vma\"); } v-&gt;vastart = vaend - sz; v-&gt;sz = sz; v-&gt;prot = prot; v-&gt;flags = flags; v-&gt;f = f; // assume f-&gt;type == FD_INODE v-&gt;offset = offset; filedup(v-&gt;f); return v-&gt;vastart;}映射之前，需要注意文件权限的问题，如果尝试将一个只读打开的文件映射为可写，并且开启了回盘（MAP_SHARED），则 mmap 应该失败。否则回盘的时候会出现回盘到一个只读文件的错误情况。由于需要对映射的页实行懒加载，仅在访问到的时候才从磁盘中加载出来，这里采用和 lab5: Lazy Page Allocation 类似的方式实现。具体请参考 lab5 笔记。// kernel/trap.cvoidusertrap(void){ int which_dev = 0; // ...... } else if((which_dev = devintr()) != 0){ // ok } else { uint64 va = r_stval(); if((r_scause() == 13 || r_scause() == 15)){ // vma lazy allocation if(!vmatrylazytouch(va)) { goto unexpected_scause; } } else { unexpected_scause: printf(\"usertrap(): unexpected scause %p pid=%d\\n\", r_scause(), p-&gt;pid); printf(\" sepc=%p stval=%p\\n\", r_sepc(), r_stval()); p-&gt;killed = 1; } } // ...... usertrapret();}// kernel/sysfile.c// find a vma using a virtual address inside that vma.struct vma *findvma(struct proc *p, uint64 va) { for(int i=0;i&lt;NVMA;i++) { struct vma *vv = &amp;p-&gt;vmas[i]; if(vv-&gt;valid == 1 &amp;&amp; va &gt;= vv-&gt;vastart &amp;&amp; va &lt; vv-&gt;vastart + vv-&gt;sz) { return vv; } } return 0;}// finds out whether a page is previously lazy-allocated for a vma// and needed to be touched before use.// if so, touch it so it's mapped to an actual physical page and contains// content of the mapped file.int vmatrylazytouch(uint64 va) { struct proc *p = myproc(); struct vma *v = findvma(p, va); if(v == 0) { return 0; } // printf(\"vma mapping: %p =&gt; %d\\n\", va, v-&gt;offset + PGROUNDDOWN(va - v-&gt;vastart)); // allocate physical page void *pa = kalloc(); if(pa == 0) { panic(\"vmalazytouch: kalloc\"); } memset(pa, 0, PGSIZE); // read data from disk begin_op(); ilock(v-&gt;f-&gt;ip); readi(v-&gt;f-&gt;ip, 0, (uint64)pa, v-&gt;offset + PGROUNDDOWN(va - v-&gt;vastart), PGSIZE); iunlock(v-&gt;f-&gt;ip); end_op(); // set appropriate perms, then map it. int perm = PTE_U; if(v-&gt;prot &amp; PROT_READ) perm |= PTE_R; if(v-&gt;prot &amp; PROT_WRITE) perm |= PTE_W; if(v-&gt;prot &amp; PROT_EXEC) perm |= PTE_X; if(mappages(p-&gt;pagetable, va, PGSIZE, (uint64)pa, PTE_R | PTE_W | PTE_U) &lt; 0) { panic(\"vmalazytouch: mappages\"); } return 1;}到这里应该可以通过 mmap 测试了，接下来实现 munmap 调用，将一个 vma 所分配的所有页释放，并在必要的情况下，将已经修改的页写回磁盘。// kernel/sysfile.cuint64sys_munmap(void){ uint64 addr, sz; if(argaddr(0, &amp;addr) &lt; 0 || argaddr(1, &amp;sz) &lt; 0 || sz == 0) return -1; struct proc *p = myproc(); struct vma *v = findvma(p, addr); if(v == 0) { return -1; } if(addr &gt; v-&gt;vastart &amp;&amp; addr + sz &lt; v-&gt;vastart + v-&gt;sz) { // trying to \"dig a hole\" inside the memory range. return -1; } uint64 addr_aligned = addr; if(addr &gt; v-&gt;vastart) { addr_aligned = PGROUNDUP(addr); } int nunmap = sz - (addr_aligned-addr); // nbytes to unmap if(nunmap &lt; 0) nunmap = 0; vmaunmap(p-&gt;pagetable, addr_aligned, nunmap, v); // custom memory page unmap routine for mmapped pages. if(addr &lt;= v-&gt;vastart &amp;&amp; addr + sz &gt; v-&gt;vastart) { // unmap at the beginning v-&gt;offset += addr + sz - v-&gt;vastart; v-&gt;vastart = addr + sz; } v-&gt;sz -= sz; if(v-&gt;sz &lt;= 0) { fileclose(v-&gt;f); v-&gt;valid = 0; } return 0; }这里首先通过传入的地址找到对应的 vma 结构体（通过前面定义的 findvma 方法），然后检测了一下在 vma 区域中间“挖洞”释放的错误情况，计算出应该开始释放的内存地址以及应该释放的内存字节数量（由于页有可能不是完整释放，如果 addr 处于一个页的中间，则那个页的后半部分释放，但是前半部分不释放，此时该页整体不应该被释放）。计算出来释放内存页的开始地址以及释放的个数后，调用自定义的 vmaunmap 方法（vm.c）对物理内存页进行释放，并在需要的时候将数据写回磁盘。将该方法独立出来并写到 vm.c 中的理由是方便调用 vm.c 中的 walk 方法。在调用 vmaunmap 释放内存页之后，对 v-&gt;offset、v-&gt;vastart 以及 v-&gt;sz 作相应的修改，并在所有页释放完毕之后，关闭对文件的引用，并完全释放该 vma。vmaunmap()：// kernel/vm.c#include \"fcntl.h\"#include \"spinlock.h\"#include \"sleeplock.h\"#include \"file.h\"#include \"proc.h\"// Remove n BYTES (not pages) of vma mappings starting from va. va must be// page-aligned. The mappings NEED NOT exist.// Also free the physical memory and write back vma data to disk if necessary.voidvmaunmap(pagetable_t pagetable, uint64 va, uint64 nbytes, struct vma *v){ uint64 a; pte_t *pte; // printf(\"unmapping %d bytes from %p\\n\",nbytes, va); // borrowed from \"uvmunmap\" for(a = va; a &lt; va + nbytes; a += PGSIZE){ if((pte = walk(pagetable, a, 0)) == 0) panic(\"sys_munmap: walk\"); if(PTE_FLAGS(*pte) == PTE_V) panic(\"sys_munmap: not a leaf\"); if(*pte &amp; PTE_V){ uint64 pa = PTE2PA(*pte); if((*pte &amp; PTE_D) &amp;&amp; (v-&gt;flags &amp; MAP_SHARED)) { // dirty, need to write back to disk begin_op(); ilock(v-&gt;f-&gt;ip); uint64 aoff = a - v-&gt;vastart; // offset relative to the start of memory range if(aoff &lt; 0) { // if the first page is not a full 4k page writei(v-&gt;f-&gt;ip, 0, pa + (-aoff), v-&gt;offset, PGSIZE + aoff); } else if(aoff + PGSIZE &gt; v-&gt;sz){ // if the last page is not a full 4k page writei(v-&gt;f-&gt;ip, 0, pa, v-&gt;offset + aoff, v-&gt;sz - aoff); } else { // full 4k pages writei(v-&gt;f-&gt;ip, 0, pa, v-&gt;offset + aoff, PGSIZE); } iunlock(v-&gt;f-&gt;ip); end_op(); } kfree((void*)pa); *pte = 0; } }}这里的实现大致上和 uvmunmap 相似，查找范围内的每一个页，检测其 dirty bit (D) 是否被设置，如果被设置，则代表该页被修改过，需要将其写回磁盘。注意不是每一个页都需要完整的写回，这里需要处理开头页不完整、结尾页不完整以及中间完整页的情况。xv6中本身不带有 dirty bit 的宏定义，在 riscv.h 中手动补齐：// kernel/riscv.h#define PTE_V (1L &lt;&lt; 0) // valid#define PTE_R (1L &lt;&lt; 1)#define PTE_W (1L &lt;&lt; 2)#define PTE_X (1L &lt;&lt; 3)#define PTE_U (1L &lt;&lt; 4) // 1 -&gt; user can access#define PTE_G (1L &lt;&lt; 5) // global mapping#define PTE_A (1L &lt;&lt; 6) // accessed#define PTE_D (1L &lt;&lt; 7) // dirty最后需要做的，是在 proc.c 中添加处理进程 vma 的各部分代码。 让 allocproc 初始化进程的时候，将 vma 槽都清空 freeproc 释放进程时，调用 vmaunmap 将所有 vma 的内存都释放，并在需要的时候写回磁盘 fork 时，拷贝父进程的所有 vma，但是不拷贝物理页// kernel/proc.cstatic struct proc*allocproc(void){ // ...... // Clear VMAs for(int i=0;i&lt;NVMA;i++) { p-&gt;vmas[i].valid = 0; } return p;}// free a proc structure and the data hanging from it,// including user pages.// p-&gt;lock must be held.static voidfreeproc(struct proc *p){ if(p-&gt;trapframe) kfree((void*)p-&gt;trapframe); p-&gt;trapframe = 0; for(int i = 0; i &lt; NVMA; i++) { struct vma *v = &amp;p-&gt;vmas[i]; vmaunmap(p-&gt;pagetable, v-&gt;vastart, v-&gt;sz, v); } if(p-&gt;pagetable) proc_freepagetable(p-&gt;pagetable, p-&gt;sz); p-&gt;pagetable = 0; p-&gt;sz = 0; p-&gt;pid = 0; p-&gt;parent = 0; p-&gt;name[0] = 0; p-&gt;chan = 0; p-&gt;killed = 0; p-&gt;xstate = 0; p-&gt;state = UNUSED;}// Create a new process, copying the parent.// Sets up child kernel stack to return as if from fork() system call.intfork(void){ // ...... // copy vmas created by mmap. // actual memory page as well as pte will not be copied over. for(i = 0; i &lt; NVMA; i++) { struct vma *v = &amp;p-&gt;vmas[i]; if(v-&gt;valid) { np-&gt;vmas[i] = *v; filedup(v-&gt;f); } } safestrcpy(np-&gt;name, p-&gt;name, sizeof(p-&gt;name)); pid = np-&gt;pid; np-&gt;state = RUNNABLE; release(&amp;np-&gt;lock); return pid;}由于 mmap 映射的页并不在 [0, p-&gt;sz) 范围内，所以其页表项在 fork 的时候并不会被拷贝。我们只拷贝了 vma 项到子进程，这样子进程尝试访问 mmap 页的时候，会重新触发懒加载，重新分配物理页以及建立映射。执行结果$ mmaptestmmap_test startingtest mmap ftest mmap f: OKtest mmap privatetest mmap private: OKtest mmap read-onlytest mmap read-only: OKtest mmap read/writetest mmap read/write: OKtest mmap dirtytest mmap dirty: OKtest not-mapped unmaptest not-mapped unmap: OKtest mmap two filestest mmap two files: OKmmap_test: ALL OKfork_test startingfork_test OKmmaptest: all tests succeeded" }, { "title": "[mit6.s081] 笔记 Lab9: File System | 文件系统", "url": "/posts/s081-lab9-file-system/", "categories": "Course Notes, MIT6.S081", "tags": "operating system", "date": "2021-10-23 23:52:00 +0800", "snippet": " 这是我自学 MIT6.S081 操作系统课程的 lab 代码笔记第九篇：File System。此 lab 大致耗时：4小时。 课程地址：https://pdos.csail.mit.edu/6.S081/2020/schedule.htmlLab 地址：https://pdos.csail.mit.edu/6.S081/2020/labs/fs.html我的代码地址：https://github.com/Miigon/my-xv6-labs-2020/tree/fsCommits: https://github.com/Miigon/my-xv6-labs-2020/commits/fs 本文中代码注释是编写博客的时候加入的，原仓库中的代码可能缺乏注释或代码不完全相同。Lab 9: File Systems为 xv6 的文件系统添加大文件以及符号链接支持。该 lab 难度较低。Large files (moderate)原理与分析与 FAT 文件系统类似，xv6 文件系统中的每一个 inode 结构体中，采用了混合索引的方式记录数据的所在具体盘块号。每个文件所占用的前 12 个盘块的盘块号是直接记录在 inode 中的（每个盘块 1024 字节），所以对于任何文件的前 12 KB 数据，都可以通过访问 inode 直接得到盘块号。这一部分称为直接记录盘块。对于大于 12 个盘块的文件，大于 12 个盘块的部分，会分配一个额外的一级索引表（一盘块大小，1024Byte），用于存储这部分数据的所在盘块号。由于一级索引表可以包含 BSIZE(1024) / 4 = 256 个盘块号，加上 inode 中的 12 个盘块号，一个文件最多可以使用 12+256 = 268 个盘块，也就是 268KB。inode 结构（含有 NDIRECT=12 个直接记录盘块，还有一个一级索引盘块，后者又可额外包含 256 个盘块号）：// kernel/fs.c// note: NDIRECT=12// On-disk inode structurestruct dinode { short type; // File type short major; // Major device number (T_DEVICE only) short minor; // Minor device number (T_DEVICE only) short nlink; // Number of links to inode in file system uint size; // Size of file (bytes) uint addrs[NDIRECT+1]; // Data block addresses};本 lab 的目标是通过为混合索引机制添加二级索引页，来扩大能够支持的最大文件大小。这里祭出上学校 OS 课的时候的笔记图：本 lab 比较简单，主要前置是需要对文件系统的理解，确保充分理解 xv6 book 中的 file system 相关部分。代码实现首先修改 struct inode（内存中的 inode 副本结构体）以及 struct dinode（磁盘上的 inode 结构体），将 NDIRECT 直接索引的盘块号减少 1，腾出 inode 中的空间来存储二级索引的索引表盘块号。// kernel/fs.h#define NDIRECT 11 // 12 -&gt; 11#define NINDIRECT (BSIZE / sizeof(uint))#define MAXFILE (NDIRECT + NINDIRECT + NINDIRECT * NINDIRECT)// On-disk inode structurestruct dinode { short type; // File type short major; // Major device number (T_DEVICE only) short minor; // Minor device number (T_DEVICE only) short nlink; // Number of links to inode in file system uint size; // Size of file (bytes) uint addrs[NDIRECT+2]; // Data block addresses (NDIRECT+1 -&gt; NDIRECT+2)};// kernel/file.h// in-memory copy of an inodestruct inode { uint dev; // Device number uint inum; // Inode number int ref; // Reference count struct sleeplock lock; // protects everything below here int valid; // inode has been read from disk? short type; // copy of disk inode short major; short minor; short nlink; uint size; uint addrs[NDIRECT+2]; // NDIRECT+1 -&gt; NDIRECT+2};修改 bmap（获取 inode 中第 bn 个块的块号）和 itrunc（释放该 inode 所使用的所有数据块），让其能够识别二级索引。（基本上和复制粘贴一致，只是在查出一级块号后，需将一级块中的数据读入，然后再次查询）// kernel/fs.c// Return the disk block address of the nth block in inode ip.// If there is no such block, bmap allocates one.static uintbmap(struct inode *ip, uint bn){ uint addr, *a; struct buf *bp; if(bn &lt; NDIRECT){ if((addr = ip-&gt;addrs[bn]) == 0) ip-&gt;addrs[bn] = addr = balloc(ip-&gt;dev); return addr; } bn -= NDIRECT; if(bn &lt; NINDIRECT){ // singly-indirect // Load indirect block, allocating if necessary. if((addr = ip-&gt;addrs[NDIRECT]) == 0) ip-&gt;addrs[NDIRECT] = addr = balloc(ip-&gt;dev); bp = bread(ip-&gt;dev, addr); a = (uint*)bp-&gt;data; if((addr = a[bn]) == 0){ a[bn] = addr = balloc(ip-&gt;dev); log_write(bp); } brelse(bp); return addr; } bn -= NINDIRECT; if(bn &lt; NINDIRECT * NINDIRECT) { // doubly-indirect // Load indirect block, allocating if necessary. if((addr = ip-&gt;addrs[NDIRECT+1]) == 0) ip-&gt;addrs[NDIRECT+1] = addr = balloc(ip-&gt;dev); bp = bread(ip-&gt;dev, addr); a = (uint*)bp-&gt;data; if((addr = a[bn/NINDIRECT]) == 0){ a[bn/NINDIRECT] = addr = balloc(ip-&gt;dev); log_write(bp); } brelse(bp); bn %= NINDIRECT; bp = bread(ip-&gt;dev, addr); a = (uint*)bp-&gt;data; if((addr = a[bn]) == 0){ a[bn] = addr = balloc(ip-&gt;dev); log_write(bp); } brelse(bp); return addr; } panic(\"bmap: out of range\");}// Truncate inode (discard contents).// Caller must hold ip-&gt;lock.voiditrunc(struct inode *ip){ int i, j; struct buf *bp; uint *a; for(i = 0; i &lt; NDIRECT; i++){ if(ip-&gt;addrs[i]){ bfree(ip-&gt;dev, ip-&gt;addrs[i]); ip-&gt;addrs[i] = 0; } } if(ip-&gt;addrs[NDIRECT]){ bp = bread(ip-&gt;dev, ip-&gt;addrs[NDIRECT]); a = (uint*)bp-&gt;data; for(j = 0; j &lt; NINDIRECT; j++){ if(a[j]) bfree(ip-&gt;dev, a[j]); } brelse(bp); bfree(ip-&gt;dev, ip-&gt;addrs[NDIRECT]); ip-&gt;addrs[NDIRECT] = 0; } if(ip-&gt;addrs[NDIRECT+1]){ bp = bread(ip-&gt;dev, ip-&gt;addrs[NDIRECT+1]); a = (uint*)bp-&gt;data; for(j = 0; j &lt; NINDIRECT; j++){ if(a[j]) { struct buf *bp2 = bread(ip-&gt;dev, a[j]); uint *a2 = (uint*)bp2-&gt;data; for(int k = 0; k &lt; NINDIRECT; k++){ if(a2[k]) bfree(ip-&gt;dev, a2[k]); } brelse(bp2); bfree(ip-&gt;dev, a[j]); } } brelse(bp); bfree(ip-&gt;dev, ip-&gt;addrs[NDIRECT+1]); ip-&gt;addrs[NDIRECT + 1] = 0; } ip-&gt;size = 0; iupdate(ip);}运行结果$ bigfile..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................wrote 65803 blocksbigfile done; okSymbolic links (moderate)实现符号链接机制。代码实现首先实现 symlink 系统调用，用于创建符号链接。符号链接与普通的文件一样，需要占用 inode 块。这里使用 inode 中的第一个 direct-mapped 块（1024字节）来存储符号链接指向的文件。// kernel/sysfile.cuint64sys_symlink(void){ struct inode *ip; char target[MAXPATH], path[MAXPATH]; if(argstr(0, target, MAXPATH) &lt; 0 || argstr(1, path, MAXPATH) &lt; 0) return -1; begin_op(); ip = create(path, T_SYMLINK, 0, 0); if(ip == 0){ end_op(); return -1; } // use the first data block to store target path. if(writei(ip, 0, (uint64)target, 0, strlen(target)) &lt; 0) { end_op(); return -1; } iunlockput(ip); end_op(); return 0;}在 fcntl.h 中补齐 O_NOFOLLOW 的定义：#define O_RDONLY 0x000#define O_WRONLY 0x001#define O_RDWR 0x002#define O_CREATE 0x200#define O_TRUNC 0x400#define O_NOFOLLOW 0x800修改 sys_open，使其在遇到符号链接的时候，可以递归跟随符号链接，直到跟随到非符号链接的 inode 为止。uint64sys_open(void){ char path[MAXPATH]; int fd, omode; struct file *f; struct inode *ip; int n; if((n = argstr(0, path, MAXPATH)) &lt; 0 || argint(1, &amp;omode) &lt; 0) return -1; begin_op(); if(omode &amp; O_CREATE){ ip = create(path, T_FILE, 0, 0); if(ip == 0){ end_op(); return -1; } } else { int symlink_depth = 0; while(1) { // recursively follow symlinks if((ip = namei(path)) == 0){ end_op(); return -1; } ilock(ip); if(ip-&gt;type == T_SYMLINK &amp;&amp; (omode &amp; O_NOFOLLOW) == 0) { if(++symlink_depth &gt; 10) { // too many layer of symlinks, might be a loop iunlockput(ip); end_op(); return -1; } if(readi(ip, 0, (uint64)path, 0, MAXPATH) &lt; 0) { iunlockput(ip); end_op(); return -1; } iunlockput(ip); } else { break; } } if(ip-&gt;type == T_DIR &amp;&amp; omode != O_RDONLY){ iunlockput(ip); end_op(); return -1; } } // ....... iunlock(ip); end_op(); return fd;}运行结果$ symlinktestStart: test symlinkstest symlinks: okStart: test concurrent symlinkstest concurrent symlinks: ok" }, { "title": "[mit6.s081] 笔记 Lab8: Locks | 锁优化", "url": "/posts/s081-lab8-locks/", "categories": "Course Notes, MIT6.S081", "tags": "operating system", "date": "2021-10-15 22:48:00 +0800", "snippet": " 这是我自学 MIT6.S081 操作系统课程的 lab 代码笔记第八篇：Locks。此 lab 大致耗时：14小时。 课程地址：https://pdos.csail.mit.edu/6.S081/2020/schedule.htmlLab 地址：https://pdos.csail.mit.edu/6.S081/2020/labs/lock.html我的代码地址：https://github.com/Miigon/my-xv6-labs-2020/tree/lockCommits: https://github.com/Miigon/my-xv6-labs-2020/commits/lock 本文中代码注释是编写博客的时候加入的，原仓库中的代码可能缺乏注释或代码不完全相同。Lab 8: Locks重新设计代码以降低锁竞争，提高多核机器上系统的并行性。Memory allocator (moderate)通过拆分 kmem 中的空闲内存链表，降低 kalloc 实现中的 kmem 锁竞争。原理与分析kalloc 原本的实现中，使用 freelist 链表，将空闲物理页本身直接用作链表项（这样可以不使用额外空间）连接成一个链表，在分配的时候，将物理页从链表中移除，回收时将物理页放回链表中。// kernel/kalloc.cstruct { struct spinlock lock; struct run *freelist;} kmem;分配物理页的实现（原版）：// kernel/kalloc.cvoid *kalloc(void){ struct run *r; acquire(&amp;kmem.lock); r = kmem.freelist; // 取出一个物理页。页表项本身就是物理页。 if(r) kmem.freelist = r-&gt;next; release(&amp;kmem.lock); if(r) memset((char*)r, 5, PGSIZE); // fill with junk return (void*)r;}在这里无论是分配物理页或释放物理页，都需要修改 freelist 链表。由于修改是多步操作，为了保持多线程一致性，必须加锁。但这样的设计也使得多线程无法并发申请内存，限制了并发效率。证据是 kmem 锁上频繁的锁竞争：$ kallocteststart test1test1 results:--- lock kmem/bcache statslock: kmem: #fetch-and-add 83375 #acquire() 433015lock: bcache: #fetch-and-add 0 #acquire() 1260--- top 5 contended locks:lock: kmem: #fetch-and-add 83375 #acquire() 433015 // kmem 是整个系统中竞争最激烈的锁lock: proc: #fetch-and-add 23737 #acquire() 130718lock: virtio_disk: #fetch-and-add 11159 #acquire() 114lock: proc: #fetch-and-add 5937 #acquire() 130786lock: proc: #fetch-and-add 4080 #acquire() 130786tot= 83375test1 FAIL这里体现了一个先 profile 再进行优化的思路。如果一个大锁并不会引起明显的性能问题，有时候大锁就足够了。只有在万分确定性能热点是在该锁的时候才进行优化，「过早优化是万恶之源」。这里解决性能热点的思路是「将共享资源变为不共享资源」。锁竞争优化一般有几个思路： 只在必须共享的时候共享（对应为将资源从 CPU 共享拆分为每个 CPU 独立） 必须共享时，尽量减少在关键区中停留的时间（对应“大锁化小锁”，降低锁的粒度）该 lab 的实验目标，即是为每个 CPU 分配独立的 freelist，这样多个 CPU 并发分配物理页就不再会互相排斥了，提高了并行性。但由于在一个 CPU freelist 中空闲页不足的情况下，仍需要从其他 CPU 的 freelist 中“偷”内存页，所以一个 CPU 的 freelist 并不是只会被其对应 CPU 访问，还可能在“偷”内存页的时候被其他 CPU 访问，故仍然需要使用单独的锁来保护每个 CPU 的 freelist。但一个 CPU freelist 中空闲页不足的情况相对来说是比较稀有的，所以总体性能依然比单独 kmem 大锁要快。在最佳情况下，也就是没有发生跨 CPU “偷”页的情况下，这些小锁不会发生任何锁竞争。代码实现// kernel/kalloc.cstruct { struct spinlock lock; struct run *freelist;} kmem[NCPU]; // 为每个 CPU 分配独立的 freelist，并用独立的锁保护它。char *kmem_lock_names[] = { \"kmem_cpu_0\", \"kmem_cpu_1\", \"kmem_cpu_2\", \"kmem_cpu_3\", \"kmem_cpu_4\", \"kmem_cpu_5\", \"kmem_cpu_6\", \"kmem_cpu_7\",};voidkinit(){ for(int i=0;i&lt;NCPU;i++) { // 初始化所有锁 initlock(&amp;kmem[i].lock, kmem_lock_names[i]); } freerange(end, (void*)PHYSTOP);}// kernel/kalloc.cvoidkfree(void *pa){ struct run *r; if(((uint64)pa % PGSIZE) != 0 || (char*)pa &lt; end || (uint64)pa &gt;= PHYSTOP) panic(\"kfree\"); // Fill with junk to catch dangling refs. memset(pa, 1, PGSIZE); r = (struct run*)pa; push_off(); int cpu = cpuid(); acquire(&amp;kmem[cpu].lock); r-&gt;next = kmem[cpu].freelist; kmem[cpu].freelist = r; release(&amp;kmem[cpu].lock); pop_off();}void *kalloc(void){ struct run *r; push_off(); int cpu = cpuid(); acquire(&amp;kmem[cpu].lock); if(!kmem[cpu].freelist) { // no page left for this cpu int steal_left = 64; // steal 64 pages from other cpu(s) for(int i=0;i&lt;NCPU;i++) { if(i == cpu) continue; // no self-robbery acquire(&amp;kmem[i].lock); struct run *rr = kmem[i].freelist; while(rr &amp;&amp; steal_left) { kmem[i].freelist = rr-&gt;next; rr-&gt;next = kmem[cpu].freelist; kmem[cpu].freelist = rr; rr = kmem[i].freelist; steal_left--; } release(&amp;kmem[i].lock); if(steal_left == 0) break; // done stealing } } r = kmem[cpu].freelist; if(r) kmem[cpu].freelist = r-&gt;next; release(&amp;kmem[cpu].lock); pop_off(); if(r) memset((char*)r, 5, PGSIZE); // fill with junk return (void*)r;}这里选择在内存页不足的时候，从其他的 CPU “偷” 64 个页，这里的数值是随意取的，在现实场景中，最好进行测量后选取合适的数值，尽量使得“偷”页频率低。UPDATE 2022-04-14: 上述代码可能产生死锁（cpu_a 尝试偷 cpu_b，cpu_b 尝试偷 cpu_a），可能的解决方案看本文评论区或 https://github.com/Miigon/blog/issues/8。再次运行 kalloctest：$ kallocteststart test1test1 results:--- lock kmem/bcache statslock: kmem_cpu_0: #fetch-and-add 0 #acquire() 35979lock: kmem_cpu_1: #fetch-and-add 0 #acquire() 195945lock: kmem_cpu_2: #fetch-and-add 0 #acquire() 201094lock: bcache: #fetch-and-add 0 #acquire() 1248--- top 5 contended locks:lock: proc: #fetch-and-add 22486 #acquire() 132299lock: virtio_disk: #fetch-and-add 16002 #acquire() 114lock: proc: #fetch-and-add 11199 #acquire() 132301lock: proc: #fetch-and-add 5330 #acquire() 132322lock: proc: #fetch-and-add 4874 #acquire() 132345tot= 0test1 OKstart test2total free number of pages: 32499 (out of 32768).....test2 OK可以看到，kmem 带来的锁竞争降低到了 0（从原本的 ~83375）。Buffer cache (hard) If multiple processes use the file system intensively, they will likely contend for bcache.lock, which protects the disk block cache in kernel/bio.c. bcachetest creates several processes that repeatedly read different files in order to generate contention on bcache.lock;多个进程同时使用文件系统的时候，bcache.lock 上会发生严重的锁竞争。bcache.lock 锁用于保护磁盘区块缓存，在原本的设计中，由于该锁的存在，多个进程不能同时操作（申请、释放）磁盘缓存。原理因为不像 kalloc 中一个物理页分配后就只归单个进程所管，bcache 中的区块缓存是会被多个进程（进一步地，被多个 CPU）共享的（由于多个进程可以同时访问同一个区块）。所以 kmem 中为每个 CPU 预先分割一部分专属的页的方法在这里是行不通的。前面提到的： 锁竞争优化一般有几个思路： 只在必须共享的时候共享（对应为将资源从 CPU 共享拆分为每个 CPU 独立） 必须共享时，尽量减少在关键区中停留的时间（对应“大锁化小锁”，降低锁的粒度） 在这里， bcache 属于“必须共享”的情况，所以需要用到第二个思路，降低锁的粒度，用更精细的锁 scheme 来降低出现竞争的概率。// kernel/bio.cstruct { struct spinlock lock; struct buf buf[NBUF]; // Linked list of all buffers, through prev/next. // Sorted by how recently the buffer was used. // head.next is most recent, head.prev is least. struct buf head;} bcache;原版 xv6 的设计中，使用双向链表存储所有的区块缓存，每次尝试获取一个区块 blockno 的时候，会遍历链表，如果目标区块已经存在缓存中则直接返回，如果不存在则选取一个最近最久未使用的，且引用计数为 0 的 buf 块作为其区块缓存，并返回。新的改进方案，可以建立一个从 blockno 到 buf 的哈希表，并为每个桶单独加锁。这样，仅有在两个进程同时访问的区块同时哈希到同一个桶的时候，才会发生锁竞争。当桶中的空闲 buf 不足的时候，从其他的桶中获取 buf。思路上是很简单的，但是具体实现的时候，需要注意死锁问题。这里的许多死锁问题比较隐晦，而且 bcachetest 测试不出来，但是在实际运行的系统中，是有可能触发死锁的。网上看过许多其他通过了的同学的博客，代码中都没有注意到这一点。死锁问题考虑一下我们的新设计，首先在 bcache 中定义哈希表 bufmap，并为每个桶设置锁：// kernel/bio.hstruct { struct buf buf[NBUF]; struct spinlock eviction_lock; // Hash map: dev and blockno to buf struct buf bufmap[NBUFMAP_BUCKET]; struct spinlock bufmap_locks[NBUFMAP_BUCKET];} bcache;bget(uint dev, uint blockno) 中，首先在 blockno 对应桶中扫描缓存是否存在，如果不存在，则在所有桶中寻找一个最近最久未使用的无引用 buf，进行缓存驱逐，然后将其重新移动到 blockno 对应的桶中（rehash），作为 blockno 的缓存返回。这里很容易就会写出这样的代码：bget(dev, blockno) { key := hash(dev, blockno); acquire(bufmap_locks[key]); // 获取 key 桶的锁 // 查找 blockno 的缓存是否存在，若是直接返回，若否继续执行 if(b := look_for_blockno_in(bufmap[key])) { b-&gt;refcnt++ release(bufmap_locks[key]); return b; } // 查找可驱逐缓存 b least_recently := NULL; for i := [0, NBUFMAP_BUCKET) { // 遍历所有的桶 acquire(bufmap_locks[i]); // 获取第 i 桶的锁 b := look_for_least_recently_used_with_no_ref(bufmap[key]); // 如果找到未使用时间更长的空闲块 if(b.last_use &lt; least_recently.last_use) { least_recently := b; } release(bufmap_locks[i]); // 查找结束后，释放第 i 桶的锁 } b := least_recently; // 驱逐 b 原本存储的缓存（将其从原来的桶删除） evict(b); // 将 b 加入到新的桶 append(bucket[key], b); release(bufmap_locks[key]); // 释放 key 桶的锁 // 设置 b 的各个属性 setup(b); return b;}上面的代码看起来很合理，但是却有两个问题，一个导致运行结果出错，一个导致死锁。问题1：可驱逐 buf 在所对应桶锁释放后不保证仍可驱逐第一个问题比较显而易见，后面进行缓存驱逐的时候，每扫描一个桶前会获取该桶的锁，但是每扫描完一个桶后又释放了该桶的锁。从释放锁的那一瞬间，获取出来的最近最久未使用的空闲 buf 就不再可靠了。因为在我们释放 b 原来所在的桶的锁后（release(bufmap_locks[i]); 后），但是从原桶删除 b 之前（evict(b); 前），另一个 CPU 完全可能会调用 bget 请求 b，使得 b 的引用计数变为不为零。此时我们对 b 进行驱逐就是不安全的了。解决方法也并不复杂，只需要在扫描桶的时候，确保找到最近最久未使用的空闲 buf 后，不释放桶锁，继续持有其对应的桶的锁直到驱逐完成即可。 这里维护的不变量（invariant）是：「扫描到的 buf 在驱逐完成前保持可驱逐」，以及「桶中若存在某个块的 buf，则这个 buf 可用，bget可以直接返回这个 buf」。bget(dev, blockno) { acquire(bufmap_locks[key]); // 获取 key 桶锁 // 查找 blockno 的缓存是否存在，若是直接返回，若否继续执行 if(b := look_for_blockno_in(bufmap[key])) { b-&gt;refcnt++ release(bufmap_locks[key]); return b; } // 缓存不存在，查找可驱逐缓存 b least_recently := NULL; holding_bucket := -1; for i := [0, NBUFMAP_BUCKET) { // 遍历所有的桶 acquire(bufmap_locks[i]); // 获取第 i 桶的锁 b := look_for_least_recently_used_with_no_ref(bufmap[key]); // 如果找到未使用时间更长的空闲块（新的 least_recently） if(b.last_use &gt;= least_recently.last_use) { release(bufmap_locks[i]); // 该桶中没有找到新的 least_recently，释放该桶的锁 } else { // b.last_use &lt; least_recently.last_use least_recently := b; // 释放原本 holding 的锁（holding_bucket &lt; i） if(holding_bucket != -1 &amp;&amp; holding_bucket != key) release(bufmap_locks[holding_bucket]); // 保持第 i 桶的锁不释放...... holding_bucket := i; } } b := least_recently; // 此时，仍然持有 b 所在的桶的锁 bufmap_locks[holding_bucket] // 驱逐 b 原本存储的缓存（将其从原来的桶删除） evict(b); release(bufmap_locks[holding_bucket]); // 驱逐后再释放 b 原本所在桶的锁 // 将 b 加入到新的桶 append(bucket[key], b); release(bufmap_locks[key]); // 释放 key 桶锁 // 设置 b 的各个属性 setup(b); return b;}问题2：两个请求形成环路死锁这里出现的第二个问题就是，一开始我们在 blockno 对应的桶中遍历检查缓存是否存在时，获取了它的锁。而在我们发现 blockno 不存在缓存中之后，需要在拿着 key 桶锁的同时，遍历所有的桶并依次获取它们每个的锁，考虑这种情况：假设块号 b1 的哈希值是 2，块号 b2 的哈希值是 5并且两个块在运行前都没有被缓存----------------------------------------CPU1 CPU2----------------------------------------bget(dev, b1) bget(dev,b2) | | V V获取桶 2 的锁 获取桶 5 的锁 | | V V缓存不存在，遍历所有桶 缓存不存在，遍历所有桶 | | V V ...... 遍历到桶 2 | 尝试获取桶 2 的锁 | | V V 遍历到桶 5 桶 2 的锁由 CPU1 持有，等待释放尝试获取桶 5 的锁 | V桶 5 的锁由 CPU2 持有，等待释放!此时 CPU1 等待 CPU2，而 CPU2 在等待 CPU1，陷入死锁!这里，由于 CPU1 持有锁 2 的情况下去申请锁 5，而 CPU2 持有锁 5 的情况下申请锁 2，造成了环路等待。复习一下死锁的四个条件： 互斥（一个资源在任何时候只能属于一个线程） 请求保持（线程在拿着一个锁的情况下，去申请另一个锁） 不剥夺（外力不强制剥夺一个线程已经拥有的资源） 环路等待（请求资源的顺序形成了一个环）只要破坏了四个条件中的任何一个，就能破坏死锁。为了尝试解决这个死锁问题，我们考虑破坏每一个条件的可行性： 互斥：在这里一个桶只能同时被一个 CPU（线程）处理，互斥条件是必须的，无法破坏。 请求保持 不剥夺：遍历桶的时候，在环路请求出现时强行释放一方的锁？即使能检测，被强制释放锁的一方的 bget 请求会失败，造成文件系统相关系统调用失败，不可行。 环路等待：改变访问顺序，比如永远只遍历当前 key 左侧的桶，使得无论如何访问都不会出现环路？可解决死锁，但假设 blockno 哈希到第一个桶，并且 cache missed 时，将无法进行缓存驱逐来腾出新块供其使用（因为第一个桶左侧没有任何桶）。从「互斥」、「不剥夺」和「环路等待」条件入手都无法解决这个死锁问题，那只能考虑「请求保持」了。这里死锁出现的原因是我们在拿着一个锁的情况下，去尝试申请另一个锁，并且请求顺序出现了环路。既然带环路的请求顺序是不可避免的，那唯一的选项就是在申请任何其他桶锁之前，先放弃之前持有的 key 的桶锁，在找到并驱逐最近最久未使用的空闲块 b 后，再重新获取 key 的桶锁，将 b 加入桶。大致代码是这样：bget(dev, blockno) { acquire(bufmap_locks[key]); // 获取 key 桶锁 // 查找 blockno 的缓存是否存在，若是直接返回，若否继续执行 if(b := look_for_blockno_in(bufmap[key])) { b-&gt;refcnt++ release(bufmap_locks[key]); return b; } release(bufmap_locks[key]); // 先释放 key 桶锁，防止查找驱逐时出现环路等待 // 缓存不存在，查找可驱逐缓存 b holding_bucket := -1; for i := [0, NBUFMAP_BUCKET) { acquire(bufmap_locks[i]); // 请求时不持有 key 桶锁，不会出现环路等待 if(b := look_for_least_recently_used_with_no_ref(bufmap[key])) { if(holding_bucket != -1) release(bufmap_locks[holding_bucket]); holding_bucket := i; // 如果找到新的未使用时间更长的空闲块，则将原来的块所属桶的锁释放掉，保持新块所属桶的锁... } else { release(bufmap_locks[holding_bucket]); } } // 驱逐 b 原本存储的缓存（将其从原来的桶删除） evict(b); release(bufmap_locks[holding_bucket]); // 释放 b 原所在桶的锁 acquire(bufmap_locks[key]); // 再次获取 key 桶锁 append(b, bucket[key]); // 将 b 加入到新的桶 release(bufmap_locks[key]); // 释放 key 桶锁 // 设置 b 的各个属性 setup(b); return b;}这样以来，bget 中无论任何位置，获取桶锁的时候都要么没拿其他锁，要么只拿了其左侧的桶锁（遍历所有桶查找可驱逐缓存 b 的过程中，对桶的遍历固定从小到大访问），所以永远不会出现环路，死锁得到了避免。但是这样的方案又会带来新的问题。新的问题：释放自身桶锁可能使得同 blockno 重复驱逐与分配注意到我们开始「搜索所有桶寻找可驱逐的 buf」这个过程前，为了防止环路等待，而释放了 key 的桶锁（key 为请求的 blockno 的哈希值），直到遍历所有桶并驱逐最近最久未使用的空闲 buf 的过程完成后才重新获取 key 桶锁。问题在于，在释放掉 key 桶锁之后，第一块关键区（“查找 blockno 的缓存是否存在，若是直接返回，若否继续执行”的区域）就得不到锁保护了。这意味着在「释放掉 key 桶锁后」到「重新获取 key 桶锁前」的这个阶段，也就是我们进行驱逐+重分配时，另外一个 CPU 完全有可能访问同一个 blockno，获取到 key 的桶锁，通过了一开始「缓存不存在」的测试，然后也进入到驱逐+重分配中，导致「一个区块有多份缓存」的错误情况出现。怎么保障同一个区块不会有两个缓存呢？这个问题相对比较棘手，我们目前知道的限制条件有： 在遍历桶查找可驱逐 buf 的过程中，不能持有 key 的桶锁，否则会出现死锁。 在遍历桶查找可驱逐 buf 的过程中，不持有 key 桶锁的话，可能会有其他 CPU 访问同一 blockno，并完成驱逐+重分配，导致同一 blockno 被重复缓存。这里不得不承认，我并没有想到什么特别好的方法，只想到了一个牺牲一点效率，但是能保证极端情况下安全的方案： 添加 eviction_lock，将驱逐+重分配的过程限制为单线程 注意此处应该先释放桶锁后，再获取 eviction_lock。写反会导致 eviction_lock 和桶锁发生死锁。（线程 1 拿着桶 A 锁请求 eviction_lock， 线程 2 拿着 eviction_lock 驱逐时遍历请求到桶 A 锁） bget(dev, blockno) { acquire(bufmap_locks[key]); // 获取 key 桶锁 // 查找 blockno 的缓存是否存在，若是直接返回，若否继续执行 if(b := look_for_blockno_in(bufmap[key])) { b-&gt;refcnt++ release(bufmap_locks[key]); return b; } // 注意这里的 acquire 和 release 的顺序 release(bufmap_locks[key]); // 先释放 key 桶锁，防止查找驱逐时出现环路死锁 acquire(eviction_lock); // 获得驱逐锁，防止多个 CPU 同时驱逐影响后续判断 // 缓存不存在，查找可驱逐缓存 b // ....... acquire(bufmap_locks[key]); // 再次获取 key 桶锁 append(b, bucket[key]); // 将 b 加入到新的桶 release(bufmap_locks[key]); // 释放 key 桶锁 release(eviction_lock); // 释放驱逐锁 // 设置 b 的各个属性 setup(b); return b;} 在获取 eviction_lock 之后，马上再次判断 blockno 的缓存是否存在，若是直接返回，若否继续执行 bget(dev, blockno) { acquire(bufmap_locks[key]); // 获取 key 桶锁 // 查找 blockno 的缓存是否存在，若是直接返回，若否继续执行 if(b := look_for_blockno_in(bufmap[key])) { b-&gt;refcnt++ release(bufmap_locks[key]); return b; } // 注意这里的 acquire 和 release 的顺序 release(bufmap_locks[key]); // 先释放 key 桶锁，防止查找驱逐时出现环路死锁 acquire(eviction_lock); // 获得驱逐锁，防止多个 CPU 同时驱逐影响后续判断 // **再次查找 blockno 的缓存是否存在**，若是直接返回，若否继续执行 // 这里由于持有 eviction_lock，没有任何其他线程能够进行驱逐操作，所以 // 没有任何其他线程能够改变 bufmap[key] 桶链表的结构，所以这里不事先获取 // 其相应桶锁而直接开始遍历是安全的。 if(b := look_for_blockno_in(bufmap[key])) { acquire(bufmap_locks[key]); // 必须获取，保护非原子操作 `refcnt++` b-&gt;refcnt++ release(bufmap_locks[key]); release(eviction_lock); return b; } // 缓存不存在，查找可驱逐缓存 b // ....... acquire(bufmap_locks[key]); // 再次获取 key 桶锁 append(b, bucket[key]); // 将 b 加入到新的桶 release(bufmap_locks[key]); // 释放 key 桶锁 release(eviction_lock); // 释放驱逐锁 // 设置 b 的各个属性 setup(b); return b;} 这样以来，即使有多个线程同时请求同一个 blockno，并且所有线程都碰巧通过了一开始的「blockno 的缓存是否存在」的判断且结果都为「缓存不存在」，则进入受 eviction_lock 保护的驱逐+重分配区代码后，能够实际进行驱逐+重分配的，也只有第一个进入的线程。第一个线程进入并驱逐+重分配完毕后才释放 eviction_lock，此时 blockno 的缓存已经由不存在变为存在了，后续的所有线程此时进入后都会被第二次「blockno 缓存是否存在」的判断代码拦住，并直接返回已分配好的缓存 buf，而不会重复对同一个 blockno 进行驱逐+重分配。这么做的好处是，保证了查找过程中不会出现死锁，并且不会出现极端情况下一个块产生多个缓存的情况。而坏处是，引入了全局 eviction_lock，使得原本可并发的遍历驱逐过程的并行性降低了。并且每一次 cache miss 的时候，都会多一次额外的桶遍历开销。然而，cache miss 本身（hopefully）为比较稀有事件，并且对于 cache miss 的块，由于后续需要从磁盘中读入其数据，磁盘读入的耗时将比一次桶遍历的耗时多好几个数量级，所以我认为这样的方案的开销还是可以接受的。 ps. 这样的设计，有一个名词称为「乐观锁（optimistic locking）」，即在冲突发生概率很小的关键区内，不使用独占的互斥锁，而是在提交操作前，检查一下操作的数据是否被其他线程修改（在这里，检测的是 blockno 的缓存是否已被加入），如果是，则代表冲突发生，需要特殊处理（在这里的特殊处理即为直接返回已加入的 buf）。这样的设计，相比较「悲观锁（pessimistic locking）」而言，可以在冲突概率较低的场景下（例如 bget），降低锁开销以及不必要的线性化，提升并行性（例如在 bget 中允许「缓存是否存在」的判断并行化）。有时候还能用于避免死锁。完整伪代码bget(dev, blockno) { acquire(bufmap_locks[key]); // 获取 key 桶锁 // 查找 blockno 的缓存是否存在，若是直接返回，若否继续执行 if(b := look_for_blockno_in(bufmap[key])) { b-&gt;refcnt++ release(bufmap_locks[key]); return b; } // 注意这里的 acquire 和 release 的顺序 release(bufmap_locks[key]); // 先释放 key 桶锁，防止查找驱逐时出现环路死锁 acquire(eviction_lock); // 获得驱逐锁，防止多个 CPU 同时驱逐影响后续判断 // **再次查找 blockno 的缓存是否存在**，若是直接返回，若否继续执行 // 这里由于持有 eviction_lock，没有任何其他线程能够进行驱逐操作，所以 // 没有任何其他线程能够改变 bufmap[key] 桶链表的结构，所以这里不事先获取 // 其相应桶锁而直接开始遍历是安全的。 if(b := look_for_blockno_in(bufmap[key])) { acquire(bufmap_locks[key]); // 必须获取，保护非原子操作 `refcnt++` b-&gt;refcnt++ release(bufmap_locks[key]); release(eviction_lock); return b; } // 缓存不存在，查找可驱逐缓存 b holding_bucket := -1; // 当前持有的桶锁 for i := [0, NBUFMAP_BUCKET) { acquire(bufmap_locks[i]); // 请求时不持有 key 桶锁，不会出现环路等待 if(b := look_for_least_recently_used_with_no_ref(bufmap[key])) { if(holding_bucket != -1) release(bufmap_locks[holding_bucket]); holding_bucket := i; // 如果找到新的未使用时间更长的空闲块，则将原来的块所属桶的锁释放掉，保持新块所属桶的锁... } else { release(bufmap_locks[holding_bucket]); } } acquire(bufmap_locks[key]); // 再次获取 key 桶锁 append(b, bucket[key]); // 将 b 加入到新的桶 release(bufmap_locks[key]); // 释放 key 桶锁 release(eviction_lock); // 释放驱逐锁 // 设置 b 的各个属性 setup(b); return b;}完整代码struct buf { int valid; // has data been read from disk? int disk; // does disk \"own\" buf? uint dev; uint blockno; struct sleeplock lock; uint refcnt; uint lastuse; // *newly added, used to keep track of the least-recently-used buf struct buf *next; uchar data[BSIZE];};// kernel/bio.c// bucket number for bufmap#define NBUFMAP_BUCKET 13// hash function for bufmap#define BUFMAP_HASH(dev, blockno) ((((dev)&lt;&lt;27)|(blockno))%NBUFMAP_BUCKET)struct { struct buf buf[NBUF]; struct spinlock eviction_lock; // Hash map: dev and blockno to buf struct buf bufmap[NBUFMAP_BUCKET]; struct spinlock bufmap_locks[NBUFMAP_BUCKET];} bcache;voidbinit(void){ // Initialize bufmap for(int i=0;i&lt;NBUFMAP_BUCKET;i++) { initlock(&amp;bcache.bufmap_locks[i], \"bcache_bufmap\"); bcache.bufmap[i].next = 0; } // Initialize buffers for(int i=0;i&lt;NBUF;i++){ struct buf *b = &amp;bcache.buf[i]; initsleeplock(&amp;b-&gt;lock, \"buffer\"); b-&gt;lastuse = 0; b-&gt;refcnt = 0; // put all the buffers into bufmap[0] b-&gt;next = bcache.bufmap[0].next; bcache.bufmap[0].next = b; } initlock(&amp;bcache.eviction_lock, \"bcache_eviction\");}// Look through buffer cache for block on device dev.// If not found, allocate a buffer.// In either case, return locked buffer.static struct buf*bget(uint dev, uint blockno){ struct buf *b; uint key = BUFMAP_HASH(dev, blockno); acquire(&amp;bcache.bufmap_locks[key]); // Is the block already cached? for(b = bcache.bufmap[key].next; b; b = b-&gt;next){ if(b-&gt;dev == dev &amp;&amp; b-&gt;blockno == blockno){ b-&gt;refcnt++; release(&amp;bcache.bufmap_locks[key]); acquiresleep(&amp;b-&gt;lock); return b; } } // Not cached. // to get a suitable block to reuse, we need to search for one in all the buckets, // which means acquiring their bucket locks. // but it's not safe to try to acquire every single bucket lock while holding one. // it can easily lead to circular wait, which produces deadlock. release(&amp;bcache.bufmap_locks[key]); // we need to release our bucket lock so that iterating through all the buckets won't // lead to circular wait and deadlock. however, as a side effect of releasing our bucket // lock, other cpus might request the same blockno at the same time and the cache buf for // blockno might be created multiple times in the worst case. since multiple concurrent // bget requests might pass the \"Is the block already cached?\" test and start the // eviction &amp; reuse process multiple times for the same blockno. // // so, after acquiring eviction_lock, we check \"whether cache for blockno is present\" // once more, to be sure that we don't create duplicate cache bufs. acquire(&amp;bcache.eviction_lock); // Check again, is the block already cached? // no other eviction &amp; reuse will happen while we are holding eviction_lock, // which means no link list structure of any bucket can change. // so it's ok here to iterate through `bcache.bufmap[key]` without holding // it's cooresponding bucket lock, since we are holding a much stronger eviction_lock. for(b = bcache.bufmap[key].next; b; b = b-&gt;next){ if(b-&gt;dev == dev &amp;&amp; b-&gt;blockno == blockno){ acquire(&amp;bcache.bufmap_locks[key]); // must do, for `refcnt++` b-&gt;refcnt++; release(&amp;bcache.bufmap_locks[key]); release(&amp;bcache.eviction_lock); acquiresleep(&amp;b-&gt;lock); return b; } } // Still not cached. // we are now only holding eviction lock, none of the bucket locks are held by us. // so it's now safe to acquire any bucket's lock without risking circular wait and deadlock. // find the one least-recently-used buf among all buckets. // finish with it's corresponding bucket's lock held. struct buf *before_least = 0; uint holding_bucket = -1; for(int i = 0; i &lt; NBUFMAP_BUCKET; i++){ // before acquiring, we are either holding nothing, or only holding locks of // buckets that are *on the left side* of the current bucket // so no circular wait can ever happen here. (safe from deadlock) acquire(&amp;bcache.bufmap_locks[i]); int newfound = 0; // new least-recently-used buf found in this bucket for(b = &amp;bcache.bufmap[i]; b-&gt;next; b = b-&gt;next) { if(b-&gt;next-&gt;refcnt == 0 &amp;&amp; (!before_least || b-&gt;next-&gt;lastuse &lt; before_least-&gt;next-&gt;lastuse)) { before_least = b; newfound = 1; } } if(!newfound) { release(&amp;bcache.bufmap_locks[i]); } else { if(holding_bucket != -1) release(&amp;bcache.bufmap_locks[holding_bucket]); holding_bucket = i; // keep holding this bucket's lock.... } } if(!before_least) { panic(\"bget: no buffers\"); } b = before_least-&gt;next; if(holding_bucket != key) { // remove the buf from it's original bucket before_least-&gt;next = b-&gt;next; release(&amp;bcache.bufmap_locks[holding_bucket]); // rehash and add it to the target bucket acquire(&amp;bcache.bufmap_locks[key]); b-&gt;next = bcache.bufmap[key].next; bcache.bufmap[key].next = b; } b-&gt;dev = dev; b-&gt;blockno = blockno; b-&gt;refcnt = 1; b-&gt;valid = 0; release(&amp;bcache.bufmap_locks[key]); release(&amp;bcache.eviction_lock); acquiresleep(&amp;b-&gt;lock); return b;}// ......// Release a locked buffer.voidbrelse(struct buf *b){ if(!holdingsleep(&amp;b-&gt;lock)) panic(\"brelse\"); releasesleep(&amp;b-&gt;lock); uint key = BUFMAP_HASH(b-&gt;dev, b-&gt;blockno); acquire(&amp;bcache.bufmap_locks[key]); b-&gt;refcnt--; if (b-&gt;refcnt == 0) { b-&gt;lastuse = ticks; } release(&amp;bcache.bufmap_locks[key]);}voidbpin(struct buf *b) { uint key = BUFMAP_HASH(b-&gt;dev, b-&gt;blockno); acquire(&amp;bcache.bufmap_locks[key]); b-&gt;refcnt++; release(&amp;bcache.bufmap_locks[key]);}voidbunpin(struct buf *b) { uint key = BUFMAP_HASH(b-&gt;dev, b-&gt;blockno); acquire(&amp;bcache.bufmap_locks[key]); b-&gt;refcnt--; release(&amp;bcache.bufmap_locks[key]);}运行结果$ bcacheteststart test0test0 results:--- lock kmem/bcache statslock: kmem_cpu_0: #fetch-and-add 0 #acquire() 32897lock: kmem_cpu_1: #fetch-and-add 0 #acquire() 77lock: kmem_cpu_2: #fetch-and-add 0 #acquire() 61lock: bcache_bufmap: #fetch-and-add 0 #acquire() 6400lock: bcache_bufmap: #fetch-and-add 0 #acquire() 6685lock: bcache_bufmap: #fetch-and-add 0 #acquire() 6696lock: bcache_bufmap: #fetch-and-add 0 #acquire() 7018lock: bcache_bufmap: #fetch-and-add 0 #acquire() 6266lock: bcache_bufmap: #fetch-and-add 0 #acquire() 4206lock: bcache_bufmap: #fetch-and-add 0 #acquire() 4206lock: bcache_bufmap: #fetch-and-add 0 #acquire() 2193lock: bcache_bufmap: #fetch-and-add 0 #acquire() 4202lock: bcache_bufmap: #fetch-and-add 0 #acquire() 2196lock: bcache_bufmap: #fetch-and-add 0 #acquire() 4359lock: bcache_bufmap: #fetch-and-add 0 #acquire() 4409lock: bcache_bufmap: #fetch-and-add 0 #acquire() 6411lock: bcache_eviction: #fetch-and-add 0 #acquire() 83--- top 5 contended locks:lock: proc: #fetch-and-add 397110 #acquire() 70988lock: proc: #fetch-and-add 262715 #acquire() 70988lock: proc: #fetch-and-add 222165 #acquire() 70987lock: virtio_disk: #fetch-and-add 161088 #acquire() 1098lock: proc: #fetch-and-add 45459 #acquire() 71331tot= 0test0: OKstart test1test1 OK$小结多线程问题往往不如单线程程序中的问题那样容易发现，并且需要对底层指令层面以及 CPU 运行原理层面有足够的认知，才能有效地发现并解决多线程问题。引用 lecture 中的几个建议作为结尾：don't share if you don't have tostart with a few coarse-grained locksinstrument your code -- which locks are preventing parallelism?use fine-grained locks only as needed for parallel performanceuse an automated race detector最后我自己的话：multithreading is a pain😭 , only worth it if there is non-insignificant performance increase. maybe try multi-process architecture for your next project, so you don't have to deal with all the multithreading hassles. you get the bonus of being able to scale horizontally (and almost infinitely) as well :)update 2021-01-07补充一下各个锁的作用： 拿着 bufmap_locks[key] 锁的时候，代表key桶这一个桶中的链表结构、以及所有链表节点的 refcnt 都不会被其他线程改变。也就是说，如果想访问/修改一个桶的结构，或者桶内任意节点的 refcnt，必须先拿那个桶 key 对应的 bufmap_locks[key] 锁。（理由：1.只有 eviction 会改变某个桶的链表结构，而 eviction 本身也会尝试获取该锁 bufmap_locks[key]，所以只要占有该锁，涉及该桶的 eviction 就不会进行，也就代表该桶链表结构不会被改变；2.所有修改某个节点 refcnt 的操作都会先获取其对应的桶锁 bufmap_locks[key]，所以只要占有该锁，桶内所有节点的 refcnt 就不会改变。） 拿着 eviction_lock 的时候，代表不会有其他线程可以进行驱逐操作。由于只有 eviction 可以改变桶的链表结构，拿着该锁，也就意味着整个哈希表中的所有桶的链表结构都不会被改变，但不保证链表内节点的refcnt不会改变。也就是说，拿着 eviction_lock 的时候，refcnt 依然可能会因为多线程导致不一致，但是可以保证拿着锁的整个过程中，每个桶的链表节点数量不会增加、减少，也不会改变顺序。所以拿着 eviction_lock 的时候，可以安全遍历每个桶的每个节点，但是不能访问 refcnt。如果遍历的时候需要访问某个 buf 的 refcnt，则需要另外再拿其所在桶的 bufmap_locks[key] 锁。更简短地讲： bufmap_locks 保护单个桶的链表结构，以及桶内所有节点的 refcnt eviction_lock 保护所有桶的链表结构，但是不保护任何 refcnt驱逐过程中，首先需要拿 eviction_lock，使得可以遍历所有桶的链表结构。然后遍历链表结构寻找可驱逐块的时候，由于在某个桶i中判断是否有可驱逐块的过程需要读取 refcnt，所以需要再拿该桶的 bufmap_locks[i]。Tricky的地方就是，bget 方法一开始判断块是否在缓存中时也获取了一个桶的 bufmap_locks[key]，此时如果遍历获取所有桶的 bufmap_locks[i] 的话，很容易引起环路等待而触发死锁。若在判断是否存在后立刻释放掉 bufmap_locks[key] 再拿 eviction_lock 的话，又会导致在释放桶锁和拿 eviction_lock 这两个操作中间的微小间隙，其他线程可能会对同一个块号进行 bget 访问，导致最终同一个块被插入两次。博客后半部分都是在讲我是如何（尝试）解决这一问题的。最终方案是在释放 bufmap_locks[key]，获取 eviction_lock 之后，再判断一次目标块号是否已经插入。这意味着依然会出现 bget 尝试对同一个 blockno 进行驱逐并插入两次的情况，但是能够保证除了第一个驱逐+插入的尝试能成功外，后续的尝试都不会导致重复驱逐+重复插入，而是能正确返回第一个成功的驱逐+插入产生的结果。也就是允许竞态条件的发生，但是对其进行检测，可以理解为一种乐观锁 optimistic locking。" }, { "title": "[mit6.s081] 笔记 Lab7: Multithreading | 多线程", "url": "/posts/s081-lab7-multithreading/", "categories": "Course Notes, MIT6.S081", "tags": "operating system", "date": "2021-10-07 15:48:00 +0800", "snippet": " 这是我自学 MIT6.S081 操作系统课程的 lab 代码笔记第七篇：Multithreading。此 lab 大致耗时：3小时。 课程地址：https://pdos.csail.mit.edu/6.S081/2020/schedule.htmlLab 地址：https://pdos.csail.mit.edu/6.S081/2020/labs/thread.html我的代码地址：https://github.com/Miigon/my-xv6-labs-2020/tree/threadCommits: https://github.com/Miigon/my-xv6-labs-2020/commits/thread 本文中代码注释是编写博客的时候加入的，原仓库中的代码可能缺乏注释或代码不完全相同。Lab 7: MultithreadingThis lab will familiarize you with multithreading. You will implement switching between threads in a user-level threads package, use multiple threads to speed up a program, and implement a barrier.实现一个用户态的线程库；尝试使用线程来为程序提速；并且尝试实现一个同步屏障Uthread: switching between threads (moderate)补全 uthread.c，完成用户态线程功能的实现。这里的线程相比现代操作系统中的线程而言，更接近一些语言中的“协程”（coroutine）。原因是这里的“线程”是完全用户态实现的，多个线程也只能运行在一个 CPU 上，并且没有时钟中断来强制执行调度，需要线程函数本身在合适的时候主动 yield 释放 CPU。这样实现起来的线程并不对线程函数透明，所以比起操作系统的线程而言更接近 coroutine。这个实验其实相当于在用户态重新实现一遍 xv6 kernel 中的 scheduler() 和 swtch() 的功能，所以大多数代码都是可以借鉴的。uthread_switch.S 中需要实现上下文切换的代码，这里借鉴 swtch.S：// uthread_switch.S\t.text\t/*\t\t * save the old thread's registers,\t\t * restore the new thread's registers.\t\t */// void thread_switch(struct context *old, struct context *new);\t.globl thread_switchthread_switch:\tsd ra, 0(a0)\tsd sp, 8(a0)\tsd s0, 16(a0)\tsd s1, 24(a0)\tsd s2, 32(a0)\tsd s3, 40(a0)\tsd s4, 48(a0)\tsd s5, 56(a0)\tsd s6, 64(a0)\tsd s7, 72(a0)\tsd s8, 80(a0)\tsd s9, 88(a0)\tsd s10, 96(a0)\tsd s11, 104(a0)\tld ra, 0(a1)\tld sp, 8(a1)\tld s0, 16(a1)\tld s1, 24(a1)\tld s2, 32(a1)\tld s3, 40(a1)\tld s4, 48(a1)\tld s5, 56(a1)\tld s6, 64(a1)\tld s7, 72(a1)\tld s8, 80(a1)\tld s9, 88(a1)\tld s10, 96(a1)\tld s11, 104(a1)\tret /* return to ra */在调用本函数 uthread_switch() 的过程中，caller-saved registers 已经被调用者保存到栈帧中了，所以这里无需保存这一部分寄存器。 引申：内核调度器无论是通过时钟中断进入（usertrap），还是线程自己主动放弃 CPU（sleep、exit），最终都会调用到 yield 进一步调用 swtch。由于上下文切换永远都发生在函数调用的边界（swtch 调用的边界），恢复执行相当于是 swtch 的返回过程，会从堆栈中恢复 caller-saved 的寄存器，所以用于保存上下文的 context 结构体只需保存 callee-saved 寄存器，以及 返回地址 ra、栈指针 sp 即可。恢复后执行到哪里是通过 ra 寄存器来决定的（swtch 末尾的 ret 转跳到 ra） 而 trapframe 则不同，一个中断可能在任何地方发生，不仅仅是函数调用边界，也有可能在函数执行中途，所以恢复的时候需要靠 pc 寄存器来定位。并且由于切换位置不一定是函数调用边界，所以几乎所有的寄存器都要保存（无论 caller-saved 还是 callee-saved），才能保证正确的恢复执行。这也是内核代码中 struct trapframe 中保存的寄存器比 struct context 多得多的原因。 另外一个，无论是程序主动 sleep，还是时钟中断，都是通过 trampoline 跳转到内核态 usertrap（保存 trapframe），然后再到达 swtch 保存上下文的。恢复上下文都是恢复到 swtch 返回前（依然是内核态），然后返回跳转回 usertrap，再继续运行直到 usertrapret 跳转到 trampoline 读取 trapframe，并返回用户态。也就是上下文恢复并不是直接恢复到用户态，而是恢复到内核态 swtch 刚执行完的状态。负责恢复用户态执行流的其实是 trampoline 以及 trapframe。从 proc.h 中借鉴一下 context 结构体，用于保存 ra、sp 以及 callee-saved registers：// uthread.c// Saved registers for thread context switches.struct context { uint64 ra; uint64 sp; // callee-saved uint64 s0; uint64 s1; uint64 s2; uint64 s3; uint64 s4; uint64 s5; uint64 s6; uint64 s7; uint64 s8; uint64 s9; uint64 s10; uint64 s11;};struct thread { char stack[STACK_SIZE]; /* the thread's stack */ int state; /* FREE, RUNNING, RUNNABLE */ struct context ctx; // 在 thread 中添加 context 结构体};struct thread all_thread[MAX_THREAD];struct thread *current_thread;extern void thread_switch(struct context* old, struct context* new); // 修改 thread_switch 函数声明在 thread_schedule 中调用 thread_switch 进行上下文切换：// uthread.cvoid thread_schedule(void){ // ...... if (current_thread != next_thread) { /* switch threads? */ next_thread-&gt;state = RUNNING; t = current_thread; current_thread = next_thread; thread_switch(&amp;t-&gt;ctx, &amp;next_thread-&gt;ctx); // 切换线程 } else next_thread = 0;}这里有个小坑是要从 t 切换到 next_thread，不是从 current_thread 切换到 next_thread（因为前面有两句赋值，没错，我在这里眼瞎了卡了一下 QAQ）再补齐 thread_create：// uthread.cvoid thread_create(void (*func)()){ struct thread *t; for (t = all_thread; t &lt; all_thread + MAX_THREAD; t++) { if (t-&gt;state == FREE) break; } t-&gt;state = RUNNABLE; t-&gt;ctx.ra = (uint64)func; // 返回地址 // thread_switch 的结尾会返回到 ra，从而运行线程代码 t-&gt;ctx.sp = (uint64)&amp;t-&gt;stack + (STACK_SIZE - 1); // 栈指针 // 将线程的栈指针指向其独立的栈，注意到栈的生长是从高地址到低地址，所以 // 要将 sp 设置为指向 stack 的最高地址}添加的部分为设置上下文中 ra 指向的地址为线程函数的地址，这样在第一次调度到该线程，执行到 thread_switch 中的 ret 之后就可以跳转到线程函数从而开始执行了。设置 sp 使得线程拥有自己独有的栈，也就是独立的执行流。$ uthreadthread_a startedthread_b startedthread_c startedthread_c 0thread_a 0thread_b 0thread_c 1thread_a 1thread_b 1......thread_c 98thread_a 98thread_b 98thread_c 99thread_a 99thread_b 99thread_c: exit after 100thread_a: exit after 100thread_b: exit after 100thread_schedule: no runnable threadsUsing threads (moderate)分析并解决一个哈希表操作的例子内，由于 race-condition 导致的数据丢失的问题。 Why are there missing keys with 2 threads, but not with 1 thread? Identify a sequence of events with 2 threads that can lead to a key being missing. Submit your sequence with a short explanation in answers-thread.txt[假设键 k1、k2 属于同个 bucket]thread 1: 尝试设置 k1thread 1: 发现 k1 不存在，尝试在 bucket 末尾插入 k1--- scheduler 切换到 thread 2thread 2: 尝试设置 k2thread 2: 发现 k2 不存在，尝试在 bucket 末尾插入 k2thread 2: 分配 entry，在桶末尾插入 k2--- scheduler 切换回 thread 1thread 1: 分配 entry，没有意识到 k2 的存在，在其认为的 “桶末尾”（实际为 k2 所处位置）插入 k1[k1 被插入，但是由于被 k1 覆盖，k2 从桶中消失了，引发了键值丢失]首先先暂时忽略速度，为 put 和 get 操作加锁保证安全：// ph.cpthread_mutex_t lock;intmain(int argc, char *argv[]){ pthread_t *tha; void *value; double t1, t0; pthread_mutex_init(&amp;lock, NULL); // ......}static void put(int key, int value){ NBUCKET; pthread_mutex_lock(&amp;lock); // ...... pthread_mutex_unlock(&amp;lock);}static struct entry*get(int key){ $ NBUCKET; pthread_mutex_lock(&amp;lock); // ...... pthread_mutex_unlock(&amp;lock); return e;}加完这个锁，就可以通过 ph_safe 测试了，编译执行：$ ./ph 1100000 puts, 4.652 seconds, 21494 puts/second0: 0 keys missing100000 gets, 5.098 seconds, 19614 gets/second$ ./ph 2100000 puts, 5.224 seconds, 19142 puts/second0: 0 keys missing1: 0 keys missing200000 gets, 10.222 seconds, 19566 gets/second可以发现，多线程执行的版本也不会丢失 key 了，说明加锁成功防止了 race-condition 的出现。但是仔细观察会发现，加锁后多线程的性能变得比单线程还要低了，虽然不会出现数据丢失，但是失去了多线程并行计算的意义：提升性能。这里的原因是，我们为整个操作加上了互斥锁，意味着每一时刻只能有一个线程在操作哈希表，这里实际上等同于将哈希表的操作变回单线程了，又由于锁操作（加锁、解锁、锁竞争）是有开销的，所以性能甚至不如单线程版本。这里的优化思路，也是多线程效率的一个常见的优化思路，就是降低锁的粒度。由于哈希表中，不同的 bucket 是互不影响的，一个 bucket 处于修改未完全的状态并不影响 put 和 get 对其他 bucket 的操作，所以实际上只需要确保两个线程不会同时操作同一个 bucket 即可，并不需要确保不会同时操作整个哈希表。所以可以将加锁的粒度，从整个哈希表一个锁降低到每个 bucket 一个锁。// ph.cpthread_mutex_t locks;intmain(int argc, char *argv[]){ pthread_t *tha; void *value; double t1, t0; for(int i=0;i&lt;NBUCKET;i++) { pthread_mutex_init(&amp;locks[i], NULL); } // ......}static void put(int key, int value){ int i = key % NBUCKET; pthread_mutex_lock(&amp;locks[i]); // ...... pthread_mutex_unlock(&amp;locks[i]);}static struct entry*get(int key){ int i = key % NBUCKET; pthread_mutex_lock(&amp;locks[i]); // ...... pthread_mutex_unlock(&amp;locks[i]); return e;}在这样修改后，编译执行：$ ./ph 1100000 puts, 4.940 seconds, 20241 puts/second0: 0 keys missing100000 gets, 4.934 seconds, 20267 gets/second$ ./ph 2100000 puts, 3.489 seconds, 28658 puts/second0: 0 keys missing1: 0 keys missing200000 gets, 6.104 seconds, 32766 gets/second$ ./ph 4100000 puts, 1.881 seconds, 53169 puts/second0: 0 keys missing3: 0 keys missing2: 0 keys missing1: 0 keys missing400000 gets, 7.376 seconds, 54229 gets/second可以看到，多线程版本的性能有了显著提升（虽然由于锁开销，依然达不到理想的 单线程速度 * 线程数 那么快），并且依然没有 missing key。此时再运行 grade，就可以通过 ph_fast 测试了。Barrier (moderate)利用 pthread 提供的条件变量方法，实现同步屏障机制。// barrier.cstatic void barrier(){ pthread_mutex_lock(&amp;bstate.barrier_mutex); if(++bstate.nthread &lt; nthread) { pthread_cond_wait(&amp;bstate.barrier_cond, &amp;bstate.barrier_mutex); } else { bstate.nthread = 0; bstate.round++; pthread_cond_broadcast(&amp;bstate.barrier_cond); } pthread_mutex_unlock(&amp;bstate.barrier_mutex);}线程进入同步屏障 barrier 时，将已进入屏障的线程数量增加 1，然后再判断是否已经达到总线程数。如果未达到，则进入睡眠，等待其他线程。如果已经达到，则唤醒所有在 barrier 中等待的线程，所有线程继续执行；屏障轮数 + 1；「将已进入屏障的线程数量增加 1，然后再判断是否已经达到总线程数」这一步并不是原子操作，并且这一步和后面的两种情况中的操作「睡眠」和「唤醒」之间也不是原子的，如果在这里发生 race-condition，则会导致出现 「lost wake-up 问题」（线程 1 即将睡眠前，线程 2 调用了唤醒，然后线程 1 才进入睡眠，导致线程 1 本该被唤醒而没被唤醒，详见 xv6 book 中的第 72 页，Sleep and wakeup）解决方法是，「屏障的线程数量增加 1；判断是否已经达到总线程数；进入睡眠」这三步必须原子。所以使用一个互斥锁 barrier_mutex 来保护这一部分代码。pthread_cond_wait 会在进入睡眠的时候原子性的释放 barrier_mutex，从而允许后续线程进入 barrier，防止死锁。执行测试：$ ./barrier 1OK; passed$ ./barrier 2OK; passed$ ./barrier 4OK; passed" }, { "title": "[mit6.s081] 笔记 Lab6: Copy-on-write fork | fork 懒拷贝", "url": "/posts/s081-lab6-copy-on-write-fork/", "categories": "Course Notes, MIT6.S081", "tags": "operating system", "date": "2021-10-05 15:21:00 +0800", "snippet": " 这是我自学 MIT6.S081 操作系统课程的 lab 代码笔记第六篇：Copy-on-write fork。此 lab 大致耗时：4小时。 课程地址：https://pdos.csail.mit.edu/6.S081/2020/schedule.htmlLab 地址：https://pdos.csail.mit.edu/6.S081/2020/labs/cow.html我的代码地址：https://github.com/Miigon/my-xv6-labs-2020/tree/cowCommits: https://github.com/Miigon/my-xv6-labs-2020/commits/cow 本文中代码注释是编写博客的时候加入的，原仓库中的代码可能缺乏注释或代码不完全相同。Lab 6: Copy-on-write forkCOW fork() creates just a pagetable for the child, with PTEs for user memory pointing to the parent’s physical pages. COW fork() marks all the user PTEs in both parent and child as not writable. When either process tries to write one of these COW pages, the CPU will force a page fault. The kernel page-fault handler detects this case, allocates a page of physical memory for the faulting process, copies the original page into the new page, and modifies the relevant PTE in the faulting process to refer to the new page, this time with the PTE marked writeable. When the page fault handler returns, the user process will be able to write its copy of the page.COW fork() makes freeing of the physical pages that implement user memory a little trickier. A given physical page may be referred to by multiple processes’ page tables, and should be freed only when the last reference disappears.实现 fork 懒复制机制，在进程 fork 后，不立刻复制内存页，而是将虚拟地址指向与父进程相同的物理地址。在父子任意一方尝试对内存页进行修改时，才对内存页进行复制。物理内存页必须保证在所有引用都消失后才能被释放，这里需要有引用计数机制。Implement copy-on write (hard) 为了便于区分，本文将只创建引用而不进行实际内存分配的页复制过程称为「懒复制」，将分配新的内存空间并将数据复制到其中的过程称为「实复制」fork 时不立刻复制内存首先修改 uvmcopy()，在复制父进程的内存到子进程的时候，不立刻复制数据，而是建立指向原物理页的映射，并将父子两端的页表项都设置为不可写。// kernel/vm.cintuvmcopy(pagetable_t old, pagetable_t new, uint64 sz){ pte_t *pte; uint64 pa, i; uint flags; for(i = 0; i &lt; sz; i += PGSIZE){ if((pte = walk(old, i, 0)) == 0) panic(\"uvmcopy: pte should exist\"); if((*pte &amp; PTE_V) == 0) panic(\"uvmcopy: page not present\"); pa = PTE2PA(*pte); if(*pte &amp; PTE_W) { // 清除父进程的 PTE_W 标志位，设置 PTE_COW 标志位表示是一个懒复制页（多个进程引用同个物理页） *pte = (*pte &amp; ~PTE_W) | PTE_COW; } flags = PTE_FLAGS(*pte); // 将父进程的物理页直接 map 到子进程 （懒复制） // 权限设置和父进程一致 // （不可写+PTE_COW，或者如果父进程页本身单纯只读非 COW，则子进程页同样只读且无 COW 标识） if(mappages(new, i, PGSIZE, (uint64)pa, flags) != 0){ goto err; } // 将物理页的引用次数增加 1 krefpage((void*)pa); } return 0; err: uvmunmap(new, 0, i / PGSIZE, 1); return -1;} UPDATE 2023-02-20: 上述代码一开始的版本没有考虑只读页的拷贝，会导致单纯的非 COW 只读页被错误标记为 COW 页从而变成可写。这里给出的代码已经修复该问题，感谢 @zztaki 指出该问题。修复后，只读页会直接共享物理页，并参与引用计数，但是不会被打上 COW 标记。上面用到了 PTE_COW 标志位，用于标示一个映射对应的物理页是否是懒复制页。这里 PTE_COW 需要在 riscv.h 中定义：// kernel/riscv.h#define PTE_V (1L &lt;&lt; 0) // valid#define PTE_R (1L &lt;&lt; 1)#define PTE_W (1L &lt;&lt; 2)#define PTE_X (1L &lt;&lt; 3)#define PTE_U (1L &lt;&lt; 4) // 1 -&gt; user can access#define PTE_COW (1L &lt;&lt; 8) // 是否为懒复制页，使用页表项 flags 中保留的第 8 位表示// （页表项 flags 中，第 8、9、10 位均为保留给操作系统使用的位，可以用作任意自定义用途）这样，fork 时就不会立刻复制内存，只会创建一个映射了。这时候如果尝试修改懒复制的页，会出现 page fault 被 usertrap() 捕获。接下来需要在 usertrap() 中捕捉这个 page fault，并在尝试修改页的时候，执行实复制操作。捕获写操作并执行复制与 lazy allocation lab 类似，在 usertrap() 中添加对 page fault 的检测，并在当前访问的地址符合懒复制页条件时，对懒复制页进行实复制操作：// kernel/trap.cvoidusertrap(void){ // ...... } else if((which_dev = devintr()) != 0){ // ok } else if((r_scause() == 13 || r_scause() == 15) &amp;&amp; uvmcheckcowpage(r_stval())) { // copy-on-write if(uvmcowcopy(r_stval()) == -1){ // 如果内存不足，则杀死进程 p-&gt;killed = 1; } } else { printf(\"usertrap(): unexpected scause %p pid=%d\\n\", r_scause(), p-&gt;pid); printf(\" sepc=%p stval=%p\\n\", r_sepc(), r_stval()); p-&gt;killed = 1; } // ......}同时 copyout() 由于是软件访问页表，不会触发缺页异常，所以需要手动添加同样的监测代码（同 lab5），检测接收的页是否是一个懒复制页，若是，执行实复制操作：// kernel/vm.cintcopyout(pagetable_t pagetable, uint64 dstva, char *src, uint64 len){ uint64 n, va0, pa0; while(len &gt; 0){ if(uvmcheckcowpage(dstva)) // 检查每一个被写的页是否是 COW 页 uvmcowcopy(dstva); va0 = PGROUNDDOWN(dstva); pa0 = walkaddr(pagetable, va0); // .......memmove from src to pa0 len -= n; src += n; dstva = va0 + PGSIZE; } // ......} UPDATE 2023-02-20: 上述代码原始版本只检查了第一个目标页的 COW 状态，对于跨越多页的 copyout，如果目标页中有多个 COW 页，只有刚好在地址范围开头的第一个页会被检查，导致共享页被误写。感谢 @zztaki 指出该问题，这里给出的代码已经修复该问题。该版本对每一个目标页，在写入之前都对 COW 标志位进行检查。实现懒复制页的检测（uvmcheckcowpage()）与实复制（uvmcowcopy()）操作：// kernel/vm.c// 检查一个地址指向的页是否是懒复制页int uvmcheckcowpage(uint64 va) { pte_t *pte; struct proc *p = myproc(); return va &lt; p-&gt;sz // 在进程内存范围内 &amp;&amp; ((pte = walk(p-&gt;pagetable, va, 0))!=0) &amp;&amp; (*pte &amp; PTE_V) // 页表项存在 &amp;&amp; (*pte &amp; PTE_COW); // 页是一个懒复制页}// 实复制一个懒复制页，并重新映射为可写int uvmcowcopy(uint64 va) { pte_t *pte; struct proc *p = myproc(); if((pte = walk(p-&gt;pagetable, va, 0)) == 0) panic(\"uvmcowcopy: walk\"); // 调用 kalloc.c 中的 kcopy_n_deref 方法，复制页 // (如果懒复制页的引用已经为 1，则不需要重新分配和复制内存页，只需清除 PTE_COW 标记并标记 PTE_W 即可) uint64 pa = PTE2PA(*pte); uint64 new = (uint64)kcopy_n_deref((void*)pa); // 将一个懒复制的页引用变为一个实复制的页 if(new == 0) return -1; // 重新映射为可写，并清除 PTE_COW 标记 uint64 flags = (PTE_FLAGS(*pte) | PTE_W) &amp; ~PTE_COW; uvmunmap(p-&gt;pagetable, PGROUNDDOWN(va), 1, 0); if(mappages(p-&gt;pagetable, va, 1, new, flags) == -1) { panic(\"uvmcowcopy: mappages\"); } return 0;}到这里，就已经确定了大体的逻辑了：在 fork 的时候不复制数据只建立映射+标记，在进程尝试写入的时候进行实复制并重新映射为可写。接下来，还需要做页的生命周期管理，确保在所有进程都不使用一个页时才将其释放物理页生命周期以及引用计数在 kalloc.c 中，我们需要定义一系列的新函数，用于完成在支持懒复制的条件下的物理页生命周期管理。在原本的 xv6 实现中，一个物理页的生命周期内，可以支持以下操作： kalloc(): 分配物理页 kfree(): 释放回收物理页而在支持了懒分配后，由于一个物理页可能被多个进程（多个虚拟地址）引用，并且必须在最后一个引用消失后才可以释放回收该物理页，所以一个物理页的生命周期内，现在需要支持以下操作： kalloc(): 分配物理页，将其引用计数置为 1 krefpage(): 创建物理页的一个新引用，引用计数加 1 kcopy_n_deref(): 将物理页的一个引用实复制到一个新物理页上（引用计数为 1），返回得到的副本页；并将本物理页的引用计数减 1 kfree(): 释放物理页的一个引用，引用计数减 1；如果计数变为 0，则释放回收物理页一个物理页 p 首先会被父进程使用 kalloc() 创建，fork 的时候，新创建的子进程会使用 krefpage() 声明自己对父进程物理页的引用。当尝试修改父进程或子进程中的页时，kcopy_n_deref() 负责将想要修改的页实复制到独立的副本，并记录解除旧的物理页的引用（引用计数减 1）。最后 kfree() 保证只有在所有的引用者都释放该物理页的引用时，才释放回收该物理页。这里首先定义一个数组 pageref[] 以及对应的宏，用于记录与获取某个物理页的引用计数：// kernel/kalloc.c// 用于访问物理页引用计数数组#define PA2PGREF_ID(p) (((p)-KERNBASE)/PGSIZE)#define PGREF_MAX_ENTRIES PA2PGREF_ID(PHYSTOP)struct spinlock pgreflock; // 用于 pageref 数组的锁，防止竞态条件引起内存泄漏int pageref[PGREF_MAX_ENTRIES]; // 从 KERNBASE 开始到 PHYSTOP 之间的每个物理页的引用计数// note: reference counts are incremented on fork, not on mapping. this means that// multiple mappings of the same physical page within a single process are only// counted as one reference.// this shouldn't be a problem, though. as there's no way for a user program to map// a physical page twice within it's address space in xv6.// 通过物理地址获得引用计数#define PA2PGREF(p) pageref[PA2PGREF_ID((uint64)(p))]voidkinit(){ initlock(&amp;kmem.lock, \"kmem\"); initlock(&amp;pgreflock, \"pgref\"); // 初始化锁 freerange(end, (void*)PHYSTOP);}voidkfree(void *pa){ struct run *r; if(((uint64)pa % PGSIZE) != 0 || (char*)pa &lt; end || (uint64)pa &gt;= PHYSTOP) panic(\"kfree\"); acquire(&amp;pgreflock); if(--PA2PGREF(pa) &lt;= 0) { // 当页面的引用计数小于等于 0 的时候，释放页面 // Fill with junk to catch dangling refs. // pa will be memset multiple times if race-condition occurred. memset(pa, 1, PGSIZE); r = (struct run*)pa; acquire(&amp;kmem.lock); r-&gt;next = kmem.freelist; kmem.freelist = r; release(&amp;kmem.lock); } release(&amp;pgreflock);}void *kalloc(void){ struct run *r; acquire(&amp;kmem.lock); r = kmem.freelist; if(r) kmem.freelist = r-&gt;next; release(&amp;kmem.lock); if(r){ memset((char*)r, 5, PGSIZE); // fill with junk // 新分配的物理页的引用计数为 1 // (这里无需加锁) PA2PGREF(r) = 1; } return (void*)r;}// Decrease reference to the page by one if it's more than one, then// allocate a new physical page and copy the page into it.// (Effectively turing one reference into one copy.)// // Do nothing and simply return pa when reference count is already// less than or equal to 1.// // 当引用已经小于等于 1 时，不创建和复制到新的物理页，而是直接返回该页本身void *kcopy_n_deref(void *pa) { acquire(&amp;pgreflock); if(PA2PGREF(pa) &lt;= 1) { // 只有 1 个引用，无需复制 release(&amp;pgreflock); return pa; } // 分配新的内存页，并复制旧页中的数据到新页 uint64 newpa = (uint64)kalloc(); if(newpa == 0) { release(&amp;pgreflock); return 0; // out of memory } memmove((void*)newpa, (void*)pa, PGSIZE); // 旧页的引用减 1 PA2PGREF(pa)--; release(&amp;pgreflock); return (void*)newpa;}// 为 pa 的引用计数增加 1void krefpage(void *pa) { acquire(&amp;pgreflock); PA2PGREF(pa)++; release(&amp;pgreflock);}这里可以看到，为 pageref[] 数组定义了自旋锁 pgreflock，并且在除了 kalloc 的其他操作中，都使用了 acquire(&amp;pgreflock); 和 release(&amp;pgreflock); 获取和释放锁来保护操作的代码。这里的锁的作用是防止竞态条件（race-condition）下导致的内存泄漏。举一个很常见的 fork() 后 exec() 的例子：父进程: 分配物理页 p（p 引用计数 = 1）父进程: fork()（p 引用计数 = 2）父进程: 尝试修改 p，触发页异常父进程: 由于 p 引用计数大于 1，开始实复制 p（p 引用计数 = 2）--- 调度器切换到子进程子进程: exec() 替换进程影像，释放所有旧的页子进程: 尝试释放 p（引用计数减 1），子进程丢弃对 p 的引用（p 引用计数 = 1）--- 调度器切换到父进程父进程: （继续实复制p）创建新页 q，将 p 复制到 q，将 q 标记为可写并建立映射，在这过程中父进程丢弃对旧 p 的引用在这一个执行流过后，最终结果是物理页 p 并没有被释放回收，然而父进程和子进程都已经丢弃了对 p 的引用（页表中均没有指向 p 的页表项），这样一来 p 占用的内存就属于泄漏内存了，永远无法被回收。加了锁之后，保证了这种情况不会出现。注意 kalloc() 可以不用加锁，因为 kmem 的锁已经保证了同一个物理页不会同时被两个进程分配，并且在 kalloc() 返回前，其他操作 pageref() 的函数也不会被调用，因为没有任何其他进程能够在 kalloc() 返回前得到这个新页的地址。执行测试$ make grade......== Test running cowtest == $ make qemu-gdb(10.4s) == Test simple == simple: OK == Test three == three: OK == Test file == file: OK == Test usertests == $ make qemu-gdb(99.8s) == Test usertests: copyin == usertests: copyin: OK == Test usertests: copyout == usertests: copyout: OK == Test usertests: all tests == usertests: all tests: OK == Test time == time: OK Score: 110/110如果测试失败，可在 xv6 中手动执行 cowtest 以及 usertests 单独测试，并观察输出。usertests 可能会输出以下错误：FAILED -- lost some free pages 32442 (out of 32448)该错误是 usertests 检测到运行前后的空闲页数量减少，也就是检测到发生了内存泄漏。检查上面的 kalloc.c 中的操作有没有正确加锁，或者一些页的分配/释放是否正确。" }, { "title": "[mit6.s081] 笔记 Lab5: Lazy Page Allocation | 内存页懒分配", "url": "/posts/s081-lab5-lazy-page-allocation/", "categories": "Course Notes, MIT6.S081", "tags": "operating system", "date": "2021-10-01 06:26:00 +0800", "snippet": " 这是我自学 MIT6.S081 操作系统课程的 lab 代码笔记第五篇：Lazy page allocation。此 lab 大致耗时：5小时。 课程地址：https://pdos.csail.mit.edu/6.S081/2020/schedule.htmlLab 地址：https://pdos.csail.mit.edu/6.S081/2020/labs/lazy.html我的代码地址：https://github.com/Miigon/my-xv6-labs-2020/tree/lazyCommits: https://github.com/Miigon/my-xv6-labs-2020/commits/lazy 本文中代码注释是编写博客的时候加入的，原仓库中的代码可能缺乏注释或代码不完全相同。Lab 5: Lazy Page Allocation One of the many neat tricks an O/S can play with page table hardware is lazy allocation of user-space heap memory. Xv6 applications ask the kernel for heap memory using the sbrk() system call. In the kernel we’ve given you, sbrk() allocates physical memory and maps it into the process’s virtual address space. It can take a long time for a kernel to allocate and map memory for a large request. Consider, for example, that a gigabyte consists of 262,144 4096-byte pages; that’s a huge number of allocations even if each is individually cheap. In addition, some programs allocate more memory than they actually use (e.g., to implement sparse arrays), or allocate memory well in advance of use. To allow sbrk() to complete more quickly in these cases, sophisticated kernels allocate user memory lazily. That is, sbrk() doesn’t allocate physical memory, but just remembers which user addresses are allocated and marks those addresses as invalid in the user page table. When the process first tries to use any given page of lazily-allocated memory, the CPU generates a page fault, which the kernel handles by allocating physical memory, zeroing it, and mapping it. You’ll add this lazy allocation feature to xv6 in this lab.实现一个内存页懒分配机制，在调用 sbrk() 的时候，不立即分配内存，而是只作记录。在访问到这一部分内存的时候才进行实际的物理内存分配。本次 lab 分为三个部分，但其实都是属于同一个实验的不同步骤，所以本文将三点集合到一起：Eliminate allocation from sbrk() (easy)Lazy allocation (moderate)Lazytests and Usertests (moderate)Lazy allocation &amp; Tests首先修改 sys_sbrk，使其不再调用 growproc()，而是只修改 p-&gt;sz 的值而不分配物理内存。// kernel/sysproc.cuint64sys_sbrk(void){ int addr; int n; struct proc *p = myproc(); if(argint(0, &amp;n) &lt; 0) return -1; addr = p-&gt;sz; if(n &lt; 0) { uvmdealloc(p-&gt;pagetable, p-&gt;sz, p-&gt;sz+n); // 如果是缩小空间，则马上释放 } p-&gt;sz += n; // 懒分配 return addr;}修改 usertrap 用户态 trap 处理函数，为缺页异常添加检测，如果为缺页异常（(r_scause() == 13 || r_scause() == 15)），且发生异常的地址是由于懒分配而没有映射的话，就为其分配物理内存，并在页表建立映射：// kernel/trap.c//// handle an interrupt, exception, or system call from user space.// called from trampoline.S//voidusertrap(void){ // ...... syscall(); } else if((which_dev = devintr()) != 0){ // ok } else { uint64 va = r_stval(); if((r_scause() == 13 || r_scause() == 15) &amp;&amp; uvmshouldtouch(va)){ // 缺页异常，并且发生异常的地址进行过懒分配 uvmlazytouch(va); // 分配物理内存，并在页表创建映射 } else { // 如果不是缺页异常，或者是在非懒加载地址上发生缺页异常，则抛出错误并杀死进程 printf(\"usertrap(): unexpected scause %p pid=%d\\n\", r_scause(), p-&gt;pid); printf(\" sepc=%p stval=%p\\n\", r_sepc(), r_stval()); p-&gt;killed = 1; } } // ......}uvmlazytouch 函数负责分配实际的物理内存并建立映射。懒分配的内存页在被 touch 后就可以被使用了。uvmshouldtouch 用于检测一个虚拟地址是不是一个需要被 touch 的懒分配内存地址，具体检测的是： 处于 [0, p-&gt;sz)地址范围之中（进程申请的内存范围） 不是栈的 guard page（具体见 xv6 book，栈页的低一页故意留成不映射，作为哨兵用于捕捉 stack overflow 错误。懒分配不应该给这个地址分配物理页和建立映射，而应该直接抛出异常） （解决 usertests 中的 stacktest 失败的问题） 页表项不存在// kernel/vm.c// touch a lazy-allocated page so it's mapped to an actual physical page.void uvmlazytouch(uint64 va) { struct proc *p = myproc(); char *mem = kalloc(); if(mem == 0) { // failed to allocate physical memory printf(\"lazy alloc: out of memory\\n\"); p-&gt;killed = 1; } else { memset(mem, 0, PGSIZE); if(mappages(p-&gt;pagetable, PGROUNDDOWN(va), PGSIZE, (uint64)mem, PTE_W|PTE_X|PTE_R|PTE_U) != 0){ printf(\"lazy alloc: failed to map page\\n\"); kfree(mem); p-&gt;killed = 1; } } // printf(\"lazy alloc: %p, p-&gt;sz: %p\\n\", PGROUNDDOWN(va), p-&gt;sz);}// whether a page is previously lazy-allocated and needed to be touched before use.int uvmshouldtouch(uint64 va) { pte_t *pte; struct proc *p = myproc(); return va &lt; p-&gt;sz // within size of memory for the process &amp;&amp; PGROUNDDOWN(va) != r_sp() // not accessing stack guard page (it shouldn't be mapped) &amp;&amp; (((pte = walk(p-&gt;pagetable, va, 0))==0) || ((*pte &amp; PTE_V)==0)); // page table entry does not exist}由于懒分配的页，在刚分配的时候是没有对应的映射的，所以要把一些原本在遇到无映射地址时会 panic 的函数的行为改为直接忽略这样的地址。uvmummap()：取消虚拟地址映射// kernel/vm.c// 修改这个解决了 proc_freepagetable 时的 panicvoiduvmunmap(pagetable_t pagetable, uint64 va, uint64 npages, int do_free){ uint64 a; pte_t *pte; if((va % PGSIZE) != 0) panic(\"uvmunmap: not aligned\"); for(a = va; a &lt; va + npages*PGSIZE; a += PGSIZE){ if((pte = walk(pagetable, a, 0)) == 0) { continue; // 如果页表项不存在，跳过当前地址 （原本是直接panic） } if((*pte &amp; PTE_V) == 0){ continue; // 如果页表项不存在，跳过当前地址 （原本是直接panic） } if(PTE_FLAGS(*pte) == PTE_V) panic(\"uvmunmap: not a leaf\"); if(do_free){ uint64 pa = PTE2PA(*pte); kfree((void*)pa); } *pte = 0; }}uvmcopy()：将父进程的页表以及内存拷贝到子进程// kernel/vm.c// 修改这个解决了 fork 时的 panicintuvmcopy(pagetable_t old, pagetable_t new, uint64 sz){ pte_t *pte; uint64 pa, i; uint flags; char *mem; for(i = 0; i &lt; sz; i += PGSIZE){ if((pte = walk(old, i, 0)) == 0) continue; // 如果一个页不存在，则认为是懒加载的页，忽略即可 if((*pte &amp; PTE_V) == 0) continue; // 如果一个页不存在，则认为是懒加载的页，忽略即可 pa = PTE2PA(*pte); flags = PTE_FLAGS(*pte); if((mem = kalloc()) == 0) goto err; memmove(mem, (char*)pa, PGSIZE); if(mappages(new, i, PGSIZE, (uint64)mem, flags) != 0){ kfree(mem); goto err; } } return 0; err: uvmunmap(new, 0, i / PGSIZE, 1); return -1;}copyin() 和 copyout()：内核/用户态之间互相拷贝数据由于这里可能会访问到懒分配但是还没实际分配的页，所以要加一个检测，确保 copy 之前，用户态地址对应的页都有被实际分配和映射。// kernel/vm.c// 修改这个解决了 read/write 时的错误 (usertests 中的 sbrkarg 失败的问题)intcopyout(pagetable_t pagetable, uint64 dstva, char *src, uint64 len){ uint64 n, va0, pa0; if(uvmshouldtouch(dstva)) uvmlazytouch(dstva); // ......}intcopyin(pagetable_t pagetable, char *dst, uint64 srcva, uint64 len){ uint64 n, va0, pa0; if(uvmshouldtouch(srcva)) uvmlazytouch(srcva); // ......}至此修改完成，在 xv6 中运行 lazytests 和 usertests 都应该能够成功了。如果在某一步出现了 remap 或者 leaf 之类的 panic，可能是由于页表项没有释放干净。可以从之前 pgtbl 实验中借用打印页表的函数 vmprint 的代码，并在可能有关的系统调用中打出，方便对页表进行调试。 tip. 如果 usertests 某一步失败了，可以用 usertests [测试名称] 直接单独运行某个之前失败过的测试，例如 usertests stacktest 可以直接运行栈 guard page 的测试，而不用等待其他测试漫长的运行。" }, { "title": "[mit6.s081] 笔记 Lab4: Traps | 中断陷阱", "url": "/posts/s081-lab4-traps/", "categories": "Course Notes, MIT6.S081", "tags": "operating system", "date": "2021-09-22 12:00:00 +0800", "snippet": " 这是我自学 MIT6.S081 操作系统课程的 lab 代码笔记第四篇：Traps。此 lab 大致耗时：8小时。 课程地址：https://pdos.csail.mit.edu/6.S081/2020/schedule.htmlLab 地址：https://pdos.csail.mit.edu/6.S081/2020/labs/traps.html我的代码地址：https://github.com/Miigon/my-xv6-labs-2020/tree/trapsCommits: https://github.com/Miigon/my-xv6-labs-2020/commits/traps 本文中代码注释是编写博客的时候加入的，原仓库中的代码可能缺乏注释或代码不完全相同。Lab 4: TrapsThis lab explores how system calls are implemented using traps. You will first do a warm-up exercises with stacks and then you will implement an example of user-level trap handling.探索 trap 实现系统调用的方式。注意本部分主要内容其实都在lecture里（lecture 5、lecture 6），实验不是非常复杂但是以理解概念为重，trap机制、trampoline作用、函数calling convention、调用栈、特权模式、riscv汇编，这些即使都不知道可能依然能完成 lab。但是不代表这些不重要，相反这些才是主要内容，否则 lab 就算跑起来也只是盲狙，没有真正达到学习效果。RISC-V assembly (easy)It will be important to understand a bit of RISC-V assembly, which you were exposed to in 6.004. There is a file user/call.c in your xv6 repo. make fs.img compiles it and also produces a readable assembly version of the program in user/call.asm. Read the code in call.asm for the functions g, f, and main. The instruction manual for RISC-V is on the reference page. Here are some questions that you should answer (store the answers in a file answers-traps.txt):阅读 call.asm，以及 RISC-V 指令集教程，回答问题。（学习 RISC-V 汇编）Q: Which registers contain arguments to functions? For example, which register holds 13 in main's call to printf?A: a0-a7; a2;Q: Where is the call to function f in the assembly code for main? Where is the call to g? (Hint: the compiler may inline functions.)A: There is none. g(x) is inlined within f(x) and f(x) is further inlined into main()Q: At what address is the function printf located?A: 0x0000000000000628, main calls it with pc-relative addressing.Q: What value is in the register ra just after the jalr to printf in main?A: 0x0000000000000038, next line of assembly right after the jalrQ: Run the following code.\tunsigned int i = 0x00646c72;\tprintf(\"H%x Wo%s\", 57616, &amp;i); What is the output?If the RISC-V were instead big-endian what would you set i to in order to yield the same output?Would you need to change 57616 to a different value?A: \"He110 World\"; 0x726c6400; no, 57616 is 110 in hex regardless of endianness.Q: In the following code, what is going to be printed after 'y='? (note: the answer is not a specific value.) Why does this happen?\tprintf(\"x=%d y=%d\", 3);A: A random value depending on what codes there are right before the call.Because printf tried to read more arguments than supplied.The second argument `3` is passed in a1, and the register for the third argument, a2, is not set to any specific value before thecall, and contains whatever there is before the call.简单翻译：Q: 哪些寄存器存储了函数调用的参数？举个例子，main 调用 printf 的时候，13 被存在了哪个寄存器中？A: a0-a7; a2;Q: main 中调用函数 f 对应的汇编代码在哪？对 g 的调用呢？ (提示：编译器有可能会内链(inline)一些函数)A: 没有这样的代码。 g(x) 被内链到 f(x) 中，然后 f(x) 又被进一步内链到 main() 中Q: printf 函数所在的地址是？A: 0x0000000000000628, main 中使用 pc 相对寻址来计算得到这个地址。Q: 在 main 中 jalr 跳转到 printf 之后，ra 的值是什么？A: 0x0000000000000038, jalr 指令的下一条汇编指令的地址。Q: 运行下面的代码\tunsigned int i = 0x00646c72;\tprintf(\"H%x Wo%s\", 57616, &amp;i); 输出是什么？如果 RISC-V 是大端序的，要实现同样的效果，需要将 i 设置为什么？需要将 57616 修改为别的值吗？A: \"He110 World\"; 0x726c6400; 不需要，57616 的十六进制是 110，无论端序（十六进制和内存中的表示不是同个概念）Q: 在下面的代码中，'y=' 之后会答应什么？ (note: 答案不是一个具体的值) 为什么?\tprintf(\"x=%d y=%d\", 3);A: 输出的是一个受调用前的代码影响的“随机”的值。因为 printf 尝试读的参数数量比提供的参数数量多。第二个参数 `3` 通过 a1 传递，而第三个参数对应的寄存器 a2 在调用前不会被设置为任何具体的值，而是会包含调用发生前的任何已经在里面的值。Backtrace (moderate)For debugging it is often useful to have a backtrace: a list of the function calls on the stack above the point at which the error occurred. Implement a backtrace() function in kernel/printf.c. Insert a call to this function in sys_sleep, and then run bttest, which calls sys_sleep. Your output should be as follows: backtrace:0x0000000080002cda0x0000000080002bb60x0000000080002898 After bttest exit qemu. In your terminal: the addresses may be slightly different but if you run addr2line -e kernel/kernel (or riscv64-unknown-elf-addr2line -e kernel/kernel) and cut-and-paste the above addresses as follows: $ addr2line -e kernel/kernel0x0000000080002de20x0000000080002f4a0x0000000080002bfcCtrl-D You should see something like this: kernel/sysproc.c:74kernel/syscall.c:224kernel/trap.c:85 添加 backtrace 功能，打印出调用栈，用于调试。在 defs.h 中添加声明// defs.hvoid printf(char*, ...);void panic(char*) __attribute__((noreturn));void printfinit(void);void backtrace(void); // new在 riscv.h 中添加获取当前 fp（frame pointer）寄存器的方法：// riscv.hstatic inline uint64r_fp(){ uint64 x; asm volatile(\"mv %0, s0\" : \"=r\" (x)); return x;}fp 指向当前栈帧的开始地址，sp 指向当前栈帧的结束地址。 （栈从高地址往低地址生长，所以 fp 虽然是帧开始地址，但是地址比 sp 高）栈帧中从高到低第一个 8 字节 fp-8 是 return address，也就是当前调用层应该返回到的地址。栈帧中从高到低第二个 8 字节 fp-16 是 previous address，指向上一层栈帧的 fp 开始地址。剩下的为保存的寄存器、局部变量等。一个栈帧的大小不固定，但是至少 16 字节。在 xv6 中，使用一个页来存储栈，如果 fp 已经到达栈页的上界，则说明已经到达栈底。查看 call.asm，可以看到，一个函数的函数体最开始首先会扩充一个栈帧给该层调用使用，在函数执行完毕后再回收，例子：int g(int x) { 0:\t1141 addi sp,sp,-16 // 扩张调用栈，得到一个 16 字节的栈帧 2:\te422 sd s0,8(sp) // 将返回地址存到栈帧的第一个 8 字节中 4:\t0800 addi s0,sp,16 return x+3;} 6:\t250d addiw a0,a0,3 8:\t6422 ld s0,8(sp) // 从栈帧读出返回地址 a:\t0141 addi sp,sp,16 // 回收栈帧 c:\t8082 ret // 返回注意栈的生长方向是从高地址到低地址，所以扩张是 -16，而回收是 +16。更多关于寄存器、栈帧以及内存调用的细节，请查看 lecture 5，或 这个很有用的 slides。实现 backtrace 函数：// printf.cvoid backtrace() { uint64 fp = r_fp(); while(fp != PGROUNDUP(fp)) { // 如果已经到达栈底 uint64 ra = *(uint64*)(fp - 8); // return address printf(\"%p\\n\", ra); fp = *(uint64*)(fp - 16); // previous fp }}在 sys_sleep 的开头调用一次 backtrace()// sysproc.cuint64sys_sleep(void){ int n; uint ticks0; backtrace(); // print stack backtrace. if(argint(0, &amp;n) &lt; 0) return -1; // ...... return 0;}编译运行：$ bttest0x0000000080002dea0x0000000080002cc40x00000000800028d0Alarm (hard) In this exercise you’ll add a feature to xv6 that periodically alerts a process as it uses CPU time. This might be useful for compute-bound processes that want to limit how much CPU time they chew up, or for processes that want to compute but also want to take some periodic action. More generally, you’ll be implementing a primitive form of user-level interrupt/fault handlers; you could use something similar to handle page faults in the application, for example. Your solution is correct if it passes alarmtest and usertests.按照如下原型添加系统调用 sigalarm 和 sigreturn（具体步骤不再赘述）：int sigalarm(int ticks, void (*handler)());int sigreturn(void);首先，在 proc 结构体的定义中，增加 alarm 相关字段： alarm_interval：时钟周期，0 为禁用 alarm_handler：时钟回调处理函数 alarm_ticks：下一次时钟响起前还剩下的 ticks 数 alarm_trapframe：时钟中断时刻的 trapframe，用于中断处理完成后恢复原程序的正常执行 alarm_goingoff：是否已经有一个时钟回调正在执行且还未返回（用于防止在 alarm_handler 中途闹钟到期再次调用 alarm_handler，导致 alarm_trapframe 被覆盖）struct proc { // ...... int alarm_interval; // Alarm interval (0 for disabled) void(*alarm_handler)(); // Alarm handler int alarm_ticks; // How many ticks left before next alarm goes off struct trapframe *alarm_trapframe; // A copy of trapframe right before running alarm_handler int alarm_goingoff; // Is an alarm currently going off and hasn't not yet returned? (prevent re-entrance of alarm_handler)};sigalarm 与 sigreturn 具体实现：// sysproc.cuint64 sys_sigalarm(void) { int n; uint64 fn; if(argint(0, &amp;n) &lt; 0) return -1; if(argaddr(1, &amp;fn) &lt; 0) return -1; return sigalarm(n, (void(*)())(fn));}uint64 sys_sigreturn(void) {\treturn sigreturn();}// trap.cint sigalarm(int ticks, void(*handler)()) { // 设置 myproc 中的相关属性 struct proc *p = myproc(); p-&gt;alarm_interval = ticks; p-&gt;alarm_handler = handler; p-&gt;alarm_ticks = ticks; return 0;}int sigreturn() { // 将 trapframe 恢复到时钟中断之前的状态，恢复原本正在执行的程序流 struct proc *p = myproc(); *p-&gt;trapframe = *p-&gt;alarm_trapframe; p-&gt;alarm_goingoff = 0; return 0;}在 proc.c 中添加初始化与释放代码：// proc.cstatic struct proc*allocproc(void){ // ......found: p-&gt;pid = allocpid(); // Allocate a trapframe page. if((p-&gt;trapframe = (struct trapframe *)kalloc()) == 0){ release(&amp;p-&gt;lock); return 0; } // Allocate a trapframe page for alarm_trapframe. if((p-&gt;alarm_trapframe = (struct trapframe *)kalloc()) == 0){ release(&amp;p-&gt;lock); return 0; } p-&gt;alarm_interval = 0; p-&gt;alarm_handler = 0; p-&gt;alarm_ticks = 0; p-&gt;alarm_goingoff = 0; // ...... return p;}static voidfreeproc(struct proc *p){ // ...... if(p-&gt;alarm_trapframe) kfree((void*)p-&gt;alarm_trapframe); p-&gt;alarm_trapframe = 0; // ...... p-&gt;alarm_interval = 0; p-&gt;alarm_handler = 0; p-&gt;alarm_ticks = 0; p-&gt;alarm_goingoff = 0; p-&gt;state = UNUSED;}在 usertrap() 函数中，实现时钟机制具体代码：voidusertrap(void){ int which_dev = 0; // ...... if(p-&gt;killed) exit(-1); // give up the CPU if this is a timer interrupt. // if(which_dev == 2) { // yield(); // } // give up the CPU if this is a timer interrupt. if(which_dev == 2) { if(p-&gt;alarm_interval != 0) { // 如果设定了时钟事件 if(--p-&gt;alarm_ticks &lt;= 0) { // 时钟倒计时 -1 tick，如果已经到达或超过设定的 tick 数 if(!p-&gt;alarm_goingoff) { // 确保没有时钟正在运行 p-&gt;alarm_ticks = p-&gt;alarm_interval; // jump to execute alarm_handler *p-&gt;alarm_trapframe = *p-&gt;trapframe; // backup trapframe p-&gt;trapframe-&gt;epc = (uint64)p-&gt;alarm_handler; p-&gt;alarm_goingoff = 1; } // 如果一个时钟到期的时候已经有一个时钟处理函数正在运行，则会推迟到原处理函数运行完成后的下一个 tick 才触发这次时钟 } } yield(); } usertrapret();}这样，在每次时钟中断的时候，如果进程有已经设置的时钟（alarm_interval != 0），则进行 alarm_ticks 倒数。当 alarm_ticks 倒数到小于等于 0 的时候，如果没有正在处理的时钟，则尝试触发时钟，将原本的程序流保存起来（*alarm_trapframe = *trapframe），然后通过修改 pc 寄存器的值，将程序流转跳到 alarm_handler 中，alarm_handler 执行完毕后再恢复原本的执行流（*trapframe = *alarm_trapframe）。这样从原本程序执行流的视角，就是不可感知的中断了。编译运行：$ alarmtesttest0 start.............alarm!test0 passedtest1 start..alarm!..alarm!..alarm!..alarm!..alarm!..alarm!..alarm!.alarm!...alarm!..alarm!test1 passedtest2 start..............alarm!test2 passedOptional challenge exercisesPrint the names of the functions and line numbers in backtrace() instead of numerical addresses (hard).xv6 默认的编译模式会在生成的可执行文件中，含有调试信息，其中包含了所有符号的名称以及其对应的地址。理论上 backtrace 可以做类似 addr2line 的操作，通过解析可执行文件本身附带的调试信息获得地址对应的源码文件以及行号。这里跳过该 challenge." }, { "title": "[mit6.s081] 笔记 Lab3: Page tables | 页表", "url": "/posts/s081-lab3-page-tables/", "categories": "Course Notes, MIT6.S081", "tags": "operating system", "date": "2021-09-16 19:00:00 +0800", "snippet": " 这是我自学 MIT6.S081 操作系统课程的 lab 代码笔记第三篇：Page tables。此 lab 大致耗时：19小时。 课程地址：https://pdos.csail.mit.edu/6.S081/2020/schedule.htmlLab 地址：https://pdos.csail.mit.edu/6.S081/2020/labs/pgtbl.html我的代码地址：https://github.com/Miigon/my-xv6-labs-2020/tree/pgtblCommits: https://github.com/Miigon/my-xv6-labs-2020/commits/pgtbl 本文中代码注释是编写博客的时候加入的，原仓库中的代码可能缺乏注释或代码不完全相同。Lab 3: Page tablesIn this lab you will explore page tables and modify them to simplify the functions that copy data from user space to kernel space.探索页表，修改页表以简化从用户态拷贝数据到内核态的方法。Print a page table (easy) Define a function called vmprint(). It should take a pagetable_t argument, and print that pagetable in the format described below. Insert if(p-&gt;pid==1) vmprint(p-&gt;pagetable) in exec.c just before the return argc, to print the first process’s page table. You receive full credit for this assignment if you pass the pte printout test of make grade.添加一个打印页表的内核函数，以如如下格式打印出传进的页表，用于后面两个实验调试用：page table 0x0000000087f6e000..0: pte 0x0000000021fda801 pa 0x0000000087f6a000.. ..0: pte 0x0000000021fda401 pa 0x0000000087f69000.. .. ..0: pte 0x0000000021fdac1f pa 0x0000000087f6b000.. .. ..1: pte 0x0000000021fda00f pa 0x0000000087f68000.. .. ..2: pte 0x0000000021fd9c1f pa 0x0000000087f67000..255: pte 0x0000000021fdb401 pa 0x0000000087f6d000.. ..511: pte 0x0000000021fdb001 pa 0x0000000087f6c000.. .. ..510: pte 0x0000000021fdd807 pa 0x0000000087f76000.. .. ..511: pte 0x0000000020001c0b pa 0x0000000080007000RISC-V 的逻辑地址寻址是采用三级页表的形式，9 bit 一级索引找到二级页表，9 bit 二级索引找到三级页表，9 bit 三级索引找到内存页，最低 12 bit 为页内偏移（即一个页 4096 bytes）。具体可以参考 xv6 book 的 Figure 3.2。本函数需要模拟如上的 CPU 查询页表的过程，对三级页表进行遍历，然后按照一定格式输出// kernel/defs.h......int copyout(pagetable_t, uint64, char *, uint64);int copyin(pagetable_t, char *, uint64, uint64);int copyinstr(pagetable_t, char *, uint64, uint64);int vmprint(pagetable_t pagetable); // 添加函数声明因为需要递归打印页表，而 xv6 已经有一个递归释放页表的函数 freewalk()，将其复制一份，并将释放部分代码改为打印即可：// kernel/vm.cint pgtblprint(pagetable_t pagetable, int depth) { // there are 2^9 = 512 PTEs in a page table. for(int i = 0; i &lt; 512; i++){ pte_t pte = pagetable[i]; if(pte &amp; PTE_V) { // 如果页表项有效 // 按格式打印页表项 printf(\"..\"); for(int j=0;j&lt;depth;j++) { printf(\" ..\"); } printf(\"%d: pte %p pa %p\\n\", i, pte, PTE2PA(pte)); // 如果该节点不是叶节点，递归打印其子节点。 if((pte &amp; (PTE_R|PTE_W|PTE_X)) == 0){ // this PTE points to a lower-level page table. uint64 child = PTE2PA(pte); pgtblprint((pagetable_t)child,depth+1); } } } return 0;}int vmprint(pagetable_t pagetable) { printf(\"page table %p\\n\", pagetable); return pgtblprint(pagetable, 0);}// exec.cintexec(char *path, char **argv){ // ...... vmprint(p-&gt;pagetable); // 按照实验要求，在 exec 返回之前打印一下页表。 return argc; // this ends up in a0, the first argument to main(argc, argv) bad: if(pagetable) proc_freepagetable(pagetable, sz); if(ip){ iunlockput(ip); end_op(); } return -1;}grade:$ ./grade-lab-pgtbl pte printoutmake: `kernel/kernel' is up to date.== Test pte printout == pte printout: OK (1.6s) A kernel page table per process (hard) Your first job is to modify the kernel so that every process uses its own copy of the kernel page table when executing in the kernel. Modify struct proc to maintain a kernel page table for each process, and modify the scheduler to switch kernel page tables when switching processes. For this step, each per-process kernel page table should be identical to the existing global kernel page table. You pass this part of the lab if usertests runs correctly.xv6 原本的设计是，用户进程在用户态使用各自的用户态页表，但是一旦进入内核态（例如使用了系统调用），则切换到内核页表（通过修改 satp 寄存器，trampoline.S）。然而这个内核页表是全局共享的，也就是全部进程进入内核态都共用同一个内核态页表：// vm.cpagetable_t kernel_pagetable; // 全局变量，共享的内核页表本 Lab 目标是让每一个进程进入内核态后，都能有自己的独立内核页表，为第三个实验做准备。创建进程内核页表与内核栈首先在进程的结构体 proc 中，添加一个 kernelpgtbl，用于存储进程专享的内核态页表。// kernel/proc.h// Per-process statestruct proc { struct spinlock lock; // p-&gt;lock must be held when using these: enum procstate state; // Process state struct proc *parent; // Parent process void *chan; // If non-zero, sleeping on chan int killed; // If non-zero, have been killed int xstate; // Exit status to be returned to parent's wait int pid; // Process ID // these are private to the process, so p-&gt;lock need not be held. uint64 kstack; // Virtual address of kernel stack uint64 sz; // Size of process memory (bytes) pagetable_t pagetable; // User page table struct trapframe *trapframe; // data page for trampoline.S struct context context; // swtch() here to run process struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory char name[16]; // Process name (debugging) pagetable_t kernelpgtbl; // Kernel page table （在 proc 中添加该 field）};接下来暴改 kvminit。内核需要依赖内核页表内一些固定的映射的存在才能正常工作，例如 UART 控制、硬盘界面、中断控制等。而 kvminit 原本只为全局内核页表 kernel_pagetable 添加这些映射。我们抽象出来一个可以为任何我们自己创建的内核页表添加这些映射的函数 kvm_map_pagetable()。void kvm_map_pagetable(pagetable_t pgtbl) { // 将各种内核需要的 direct mapping 添加到页表 pgtbl 中。 // uart registers kvmmap(pgtbl, UART0, UART0, PGSIZE, PTE_R | PTE_W); // virtio mmio disk interface kvmmap(pgtbl, VIRTIO0, VIRTIO0, PGSIZE, PTE_R | PTE_W); // CLINT kvmmap(pgtbl, CLINT, CLINT, 0x10000, PTE_R | PTE_W); // PLIC kvmmap(pgtbl, PLIC, PLIC, 0x400000, PTE_R | PTE_W); // map kernel text executable and read-only. kvmmap(pgtbl, KERNBASE, KERNBASE, (uint64)etext-KERNBASE, PTE_R | PTE_X); // map kernel data and the physical RAM we'll make use of. kvmmap(pgtbl, (uint64)etext, (uint64)etext, PHYSTOP-(uint64)etext, PTE_R | PTE_W); // map the trampoline for trap entry/exit to // the highest virtual address in the kernel. kvmmap(pgtbl, TRAMPOLINE, (uint64)trampoline, PGSIZE, PTE_R | PTE_X);}pagetable_tkvminit_newpgtbl(){ pagetable_t pgtbl = (pagetable_t) kalloc(); memset(pgtbl, 0, PGSIZE); kvm_map_pagetable(pgtbl); return pgtbl;}/* * create a direct-map page table for the kernel. */voidkvminit(){ kernel_pagetable = kvminit_newpgtbl(); // 仍然需要有全局的内核页表，用于内核 boot 过程，以及无进程在运行时使用。}// ......// 将某个逻辑地址映射到某个物理地址（添加第一个参数 pgtbl）voidkvmmap(pagetable_t pgtbl, uint64 va, uint64 pa, uint64 sz, int perm){ if(mappages(pgtbl, va, sz, pa, perm) != 0) panic(\"kvmmap\");}// kvmpa 将内核逻辑地址转换为物理地址（添加第一个参数 kernelpgtbl）uint64kvmpa(pagetable_t pgtbl, uint64 va){ uint64 off = va % PGSIZE; pte_t *pte; uint64 pa; pte = walk(pgtbl, va, 0); if(pte == 0) panic(\"kvmpa\"); if((*pte &amp; PTE_V) == 0) panic(\"kvmpa\"); pa = PTE2PA(*pte); return pa+off;}现在可以创建进程间相互独立的内核页表了，但是还有一个东西需要处理：内核栈。原本的 xv6 设计中，所有处于内核态的进程都共享同一个页表，即意味着共享同一个地址空间。由于 xv6 支持多核/多进程调度，同一时间可能会有多个进程处于内核态，所以需要对所有处于内核态的进程创建其独立的内核态内的栈，也就是内核栈，供给其内核态代码执行过程。xv6 在启动过程中，会在 procinit() 中为所有可能的 64 个进程位都预分配好内核栈 kstack，具体为在高地址空间里，每个进程使用一个页作为 kstack，并且两个不同 kstack 中间隔着一个无映射的 guard page 用于检测栈溢出错误。具体参考 xv6 book 的 Figure 3.3。在 xv6 原来的设计中，内核页表本来是只有一个的，所有进程共用，所以需要为不同进程创建多个内核栈，并 map 到不同位置（见 procinit() 和 KSTACK 宏）。而我们的新设计中，每一个进程都会有自己独立的内核页表，并且每个进程也只需要访问自己的内核栈，而不需要能够访问所有 64 个进程的内核栈。所以可以将所有进程的内核栈 map 到其各自内核页表内的固定位置（不同页表内的同一逻辑地址，指向不同物理内存）。// initialize the proc table at boot time.voidprocinit(void){ struct proc *p; initlock(&amp;pid_lock, \"nextpid\"); for(p = proc; p &lt; &amp;proc[NPROC]; p++) { initlock(&amp;p-&gt;lock, \"proc\"); // 这里删除了为所有进程预分配内核栈的代码，变为创建进程的时候再创建内核栈，见 allocproc() } kvminithart();}然后，在创建进程的时候，为进程分配独立的内核页表，以及内核栈// kernel/proc.cstatic struct proc*allocproc(void){ struct proc *p; for(p = proc; p &lt; &amp;proc[NPROC]; p++) { acquire(&amp;p-&gt;lock); if(p-&gt;state == UNUSED) { goto found; } else { release(&amp;p-&gt;lock); } } return 0;found: p-&gt;pid = allocpid(); // Allocate a trapframe page. if((p-&gt;trapframe = (struct trapframe *)kalloc()) == 0){ release(&amp;p-&gt;lock); return 0; } // An empty user page table. p-&gt;pagetable = proc_pagetable(p); if(p-&gt;pagetable == 0){ freeproc(p); release(&amp;p-&gt;lock); return 0; }////// 新加部分 start ////// // 为新进程创建独立的内核页表，并将内核所需要的各种映射添加到新页表上 p-&gt;kernelpgtbl = kvminit_newpgtbl(); // printf(\"kernel_pagetable: %p\\n\", p-&gt;kernelpgtbl); // 分配一个物理页，作为新进程的内核栈使用 char *pa = kalloc(); if(pa == 0) panic(\"kalloc\"); uint64 va = KSTACK((int)0); // 将内核栈映射到固定的逻辑地址上 // printf(\"map krnlstack va: %p to pa: %p\\n\", va, pa); kvmmap(p-&gt;kernelpgtbl, va, (uint64)pa, PGSIZE, PTE_R | PTE_W); p-&gt;kstack = va; // 记录内核栈的逻辑地址，其实已经是固定的了，依然这样记录是为了避免需要修改其他部分 xv6 代码////// 新加部分 end ////// // Set up new context to start executing at forkret, // which returns to user space. memset(&amp;p-&gt;context, 0, sizeof(p-&gt;context)); p-&gt;context.ra = (uint64)forkret; p-&gt;context.sp = p-&gt;kstack + PGSIZE; return p;}到这里进程独立的内核页表就创建完成了，但是目前只是创建而已，用户进程进入内核态后依然会使用全局共享的内核页表，因此还需要在 scheduler() 中进行相关修改。切换到进程内核页表在调度器将 CPU 交给进程执行之前，切换到该进程对应的内核页表：// kernel/proc.cvoidscheduler(void){ struct proc *p; struct cpu *c = mycpu(); c-&gt;proc = 0; for(;;){ // Avoid deadlock by ensuring that devices can interrupt. intr_on(); int found = 0; for(p = proc; p &lt; &amp;proc[NPROC]; p++) { acquire(&amp;p-&gt;lock); if(p-&gt;state == RUNNABLE) { // Switch to chosen process. It is the process's job // to release its lock and then reacquire it // before jumping back to us. p-&gt;state = RUNNING; c-&gt;proc = p; // 切换到进程独立的内核页表 w_satp(MAKE_SATP(p-&gt;kernelpgtbl)); sfence_vma(); // 清除快表缓存 // 调度，执行进程 swtch(&amp;c-&gt;context, &amp;p-&gt;context); // 切换回全局内核页表 kvminithart(); // Process is done running for now. // It should have changed its p-&gt;state before coming back. c-&gt;proc = 0; found = 1; } release(&amp;p-&gt;lock); }#if !defined (LAB_FS) if(found == 0) { intr_on(); asm volatile(\"wfi\"); }#else ;#endif }}到这里，每个进程执行的时候，就都会在内核态采用自己独立的内核页表了。释放进程内核页表最后需要做的事情就是在进程结束后，应该释放进程独享的页表以及内核栈，回收资源，否则会导致内存泄漏。（如果 usertests 在 reparent2 的时候出现了 panic: kvmmap，大概率是因为大量内存泄漏消耗完了内存，导致 kvmmap 分配页表项所需内存失败，这时候应该检查是否正确释放了每一处分配的内存，尤其是页表是否每个页表项都释放干净了，）// kernel/proc.cstatic voidfreeproc(struct proc *p){ if(p-&gt;trapframe) kfree((void*)p-&gt;trapframe); p-&gt;trapframe = 0; if(p-&gt;pagetable) proc_freepagetable(p-&gt;pagetable, p-&gt;sz); p-&gt;pagetable = 0; p-&gt;sz = 0; p-&gt;pid = 0; p-&gt;parent = 0; p-&gt;name[0] = 0; p-&gt;chan = 0; p-&gt;killed = 0; p-&gt;xstate = 0; // 释放进程的内核栈 void *kstack_pa = (void *)kvmpa(p-&gt;kernelpgtbl, p-&gt;kstack); // printf(\"trace: free kstack %p\\n\", kstack_pa); kfree(kstack_pa); p-&gt;kstack = 0; // 注意：此处不能使用 proc_freepagetable，因为其不仅会释放页表本身，还会把页表内所有的叶节点对应的物理页也释放掉。 // 这会导致内核运行所需要的关键物理页被释放，从而导致内核崩溃。 // 这里使用 kfree(p-&gt;kernelpgtbl) 也是不足够的，因为这只释放了**一级页表本身**，而不释放二级以及三级页表所占用的空间。 // 递归释放进程独享的页表，释放页表本身所占用的空间，但**不释放页表指向的物理页** kvm_free_kernelpgtbl(p-&gt;kernelpgtbl); p-&gt;kernelpgtbl = 0; p-&gt;state = UNUSED;}kvm_free_kernelpgtbl() 用于递归释放整个多级页表树，也是从 freewalk() 修改而来。// kernel/vm.c// 递归释放一个内核页表中的所有 mapping，但是不释放其指向的物理页voidkvm_free_kernelpgtbl(pagetable_t pagetable){ // there are 2^9 = 512 PTEs in a page table. for(int i = 0; i &lt; 512; i++){ pte_t pte = pagetable[i]; uint64 child = PTE2PA(pte); if((pte &amp; PTE_V) &amp;&amp; (pte &amp; (PTE_R|PTE_W|PTE_X)) == 0){ // 如果该页表项指向更低一级的页表 // 递归释放低一级页表及其页表项 kvm_free_kernelpgtbl((pagetable_t)child); pagetable[i] = 0; } } kfree((void*)pagetable); // 释放当前级别页表所占用空间}这里释放部分就实现完成了。注意到我们的修改影响了其他代码： virtio 磁盘驱动 virtio_disk.c 中调用了 kvmpa() 用于将虚拟地址转换为物理地址，这一操作在我们修改后的版本中，需要传入进程的内核页表。对应修改即可。// virtio_disk.c#include \"proc.h\" // 添加头文件引入// ......voidvirtio_disk_rw(struct buf *b, int write){// ......disk.desc[idx[0]].addr = (uint64) kvmpa(myproc()-&gt;kernelpgtbl, (uint64) &amp;buf0); // 调用 myproc()，获取进程内核页表// ......}Simplify copyin/copyinstr (hard) Replace the body of copyin in kernel/vm.c with a call to copyin_new (defined in kernel/vmcopyin.c); do the same for copyinstr and copyinstr_new. Add mappings for user addresses to each process’s kernel page table so that copyin_new and copyinstr_new work. You pass this assignment if usertests runs correctly and all the make grade tests pass.在上一个实验中，已经使得每一个进程都拥有独立的内核态页表了，这个实验的目标是，在进程的内核态页表中维护一个用户态页表映射的副本，这样使得内核态也可以对用户态传进来的指针（逻辑地址）进行解引用。这样做相比原来 copyin 的实现的优势是，原来的 copyin 是通过软件模拟访问页表的过程获取物理地址的，而在内核页表内维护映射副本的话，可以利用 CPU 的硬件寻址功能进行寻址，效率更高并且可以受快表加速。要实现这样的效果，我们需要在每一处内核对用户页表进行修改的时候，将同样的修改也同步应用在进程的内核页表上，使得两个页表的程序段（0 到 PLIC 段）地址空间的映射同步。准备首先实现一些工具方法，多数是参考现有方法改造得来：// kernel/vm.c// 注：需要在 defs.h 中添加相应的函数声明，这里省略。// 将 src 页表的一部分页映射关系拷贝到 dst 页表中。// 只拷贝页表项，不拷贝实际的物理页内存。// 成功返回0，失败返回 -1intkvmcopymappings(pagetable_t src, pagetable_t dst, uint64 start, uint64 sz){ pte_t *pte; uint64 pa, i; uint flags; // PGROUNDUP: prevent re-mapping already mapped pages (eg. when doing growproc) for(i = PGROUNDUP(start); i &lt; start + sz; i += PGSIZE){ if((pte = walk(src, i, 0)) == 0) panic(\"kvmcopymappings: pte should exist\"); if((*pte &amp; PTE_V) == 0) panic(\"kvmcopymappings: page not present\"); pa = PTE2PA(*pte); // `&amp; ~PTE_U` 表示将该页的权限设置为非用户页 // 必须设置该权限，RISC-V 中内核是无法直接访问用户页的。 flags = PTE_FLAGS(*pte) &amp; ~PTE_U; if(mappages(dst, i, PGSIZE, pa, flags) != 0){ goto err; } } return 0; err: // thanks @hdrkna for pointing out a mistake here. // original code incorrectly starts unmapping from 0 instead of PGROUNDUP(start) uvmunmap(dst, PGROUNDUP(start), (i - PGROUNDUP(start)) / PGSIZE, 0); return -1;}// 与 uvmdealloc 功能类似，将程序内存从 oldsz 缩减到 newsz。但区别在于不释放实际内存// 用于内核页表内程序内存映射与用户页表程序内存映射之间的同步uint64kvmdealloc(pagetable_t pagetable, uint64 oldsz, uint64 newsz){ if(newsz &gt;= oldsz) return oldsz; if(PGROUNDUP(newsz) &lt; PGROUNDUP(oldsz)){ int npages = (PGROUNDUP(oldsz) - PGROUNDUP(newsz)) / PGSIZE; uvmunmap(pagetable, PGROUNDUP(newsz), npages, 0); } return newsz;}接下来，为映射程序内存做准备。实验中提示内核启动后，能够用于映射程序内存的地址范围是 [0,PLIC)，我们将把进程程序内存映射到其内核页表的这个范围内，首先要确保这个范围没有和其他映射冲突。查阅 xv6 book 可以看到，在 PLIC 之前还有一个 CLINT（核心本地中断器）的映射，该映射会与我们要 map 的程序内存冲突。查阅 xv6 book 的 Chapter 5 以及 start.c 可以知道 CLINT 仅在内核启动的时候需要使用到，而用户进程在内核态中的操作并不需要使用到该映射。所以修改 kvm_map_pagetable()，去除 CLINT 的映射，这样进程内核页表就不会有 CLINT 与程序内存映射冲突的问题。但是由于全局内核页表也使用了 kvm_map_pagetable() 进行初始化，并且内核启动的时候需要 CLINT 映射存在，故在 kvminit() 中，另外单独给全局内核页表映射 CLINT。// kernel/vm.cvoid kvm_map_pagetable(pagetable_t pgtbl) { // uart registers kvmmap(pgtbl, UART0, UART0, PGSIZE, PTE_R | PTE_W); // virtio mmio disk interface kvmmap(pgtbl, VIRTIO0, VIRTIO0, PGSIZE, PTE_R | PTE_W); // CLINT // kvmmap(pgtbl, CLINT, CLINT, 0x10000, PTE_R | PTE_W); // PLIC kvmmap(pgtbl, PLIC, PLIC, 0x400000, PTE_R | PTE_W); // ......}// ......voidkvminit(){ kernel_pagetable = kvminit_newpgtbl(); // CLINT *is* however required during kernel boot up and // we should map it for the global kernel pagetable kvmmap(kernel_pagetable, CLINT, CLINT, 0x10000, PTE_R | PTE_W);}同时在 exec 中加入检查，防止程序内存超过 PLIC：intexec(char *path, char **argv){ // ...... // Load program into memory. for(i=0, off=elf.phoff; i&lt;elf.phnum; i++, off+=sizeof(ph)){ if(readi(ip, 0, (uint64)&amp;ph, off, sizeof(ph)) != sizeof(ph)) goto bad; if(ph.type != ELF_PROG_LOAD) continue; if(ph.memsz &lt; ph.filesz) goto bad; if(ph.vaddr + ph.memsz &lt; ph.vaddr) goto bad; uint64 sz1; if((sz1 = uvmalloc(pagetable, sz, ph.vaddr + ph.memsz)) == 0) goto bad; if(sz1 &gt;= PLIC) { // 添加检测，防止程序大小超过 PLIC goto bad; } sz = sz1; if(ph.vaddr % PGSIZE != 0) goto bad; if(loadseg(pagetable, ph.vaddr, ip, ph.off, ph.filesz) &lt; 0) goto bad; } iunlockput(ip); end_op(); ip = 0; // .......同步映射后面的步骤就是在每个修改到进程用户页表的位置，都将相应的修改同步到进程内核页表中。一共要修改：fork()、exec()、growproc()、userinit()。fork()// kernel/proc.cintfork(void){ // ...... // Copy user memory from parent to child. （调用 kvmcopymappings，将**新进程**用户页表映射拷贝一份到新进程内核页表中） if(uvmcopy(p-&gt;pagetable, np-&gt;pagetable, p-&gt;sz) &lt; 0 || kvmcopymappings(np-&gt;pagetable, np-&gt;kernelpgtbl, 0, p-&gt;sz) &lt; 0){ freeproc(np); release(&amp;np-&gt;lock); return -1; } np-&gt;sz = p-&gt;sz; // ......}exec()// kernel/exec.cintexec(char *path, char **argv){ // ...... // Save program name for debugging. for(last=s=path; *s; s++) if(*s == '/') last = s+1; safestrcpy(p-&gt;name, last, sizeof(p-&gt;name)); // 清除内核页表中对程序内存的旧映射，然后重新建立映射。 uvmunmap(p-&gt;kernelpgtbl, 0, PGROUNDUP(oldsz)/PGSIZE, 0); kvmcopymappings(pagetable, p-&gt;kernelpgtbl, 0, sz); // Commit to the user image. oldpagetable = p-&gt;pagetable; p-&gt;pagetable = pagetable; p-&gt;sz = sz; p-&gt;trapframe-&gt;epc = elf.entry; // initial program counter = main p-&gt;trapframe-&gt;sp = sp; // initial stack pointer proc_freepagetable(oldpagetable, oldsz); // ......}growproc()// kernel/proc.cintgrowproc(int n){ uint sz; struct proc *p = myproc(); sz = p-&gt;sz; if(n &gt; 0){ uint64 newsz; if((newsz = uvmalloc(p-&gt;pagetable, sz, sz + n)) == 0) { return -1; } // 内核页表中的映射同步扩大 if(kvmcopymappings(p-&gt;pagetable, p-&gt;kernelpgtbl, sz, n) != 0) { uvmdealloc(p-&gt;pagetable, newsz, sz); return -1; } sz = newsz; } else if(n &lt; 0){ uvmdealloc(p-&gt;pagetable, sz, sz + n); // 内核页表中的映射同步缩小 sz = kvmdealloc(p-&gt;kernelpgtbl, sz, sz + n); } p-&gt;sz = sz; return 0;}userinit()对于 init 进程，由于不像其他进程，init 不是 fork 得来的，所以需要在 userinit 中也添加同步映射的代码。// kernel/proc.cvoiduserinit(void){ // ...... // allocate one user page and copy init's instructions // and data into it. uvminit(p-&gt;pagetable, initcode, sizeof(initcode)); p-&gt;sz = PGSIZE; kvmcopymappings(p-&gt;pagetable, p-&gt;kernelpgtbl, 0, p-&gt;sz); // 同步程序内存映射到进程内核页表中 // ......}到这里，两个页表的同步操作就都完成了。替换 copyin、copyinstr 实现// kernel/vm.c// 声明新函数原型int copyin_new(pagetable_t pagetable, char *dst, uint64 srcva, uint64 len);int copyinstr_new(pagetable_t pagetable, char *dst, uint64 srcva, uint64 max);// 将 copyin、copyinstr 改为转发到新函数intcopyin(pagetable_t pagetable, char *dst, uint64 srcva, uint64 len){ return copyin_new(pagetable, dst, srcva, len);}intcopyinstr(pagetable_t pagetable, char *dst, uint64 srcva, uint64 max){ return copyinstr_new(pagetable, dst, srcva, max);}运行 grade：pte printout: OK (4.8s) == Test answers-pgtbl.txt == answers-pgtbl.txt: OK == Test count copyin == $ make qemu-gdbcount copyin: OK (1.1s) == Test usertests == $ make qemu-gdb(141.1s) == Test usertests: copyin == usertests: copyin: OK == Test usertests: copyinstr1 == usertests: copyinstr1: OK == Test usertests: copyinstr2 == usertests: copyinstr2: OK == Test usertests: copyinstr3 == usertests: copyinstr3: OK == Test usertests: sbrkmuch == usertests: sbrkmuch: OK == Test usertests: all tests == usertests: all tests: OK == Test time == time: OK Score: 66/66Optional challenges Use super-pages to reduce the number of PTEs in page tables.（跳过） Extend your solution to support user programs that are as large as possible; that is, eliminate the restriction that user programs be smaller than PLIC.（跳过） Unmap the first page of a user process so that dereferencing a null pointer will result in a fault. You will have to start the user text segment at, for example, 4096, instead of 0.（跳过）" }, { "title": "[mit6.s081] 笔记 Lab2: System calls | 系统调用", "url": "/posts/s081-lab2-system-calls/", "categories": "Course Notes, MIT6.S081", "tags": "operating system", "date": "2021-09-09 19:00:00 +0800", "snippet": " 这是我自学 MIT6.S081 操作系统课程的 lab 代码笔记第二篇：System calls。此 lab 大致耗时：4小时。 课程地址：https://pdos.csail.mit.edu/6.S081/2020/schedule.htmlLab 地址：https://pdos.csail.mit.edu/6.S081/2020/labs/syscall.html我的代码地址：https://github.com/Miigon/my-xv6-labs-2020/tree/syscallCommits: https://github.com/Miigon/my-xv6-labs-2020/commits/syscall 本文中代码注释是编写博客的时候加入的，原仓库中的代码可能缺乏注释或代码不完全相同。Lab 2: System callsIn this lab you will add some new system calls to xv6, which will help you understand how they work and will expose you to some of the internals of the xv6 kernel. You will add more system calls in later labs.对 xv6 添加一些新的系统调用，帮助加深对 xv6 内核的理解。System call tracing (moderate)准备环境，编译编译器、QEMU，克隆仓库，略过。 In this assignment you will add a system call tracing feature that may help you when debugging later labs. You’ll create a new trace system call that will control tracing. It should take one argument, an integer “mask”, whose bits specify which system calls to trace. For example, to trace the fork system call, a program calls trace(1 « SYS_fork), where SYS_fork is a syscall number from kernel/syscall.h. You have to modify the xv6 kernel to print out a line when each system call is about to return, if the system call’s number is set in the mask. The line should contain the process id, the name of the system call and the return value; you don’t need to print the system call arguments. The trace system call should enable tracing for the process that calls it and any children that it subsequently forks, but should not affect other processes.添加一个系统调用 trace 的功能，为每个进程设定一个位 mask，用 mask 中设定的位来指定要为哪些系统调用输出调试信息。如何创建新系统调用 首先在内核中合适的位置（取决于要实现的功能属于什么模块，理论上随便放都可以，只是主要起归类作用），实现我们的内核调用（在这里是 trace 调用）： // kernel/sysproc.c // 这里着重理解如何添加系统调用，对于这个调用的具体代码细节在后面的部分分析 uint64 sys_trace(void) { int mask; if(argint(0, &amp;mask) &lt; 0) return -1;\t myproc()-&gt;syscall_trace = mask; return 0; } 这里因为我们的系统调用会对进程进行操作，所以放在 sysproc.c 较为合适。 在 syscall.h 中加入新 system call 的序号： // kernel/syscall.h // System call numbers #define SYS_fork 1 #define SYS_exit 2 #define SYS_wait 3 #define SYS_pipe 4 #define SYS_read 5 #define SYS_kill 6 #define SYS_exec 7 #define SYS_fstat 8 #define SYS_chdir 9 #define SYS_dup 10 #define SYS_getpid 11 #define SYS_sbrk 12 #define SYS_sleep 13 #define SYS_uptime 14 #define SYS_open 15 #define SYS_write 16 #define SYS_mknod 17 #define SYS_unlink 18 #define SYS_link 19 #define SYS_mkdir 20 #define SYS_close 21 #define SYS_trace 22 // here!!!!! 用 extern 全局声明新的内核调用函数，并且在 syscalls 映射表中，加入从前面定义的编号到系统调用函数指针的映射 // kernel/syscall.c extern uint64 sys_chdir(void); extern uint64 sys_close(void); extern uint64 sys_dup(void); extern uint64 sys_exec(void); extern uint64 sys_exit(void); extern uint64 sys_fork(void); extern uint64 sys_fstat(void); extern uint64 sys_getpid(void); extern uint64 sys_kill(void); extern uint64 sys_link(void); extern uint64 sys_mkdir(void); extern uint64 sys_mknod(void); extern uint64 sys_open(void); extern uint64 sys_pipe(void); extern uint64 sys_read(void); extern uint64 sys_sbrk(void); extern uint64 sys_sleep(void); extern uint64 sys_unlink(void); extern uint64 sys_wait(void); extern uint64 sys_write(void); extern uint64 sys_uptime(void); extern uint64 sys_trace(void); // HERE static uint64 (*syscalls[])(void) = { [SYS_fork] sys_fork, [SYS_exit] sys_exit, [SYS_wait] sys_wait, [SYS_pipe] sys_pipe, [SYS_read] sys_read, [SYS_kill] sys_kill, [SYS_exec] sys_exec, [SYS_fstat] sys_fstat, [SYS_chdir] sys_chdir, [SYS_dup] sys_dup, [SYS_getpid] sys_getpid, [SYS_sbrk] sys_sbrk, [SYS_sleep] sys_sleep, [SYS_uptime] sys_uptime, [SYS_open] sys_open, [SYS_write] sys_write, [SYS_mknod] sys_mknod, [SYS_unlink] sys_unlink, [SYS_link] sys_link, [SYS_mkdir] sys_mkdir, [SYS_close] sys_close, [SYS_trace] sys_trace, // AND HERE }; 这里 [SYS_trace] sys_trace 是 C 语言数组的一个语法，表示以方括号内的值作为元素下标。比如 int arr[] = {[3] 2333, [6] 6666} 代表 arr 的下标 3 的元素为 2333，下标 6 的元素为 6666，其他元素填充 0 的数组。（该语法在 C++ 中已不可用） 在 usys.pl 中，加入用户态到内核态的跳板函数。 # user/usys.pl entry(\"fork\"); entry(\"exit\"); entry(\"wait\"); entry(\"pipe\"); entry(\"read\"); entry(\"write\"); entry(\"close\"); entry(\"kill\"); entry(\"exec\"); entry(\"open\"); entry(\"mknod\"); entry(\"unlink\"); entry(\"fstat\"); entry(\"link\"); entry(\"mkdir\"); entry(\"chdir\"); entry(\"dup\"); entry(\"getpid\"); entry(\"sbrk\"); entry(\"sleep\"); entry(\"uptime\"); entry(\"trace\"); # HERE 这个脚本在运行后会生成 usys.S 汇编文件，里面定义了每个 system call 的用户态跳板函数： trace:\t\t# 定义用户态跳板函数 li a7, SYS_trace\t# 将系统调用 id 存入 a7 寄存器 ecall\t\t\t\t# ecall，调用 system call ，跳到内核态的统一系统调用处理函数 syscall() (syscall.c) ret 在用户态的头文件加入定义，使得用户态程序可以找到这个跳板入口函数。 // user/user.h // system calls int fork(void); int exit(int) __attribute__((noreturn)); int wait(int*); int pipe(int*); int write(int, const void*, int); int read(int, void*, int); int close(int); int kill(int); int exec(char*, char**); int open(const char*, int); int mknod(const char*, short, short); int unlink(const char*); int fstat(int fd, struct stat*); int link(const char*, const char*); int mkdir(const char*); int chdir(const char*); int dup(int); int getpid(void); char* sbrk(int); int sleep(int); int uptime(void); int trace(int);\t\t// HERE 系统调用全流程user/user.h:\t\t用户态程序调用跳板函数 trace()user/usys.S:\t\t跳板函数 trace() 使用 CPU 提供的 ecall 指令，调用到内核态kernel/syscall.c\t到达内核态统一系统调用处理函数 syscall()，所有系统调用都会跳到这里来处理。kernel/syscall.c\tsyscall() 根据跳板传进来的系统调用编号，查询 syscalls[] 表，找到对应的内核函数并调用。kernel/sysproc.c\t到达 sys_trace() 函数，执行具体内核操作这么繁琐的调用流程的主要目的是实现用户态和内核态的良好隔离。并且由于内核与用户进程的页表不同，寄存器也不互通，所以参数无法直接通过 C 语言参数的形式传过来，而是需要使用 argaddr、argint、argstr 等系列函数，从进程的 trapframe 中读取用户进程寄存器中的参数。同时由于页表不同，指针也不能直接互通访问（也就是内核不能直接对用户态传进来的指针进行解引用），而是需要使用 copyin、copyout 方法结合进程的页表，才能顺利找到用户态指针（逻辑地址）对应的物理内存地址。（在本 lab 第二个实验会用到）struct proc *p = myproc(); // 获取调用该 system call 的进程的 proc 结构copyout(p-&gt;pagetable, addr, (char *)&amp;data, sizeof(data)); // 将内核态的 data 变量（常为struct），结合进程的页表，写到进程内存空间内的 addr 地址处。该 lab 代码首先在 proc.h 中修改 proc 结构的定义，添加 syscall_trace field，用 mask 的方式记录要 trace 的 system call。// kernel/proc.h// Per-process statestruct proc { struct spinlock lock; // p-&gt;lock must be held when using these: enum procstate state; // Process state struct proc *parent; // Parent process void *chan; // If non-zero, sleeping on chan int killed; // If non-zero, have been killed int xstate; // Exit status to be returned to parent's wait int pid; // Process ID // these are private to the process, so p-&gt;lock need not be held. uint64 kstack; // Virtual address of kernel stack uint64 sz; // Size of process memory (bytes) pagetable_t pagetable; // User page table struct trapframe *trapframe; // data page for trampoline.S struct context context; // swtch() here to run process struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory char name[16]; // Process name (debugging) uint64 syscall_trace; // Mask for syscall tracing (新添加的用于标识追踪哪些 system call 的 mask)};在 proc.c 中，创建新进程的时候，为新添加的 syscall_trace 附上默认值 0（否则初始状态下可能会有垃圾数据）。// kernel/proc.cstatic struct proc*allocproc(void){ ...... memset(&amp;p-&gt;context, 0, sizeof(p-&gt;context)); p-&gt;context.ra = (uint64)forkret; p-&gt;context.sp = p-&gt;kstack + PGSIZE; p-&gt;syscall_trace = 0; // (newly added) 为 syscall_trace 设置一个 0 的默认值 return p;}在 sysproc.c 中，实现 system call 的具体代码，也就是设置当前进程的 syscall_trace mask：// kernel/sysproc.cuint64sys_trace(void){ int mask; if(argint(0, &amp;mask) &lt; 0) // 通过读取进程的 trapframe，获得 mask 参数 return -1; myproc()-&gt;syscall_trace = mask; // 设置调用进程的 syscall_trace mask return 0;}修改 fork 函数，使得子进程可以继承父进程的 syscall_trace mask：// kernel/proc.cintfork(void){ ...... safestrcpy(np-&gt;name, p-&gt;name, sizeof(p-&gt;name)); np-&gt;syscall_trace = p-&gt;syscall_trace; // HERE!!! 子进程继承父进程的 syscall_trace pid = np-&gt;pid; np-&gt;state = RUNNABLE; release(&amp;np-&gt;lock); return pid;}根据上方提到的系统调用的全流程，可以知道，所有的系统调用到达内核态后，都会进入到 syscall() 这个函数进行处理，所以要跟踪所有的内核函数，只需要在 syscall() 函数里埋点就行了。// kernel/syscall.cvoidsyscall(void){ int num; struct proc *p = myproc(); num = p-&gt;trapframe-&gt;a7; if(num &gt; 0 &amp;&amp; num &lt; NELEM(syscalls) &amp;&amp; syscalls[num]) { // 如果系统调用编号有效 p-&gt;trapframe-&gt;a0 = syscalls[num](); // 通过系统调用编号，获取系统调用处理函数的指针，调用并将返回值存到用户进程的 a0 寄存器中\t// 如果当前进程设置了对该编号系统调用的 trace，则打出 pid、系统调用名称和返回值。 if((p-&gt;syscall_trace &gt;&gt; num) &amp; 1) { printf(\"%d: syscall %s -&gt; %d\\n\",p-&gt;pid, syscall_names[num], p-&gt;trapframe-&gt;a0); // syscall_names[num]: 从 syscall 编号到 syscall 名的映射表 } } else { printf(\"%d %s: unknown sys call %d\\n\", p-&gt;pid, p-&gt;name, num); p-&gt;trapframe-&gt;a0 = -1; }}上面打出日志的过程还需要知道系统调用的名称字符串，在这里定义一个字符串数组进行映射：// kernel/syscall.cconst char *syscall_names[] = {[SYS_fork] \"fork\",[SYS_exit] \"exit\",[SYS_wait] \"wait\",[SYS_pipe] \"pipe\",[SYS_read] \"read\",[SYS_kill] \"kill\",[SYS_exec] \"exec\",[SYS_fstat] \"fstat\",[SYS_chdir] \"chdir\",[SYS_dup] \"dup\",[SYS_getpid] \"getpid\",[SYS_sbrk] \"sbrk\",[SYS_sleep] \"sleep\",[SYS_uptime] \"uptime\",[SYS_open] \"open\",[SYS_write] \"write\",[SYS_mknod] \"mknod\",[SYS_unlink] \"unlink\",[SYS_link] \"link\",[SYS_mkdir] \"mkdir\",[SYS_close] \"close\",[SYS_trace] \"trace\",};编译执行：$ trace 32 grep hello README3: syscall read -&gt; 10233: syscall read -&gt; 9663: syscall read -&gt; 703: syscall read -&gt; 0$$ trace 2147483647 grep hello README4: syscall trace -&gt; 04: syscall exec -&gt; 34: syscall open -&gt; 34: syscall read -&gt; 10234: syscall read -&gt; 9664: syscall read -&gt; 704: syscall read -&gt; 04: syscall close -&gt; 0$成功追踪并打印出相应的系统调用。Sysinfo (moderate) In this assignment you will add a system call, sysinfo, that collects information about the running system. The system call takes one argument: a pointer to a struct sysinfo (see kernel/sysinfo.h). The kernel should fill out the fields of this struct: the freemem field should be set to the number of bytes of free memory, and the nproc field should be set to the number of processes whose state is not UNUSED. We provide a test program sysinfotest; you pass this assignment if it prints “sysinfotest: OK”.添加一个系统调用，返回空闲的内存、以及已创建的进程数量。大多数步骤和上个实验是一样的，所以不再描述。唯一不同就是需要把结构体从内核内存拷贝到用户进程内存中。其他的难点可能就是在如何获取空闲内存和如何获取已创建进程上面了，因为涉及到了一些后面的知识。获取空闲内存在内核的头文件中声明计算空闲内存的函数，因为是内存相关的，所以放在 kalloc、kfree 等函数的的声明之后。// kernel/defs.hvoid* kalloc(void);void kfree(void *);void kinit(void);uint64 \t\t\tcount_free_mem(void); // here在 kalloc.c 中添加计算空闲内存的函数：// kernel/kalloc.cuint64count_free_mem(void) // added for counting free memory in bytes (lab2){ acquire(&amp;kmem.lock); // 必须先锁内存管理结构，防止竞态条件出现 // 统计空闲页数，乘上页大小 PGSIZE 就是空闲的内存字节数 uint64 mem_bytes = 0; struct run *r = kmem.freelist; while(r){ mem_bytes += PGSIZE; r = r-&gt;next; } release(&amp;kmem.lock); return mem_bytes;}xv6 中，空闲内存页的记录方式是，将空虚内存页本身直接用作链表节点，形成一个空闲页链表，每次需要分配，就把链表根部对应的页分配出去。每次需要回收，就把这个页作为新的根节点，把原来的 freelist 链表接到后面。注意这里是直接使用空闲页本身作为链表节点，所以不需要使用额外空间来存储空闲页链表，在 kalloc() 里也可以看到，分配内存的最后一个阶段，是直接将 freelist 的根节点地址（物理地址）返回出去了：// kernel/kalloc.c// Allocate one 4096-byte page of physical memory.// Returns a pointer that the kernel can use.// Returns 0 if the memory cannot be allocated.void *kalloc(void){ struct run *r; acquire(&amp;kmem.lock); r = kmem.freelist; // 获得空闲页链表的根节点 if(r) kmem.freelist = r-&gt;next; release(&amp;kmem.lock); if(r) memset((char*)r, 5, PGSIZE); // fill with junk return (void*)r; // 把空闲页链表的根节点返回出去，作为内存页使用（长度是 4096）}常见的记录空闲页的方法有：空闲表法、空闲链表法、位示图法（位图法）、成组链接法。这里 xv6 采用的是空闲链表法。获取运行的进程数同样在内核的头文件中添加函数声明：// kernel/defs.h......void sleep(void*, struct spinlock*);void userinit(void);int wait(uint64);void wakeup(void*);void yield(void);int either_copyout(int user_dst, uint64 dst, void *src, uint64 len);int either_copyin(void *dst, int user_src, uint64 src, uint64 len);void procdump(void);uint64\t\t\tcount_process(void); // here在 proc.c 中实现该函数：uint64count_process(void) { // added function for counting used process slots (lab2) uint64 cnt = 0; for(struct proc *p = proc; p &lt; &amp;proc[NPROC]; p++) { // acquire(&amp;p-&gt;lock); // 不需要锁进程 proc 结构，因为我们只需要读取进程列表，不需要写 if(p-&gt;state != UNUSED) { // 不是 UNUSED 的进程位，就是已经分配的 cnt++; } } return cnt;}实现 sysinfo 系统调用添加系统调用的流程与实验 1 类似，不再赘述。这是具体系统信息函数的实现，其中调用了前面实现的 count_free_mem() 和 count_process()：uint64sys_sysinfo(void){ // 从用户态读入一个指针，作为存放 sysinfo 结构的缓冲区 uint64 addr; if(argaddr(0, &amp;addr) &lt; 0) return -1; struct sysinfo sinfo; sinfo.freemem = count_free_mem(); // kalloc.c sinfo.nproc = count_process(); // proc.c // 使用 copyout，结合当前进程的页表，获得进程传进来的指针（逻辑地址）对应的物理地址 // 然后将 &amp;sinfo 中的数据复制到该指针所指位置，供用户进程使用。 if(copyout(myproc()-&gt;pagetable, addr, (char *)&amp;sinfo, sizeof(sinfo)) &lt; 0) return -1; return 0;}在 user.h 提供用户态入口：// user.hchar* sbrk(int);int sleep(int);int uptime(void);int trace(int);struct sysinfo; // 这里要声明一下 sysinfo 结构，供用户态使用。int sysinfo(struct sysinfo *);编译运行：$ sysinfotestsysinfotest: startsysinfotest: OKOptional challengesPrint the system call arguments for traced system calls (easy). （跳过）Compute the load average and export it through sysinfo(moderate). （跳过）" }, { "title": "[mit6.s081] 笔记 Lab1: Unix utilities | Unix 实用工具", "url": "/posts/s081-lab1-unix-utilities/", "categories": "Course Notes, MIT6.S081", "tags": "operating system", "date": "2021-09-07 17:00:00 +0800", "snippet": " 这是我自学 MIT6.S081 操作系统课程的 lab 代码笔记第一篇：Unix utilities。此 lab 大致耗时：4小时。 课程地址：https://pdos.csail.mit.edu/6.S081/2020/schedule.htmlLab 地址：https://pdos.csail.mit.edu/6.S081/2020/labs/util.html我的代码地址：https://github.com/Miigon/my-xv6-labs-2020/tree/utilCommits: https://github.com/Miigon/my-xv6-labs-2020/commits/util 本文中代码注释是编写博客的时候加入的，原仓库中的代码可能缺乏注释或代码不完全相同。Lab 1: Unix utilitiesThis lab will familiarize you with xv6 and its system calls.实现几个 unix 实用工具，熟悉 xv6 的开发环境以及系统调用。Boot xv6 (easy)准备环境，编译编译器、QEMU，克隆仓库，略过。$ git clone git://g.csail.mit.edu/xv6-labs-2020$ cd xv6-labs-2020$ git checkout util$ make qemusleep (easy) Implement the UNIX program sleep for xv6; your sleep should pause for a user-specified number of ticks. A tick is a notion of time defined by the xv6 kernel, namely the time between two interrupts from the timer chip. Your solution should be in the file user/sleep.c.练手题，记得在 Makefile 中将 sleep 加入构建目标里。// sleep.c#include \"kernel/types.h\"#include \"kernel/stat.h\"#include \"user/user.h\" // 必须以这个顺序 include，由于三个头文件有依赖关系int main(int argc, char **argv) {\tif(argc &lt; 2) {\t\tprintf(\"usage: sleep &lt;ticks&gt;\\n\");\t}\tsleep(atoi(argv[1]));\texit(0);}UPROGS=\\\t$U/_cat\\\t$U/_echo\\\t$U/_forktest\\\t$U/_grep\\\t$U/_init\\\t$U/_kill\\\t$U/_ln\\\t$U/_ls\\\t$U/_mkdir\\\t$U/_rm\\\t$U/_sh\\\t$U/_stressfs\\\t$U/_usertests\\\t$U/_grind\\\t$U/_wc\\\t$U/_zombie\\\t$U/_sleep\\ . # here !!!pingpong (easy) Write a program that uses UNIX system calls to “ping-pong” a byte between two processes over a pair of pipes, one for each direction. The parent should send a byte to the child; the child should print “: received ping\", where is its process ID, write the byte on the pipe to the parent, and exit; the parent should read the byte from the child, print \": received pong\", and exit. Your solution should be in the file user/pingpong.c.管道练手题，使用 fork() 复制本进程创建子进程，创建两个管道，分别用于父子之间两个方向的数据传输。// pingpong.c#include \"kernel/types.h\"#include \"kernel/stat.h\"#include \"user/user.h\"int main(int argc, char **argv) {\t// 创建管道会得到一个长度为 2 的 int 数组\t// 其中 0 为用于从管道读取数据的文件描述符，1 为用于向管道写入数据的文件描述符\tint pp2c[2], pc2p[2];\tpipe(pp2c); // 创建用于 父进程 -&gt; 子进程 的管道\tpipe(pc2p); // 创建用于 子进程 -&gt; 父进程 的管道\t\tif(fork() != 0) { // parent process\t\twrite(pp2c[1], \"!\", 1); // 1. 父进程首先向发出该字节\t\tchar buf;\t\tread(pc2p[0], &amp;buf, 1); // 2. 父进程发送完成后，开始等待子进程的回复\t\tprintf(\"%d: received pong\\n\", getpid()); // 5. 子进程收到数据，read 返回，输出 pong\t\twait(0);\t} else { // child process\t\tchar buf;\t\tread(pp2c[0], &amp;buf, 1); // 3. 子进程读取管道，收到父进程发送的字节数据\t\tprintf(\"%d: received ping\\n\", getpid());\t\twrite(pc2p[1], &amp;buf, 1); // 4. 子进程通过 子-&gt;父 管道，将字节送回父进程\t}\texit(0);}注：序号只为方便理解，实际执行顺序由于两进程具体调度情况不定，不一定严格按照该顺序执行，但是结果相同。$ pingpong4: received ping3: received pong$primes (moderate) / (hard) Write a concurrent version of prime sieve using pipes. This idea is due to Doug McIlroy, inventor of Unix pipes. The picture halfway down this page and the surrounding text explain how to do it. Your solution should be in the file user/primes.c.十分好玩的一道题hhhhh，使用多进程和管道，每一个进程作为一个 stage，筛掉某个素数的所有倍数（筛法）。很巧妙的形式实现了多线程的筛法求素数。具体原理和流程可以看课程提供的这篇文章。主进程：生成 n ∈ [2,35] -&gt; 子进程1：筛掉所有 2 的倍数 -&gt; 子进程2：筛掉所有 3 的倍数 -&gt; 子进程3：筛掉所有 5 的倍数 -&gt; .....每一个 stage 以当前数集中最小的数字作为素数输出（每个 stage 中数集中最小的数一定是一个素数，因为它没有被任何比它小的数筛掉），并筛掉输入中该素数的所有倍数（必然不是素数），然后将剩下的数传递给下一 stage。最后会形成一条子进程链，而由于每一个进程都调用了 wait(0); 等待其子进程，所以会在最末端也就是最后一个 stage 完成的时候，沿着链条向上依次退出各个进程。// primes.c#include \"kernel/types.h\"#include \"kernel/stat.h\"#include \"user/user.h\"// 一次 sieve 调用是一个筛子阶段，会从 pleft 获取并输出一个素数 p，筛除 p 的所有倍数// 同时创建下一 stage 的进程以及相应输入管道 pright，然后将剩下的数传到下一 stage 处理void sieve(int pleft[2]) { // pleft 是来自该 stage 左端进程的输入管道\tint p;\tread(pleft[0], &amp;p, sizeof(p)); // 读第一个数，必然是素数\tif(p == -1) { // 如果是哨兵 -1，则代表所有数字处理完毕，退出程序\t\texit(0);\t}\tprintf(\"prime %d\\n\", p);\tint pright[2];\tpipe(pright); // 创建用于输出到下一 stage 的进程的输出管道 pright\tif(fork() == 0) {\t\t// 子进程 （下一个 stage）\t\tclose(pright[1]); // 子进程只需要对输入管道 pright 进行读，而不需要写，所以关掉子进程的输入管道写文件描述符，降低进程打开的文件描述符数量\t\tclose(pleft[0]); // 这里的 pleft 是*父进程*的输入管道，子进程用不到，关掉\t\tsieve(pright); // 子进程以父进程的输出管道作为输入，开始进行下一个 stage 的处理。\t} else {\t\t// 父进程 （当前 stage）\t\tclose(pright[0]); // 同上，父进程只需要对子进程的输入管道进行写而不需要读，所以关掉父进程的读文件描述符\t\tint buf;\t\twhile(read(pleft[0], &amp;buf, sizeof(buf)) &amp;&amp; buf != -1) { // 从左端的进程读入数字\t\t\tif(buf % p != 0) { // 筛掉能被该进程筛掉的数字\t\t\t\twrite(pright[1], &amp;buf, sizeof(buf)); // 将剩余的数字写到右端进程\t\t\t}\t\t}\t\tbuf = -1;\t\twrite(pright[1], &amp;buf, sizeof(buf)); // 补写最后的 -1，标示输入完成。\t\twait(0); // 等待该进程的子进程完成，也就是下一 stage\t\texit(0);\t}}int main(int argc, char **argv) {\t// 主进程\tint input_pipe[2];\tpipe(input_pipe); // 准备好输入管道，输入 2 到 35 之间的所有整数。\tif(fork() == 0) {\t\t// 第一个 stage 的子进程\t\tclose(input_pipe[1]); // 子进程只需要读输入管道，而不需要写，关掉子进程的管道写文件描述符\t\tsieve(input_pipe);\t\texit(0);\t} else {\t\t// 主进程\t\tclose(input_pipe[0]); // 同上\t\tint i;\t\tfor(i=2;i&lt;=35;i++){ // 生成 [2, 35]，输入管道链最左端\t\t\twrite(input_pipe[1], &amp;i, sizeof(i));\t\t}\t\ti = -1;\t\twrite(input_pipe[1], &amp;i, sizeof(i)); // 末尾输入 -1，用于标识输入完成\t}\twait(0); // 等待第一个 stage 完成。注意：这里无法等待子进程的子进程，只能等待直接子进程，无法等待间接子进程。在 sieve() 中会为每个 stage 再各自执行 wait(0)，形成等待链。\texit(0);}这一道的主要坑就是，stage 之间的管道 pleft 和 pright，要注意关闭不需要用到的文件描述符，否则跑到 n = 13 的时候就会爆掉，出现读到全是 0 的情况。prime 2prime 3prime 5prime 7prime 11prime 13prime 0prime 0prime 0prime 0prime 0prime 0...这里的理由是，fork 会将父进程的所有文件描述符都复制到子进程里，而 xv6 每个进程能打开的文件描述符总数只有 16 个 （见 defs.h 中的 NOFILE 和 proc.h 中的 struct file *ofile[NOFILE]; // Open files）。由于一个管道会同时打开一个输入文件和一个输出文件，所以一个管道就占用了 2 个文件描述符，并且复制的子进程还会复制父进程的描述符，于是跑到第六七层后，就会由于最末端的子进程出现 16 个文件描述符都被占满的情况，导致新管道创建失败。解决方法有两部分： 关闭管道的两个方向中不需要用到的方向的文件描述符（在具体进程中将管道变成只读/只写） 原理：每个进程从左侧的读入管道中只需要读数据，并且只需要写数据到右侧的输出管道，所以可以把左侧管道的写描述符，以及右侧管道的读描述符关闭，而不会影响程序运行 这里注意文件描述符是进程独立的，在某个进程内关闭文件描述符，不会影响到其他进程 子进程创建后，关闭父进程与祖父进程之间的文件描述符（因为子进程并不需要用到之前 stage 的管道）具体的操作在上面代码中有体现。（fork 后、执行操作前，close 掉不需要用掉的文件描述符）$ primesprime 2prime 3prime 5prime 7prime 11prime 13prime 17prime 19prime 23prime 29prime 31$find (moderate) Write a simple version of the UNIX find program: find all the files in a directory tree with a specific name. Your solution should be in the file user/find.c.这里基本原理与 ls 相同，基本上可以从 ls.c 改造得到：// find.c#include \"kernel/types.h\"#include \"kernel/stat.h\"#include \"user/user.h\"#include \"kernel/fs.h\"void find(char *path, char *target) {\tchar buf[512], *p;\tint fd;\tstruct dirent de;\tstruct stat st;\tif((fd = open(path, 0)) &lt; 0){\t\tfprintf(2, \"find: cannot open %s\\n\", path);\t\treturn;\t}\tif(fstat(fd, &amp;st) &lt; 0){\t\tfprintf(2, \"find: cannot stat %s\\n\", path);\t\tclose(fd);\t\treturn;\t}\tswitch(st.type){\tcase T_FILE:\t\t// 如果文件名结尾匹配 `/target`，则视为匹配\t\tif(strcmp(path+strlen(path)-strlen(target), target) == 0) {\t\t\tprintf(\"%s\\n\", path);\t\t}\t\tbreak;\tcase T_DIR:\t\tif(strlen(path) + 1 + DIRSIZ + 1 &gt; sizeof buf){\t\t\tprintf(\"find: path too long\\n\");\t\t\tbreak;\t\t}\t\tstrcpy(buf, path);\t\tp = buf+strlen(buf);\t\t*p++ = '/';\t\twhile(read(fd, &amp;de, sizeof(de)) == sizeof(de)){\t\t\tif(de.inum == 0)\t\t\t\tcontinue;\t\t\tmemmove(p, de.name, DIRSIZ);\t\t\tp[DIRSIZ] = 0;\t\t\tif(stat(buf, &amp;st) &lt; 0){\t\t\t\tprintf(\"find: cannot stat %s\\n\", buf);\t\t\t\tcontinue;\t\t\t}\t\t\t// 不要进入 `.` 和 `..`\t\t\tif(strcmp(buf+strlen(buf)-2, \"/.\") != 0 &amp;&amp; strcmp(buf+strlen(buf)-3, \"/..\") != 0) {\t\t\t\tfind(buf, target); // 递归查找\t\t\t}\t\t}\t\tbreak;\t}\tclose(fd);}int main(int argc, char *argv[]){\tif(argc &lt; 3){\t\texit(0);\t}\tchar target[512];\ttarget[0] = '/'; // 为查找的文件名添加 / 在开头\tstrcpy(target+1, argv[2]);\tfind(argv[1], target);\texit(0);}$ find . b ./b ./a/bxargs (moderate) Write a simple version of the UNIX xargs program: read lines from the standard input and run a command for each line, supplying the line as arguments to the command. Your solution should be in the file user/xargs.c.编写 xargs 工具，从标准输入读入数据，将每一行当作参数，加入到传给 xargs 的程序名和参数后面作为额外参数，然后执行。// xargs.c#include \"kernel/types.h\"#include \"kernel/stat.h\"#include \"user/user.h\"#include \"kernel/fs.h\"// 带参数列表，执行某个程序void run(char *program, char **args) {\tif(fork() == 0) { // child exec\t\texec(program, args);\t\texit(0);\t}\treturn; // parent return}int main(int argc, char *argv[]){\tchar buf[2048]; // 读入时使用的内存池\tchar *p = buf, *last_p = buf; // 当前参数的结束、开始指针\tchar *argsbuf[128]; // 全部参数列表，字符串指针数组，包含 argv 传进来的参数和 stdin 读入的参数\tchar **args = argsbuf; // 指向 argsbuf 中第一个从 stdin 读入的参数\tfor(int i=1;i&lt;argc;i++) {\t\t// 将 argv 提供的参数加入到最终的参数列表中\t\t*args = argv[i];\t\targs++;\t}\tchar **pa = args; // 开始读入参数\twhile(read(0, p, 1) != 0) {\t\tif(*p == ' ' || *p == '\\n') {\t\t\t// 读入一个参数完成（以空格分隔，如 `echo hello world`，则 hello 和 world 各为一个参数）\t\t\t*p = '\\0';\t// 将空格替换为 \\0 分割开各个参数，这样可以直接使用内存池中的字符串作为参数字符串\t\t\t\t\t\t// 而不用额外开辟空间\t\t\t*(pa++) = last_p;\t\t\tlast_p = p+1;\t\t\tif(*p == '\\n') {\t\t\t\t// 读入一行完成\t\t\t\t*pa = 0; // 参数列表末尾用 null 标识列表结束\t\t\t\trun(argv[1], argsbuf); // 执行最后一行指令\t\t\t\tpa = args; // 重置读入参数指针，准备读入下一行\t\t\t}\t\t}\t\tp++;\t}\tif(pa != args) { // 如果最后一行不是空行\t\t// 收尾最后一个参数\t\t*p = '\\0';\t\t*(pa++) = last_p;\t\t// 收尾最后一行\t\t*pa = 0; // 参数列表末尾用 null 标识列表结束\t\t// 执行最后一行指令\t\trun(argv[1], argsbuf);\t}\twhile(wait(0) != -1) {}; // 循环等待所有子进程完成，每一次 wait(0) 等待一个\texit(0);}上程序用到了一些指针与C字符串构成的取巧用法。Optional challenges额外 challenge，不是满分必要条件，会挑选有意思的或有意义的做。uptime (easy) Write an uptime program that prints the uptime in terms of ticks using the uptime system call. (easy)// uptime.c#include \"kernel/types.h\"#include \"kernel/stat.h\"#include \"user/user.h\"int main() {\tprintf(\"%d\\n\", uptime());\texit(0);}regex for find (easy) Support regular expressions in name matching for find. grep.c has some primitive support for regular expressions. (easy)从 grep.c 把 match 函数抄过来：// find.cint matchhere(char*, char*);int matchstar(int, char*, char*);int match(char *re, char *text) {\tif(re[0] == '^')\t\treturn matchhere(re+1, text);\tdo{ // must look at empty string\t\tif(matchhere(re, text))\t\treturn 1;\t}while(*text++ != '\\0');\treturn 0;}// matchhere: search for re at beginning of textint matchhere(char *re, char *text) {\tif(re[0] == '\\0')\t\treturn 1;\tif(re[1] == '*')\t\treturn matchstar(re[0], re+2, text);\tif(re[0] == '$' &amp;&amp; re[1] == '\\0')\t\treturn *text == '\\0';\tif(*text!='\\0' &amp;&amp; (re[0]=='.' || re[0]==*text))\t\treturn matchhere(re+1, text+1);\treturn 0;}// matchstar: search for c*re at beginning of textint matchstar(int c, char *re, char *text){\tdo{ // a * matches zero or more instances\t\tif(matchhere(re, text))\t\t\treturn 1;\t}while(*text!='\\0' &amp;&amp; (*text++==c || c=='.'));\treturn 0;}再将匹配规则改为用 match 匹配即可：// find.c// ....switch(st.type){case T_FILE:\tif(match(target, path)) {\t\tprintf(\"%s\\n\", path);\t}\tbreak;case T_DIR:// ....improved shell not print a $ when processing shell commands from a file (moderate) - 完成改进前（输出了很多多余 $）：$ cat xargstest.sh | sh$ $ $ $ $ $ hellohellohello$ $ 改进后：$ sh xargstest.shhellohellohello$ modify the shell to support wait (easy) - 完成$ wait 20(wait 20 ticks with no output...)$ modify the shell to support lists of commands, separated by “;” (moderate) - 完成$ echo hello; echo world; echo 2333333!helloworld2333333!$ modify the shell to support sub-shells by implementing “(“ and “)” (moderate) - 跳过 modify the shell to support tab completion (easy) - 完成$ sh xar [tab][回车]auto-completed: xargstest.shhellohellohello$ ec [tab][空格] hello,world! [回车]auto-completed: echohello,world! modify the shell to keep a history of passed shell commands (moderate) - 跳过shell 代码过长，已经放到 GitHub: 修改记录：https://github.com/Miigon/my-xv6-labs-2020/commit/5f91ae357e5dbc031e4164e13141e6096596656d#diff-c5682e6f79d8e68b805047fc80c703adb4dbb0b972fa009bdfed1ea69dddd93f完整文件：https://github.com/Miigon/my-xv6-labs-2020/blob/5f91ae357e5dbc031e4164e13141e6096596656d/user/sh.c主要点在，系统提供的 gets 不足以满足我们的需求，所以将系统的 gets 实现拷贝到 sh.c 中，然后进行改进（支持对 \\t 的检测）。然后就是自动补全的实现，使用与 ls.c 相同的目录遍历逻辑，然后将前缀匹配的文件名自动替换到当前输入缓冲区内，实现自动补全。从文件执行 shell 脚本，由于 cat foobar.sh | sh 的形式，shell 收到的指令来自标准输入（无法分辨是来自文件还是来自用户输入），故加入一个参数，输入要执行的脚本文件名，然后另外打开该脚本执行，并判断不输出 $。" }, { "title": "[StackOverflow] Difference between synchronous and asynchorous gRPC API", "url": "/posts/so-difference-between-sync-and-async-grpc/", "categories": "Backend, Networking, gRPC", "tags": "C++, gRPC", "date": "2021-08-20 23:09:00 +0800", "snippet": " This is from one of my answers on StackOverflow. Original question: I am working on a service based on gRPC, which requires high throughput. But currently my program suffers low throughput when using C++ synchronous gRPC. I’ve read through gRPC documentations, but don’t find explicit explanation on the difference between sync/async APIs. Except async has control over completion queue, while it’s transparent to sync APIs. I want to know whether synchronous gRPC sends messages to TCP layer, and wait for its “ack”, thus the next message would be blocked? Meanwhile async APIs would send them asynchronously without latter messages waiting?TLDR: Yes, async APIs would send the messages asynchronously without latter messages waiting, while synchronous APIs will block the whole thread while one message is being sent/received.gRPC uses CompletionQueue for it’s asynchronous operations. You can find the official tutorial here: https://grpc.io/docs/languages/cpp/async/CompletionQueue is an event queue. “event” here can be the completion of a request data reception or the expiry of an alarm(timer), etc. (basically, the completion of any asynchronous operation.)Using the official gRPC asynchronous APIs example as example, focus on the CallData class and HandleRpcs(): void HandleRpcs() { // Spawn a new CallData instance to serve new clients. new CallData(&amp;service_, cq_.get()); void* tag; // uniquely identifies a request. bool ok; while (true) { // Block waiting to read the next event from the completion queue. The // event is uniquely identified by its tag, which in this case is the // memory address of a CallData instance. // The return value of Next should always be checked. This return value // tells us whether there is any kind of event or cq_ is shutting down. GPR_ASSERT(cq_-&gt;Next(&amp;tag, &amp;ok)); GPR_ASSERT(ok); static_cast&lt;CallData*&gt;(tag)-&gt;Proceed(); } }HandleRpcs() is the main loop of the server. It’s an infinite loop which continuously gets the next event from the completion queue by using cq-&gt;Next() , and calls it’s Proceed() method (our custom method to process client request of different states).The CallData class (instance of which represents a complete processing cycle of a client request): class CallData { public: // Take in the \"service\" instance (in this case representing an asynchronous // server) and the completion queue \"cq\" used for asynchronous communication // with the gRPC runtime. CallData(Greeter::AsyncService* service, ServerCompletionQueue* cq) : service_(service), cq_(cq), responder_(&amp;ctx_), status_(CREATE) { // Invoke the serving logic right away. Proceed(); } void Proceed() { if (status_ == CREATE) { // Make this instance progress to the PROCESS state. status_ = PROCESS; // As part of the initial CREATE state, we *request* that the system // start processing SayHello requests. In this request, \"this\" acts are // the tag uniquely identifying the request (so that different CallData // instances can serve different requests concurrently), in this case // the memory address of this CallData instance. service_-&gt;RequestSayHello(&amp;ctx_, &amp;request_, &amp;responder_, cq_, cq_, this); } else if (status_ == PROCESS) { // Spawn a new CallData instance to serve new clients while we process // the one for this CallData. The instance will deallocate itself as // part of its FINISH state. new CallData(service_, cq_); // The actual processing. std::string prefix(\"Hello \"); reply_.set_message(prefix + request_.name()); // And we are done! Let the gRPC runtime know we've finished, using the // memory address of this instance as the uniquely identifying tag for // the event. status_ = FINISH; responder_.Finish(reply_, Status::OK, this); } else { GPR_ASSERT(status_ == FINISH); // Once in the FINISH state, deallocate ourselves (CallData). delete this; } } private: // The means of communication with the gRPC runtime for an asynchronous // server. Greeter::AsyncService* service_; // The producer-consumer queue where for asynchronous server notifications. ServerCompletionQueue* cq_; // Context for the rpc, allowing to tweak aspects of it such as the use // of compression, authentication, as well as to send metadata back to the // client. ServerContext ctx_; // What we get from the client. HelloRequest request_; // What we send back to the client. HelloReply reply_; // The means to get back to the client. ServerAsyncResponseWriter&lt;HelloReply&gt; responder_; // Let's implement a tiny state machine with the following states. enum CallStatus { CREATE, PROCESS, FINISH }; CallStatus status_; // The current serving state. };As we can see, a CallData has three states: CREATE, PROCESS and FINISH.A request routine looks like this: At startup, preallocates one CallData for a future incoming client. During the construction of that CallData object, service_-&gt;RequestSayHello(&amp;ctx_, &amp;request_, &amp;responder_, cq_, cq_, this) gets called, which tells gRPC to prepare for the reception of exactly one SayHello request.At this point we don’t know where the request will come from or when it will come, we are just telling gRPC that we are ready to process when one actually arrives, and let gRPC notice us when it happens.Arguments to RequestSayHello tells gRPC where to put the context, request body and responder of the request after receiving one, as well as which completion queue to use for the notice and what tags should be attached to the notice event (in this case, this is used as the tag). HandleRpcs() blocks on cq-&gt;Next(). Waiting for an event to occur.some time later…. client makes a SayHello request to the server, gRPC starts receiving and decoding that request. (IO operation)some time later…. gRPC have finished receiving the request. It puts the request body into the request_ field of the CallData object (via the pointer supplied earlier), then creates an event (with the pointer to the CallData object as tag, as asked earlier by the last argument to RequestSayHello). gRPC then puts that event into the completion queue cq_. The loop in HandleRpcs() received the event(the previously blocked call to cq-&gt;Next() returns now), calls CallData::Proceed() to process the request. status_ of the CallData is PROCESS, so it does the following: 6.1. Creates a new CallData object, so that new client requests after this one can be processed. 6.2. Generates the reply for the request, tells gRPC we have finished processing and please send the reply back to the client. 6.3 gRPC starts transmission of the reply. (IO operation) 6.4 The loop in HandleRpcs() goes into the next iteration and blocks on cq-&gt;Next() again, waiting for a new event to occur.some time later…. gRPC have finished transmission of the reply and tells us that by again putting an event into the completion queue with a pointer to CallData as the tag. cq-&gt;Next() receives the event and returns, CallData::Proceed() deallocates the CallData object (by using delete this;). HandleRpcs() loops and blocks on cq-&gt;Next() again, waiting for a new event.It might look like the process is largely the same as synchonous API, just with extra access to the completion queue. However, by doing it this way, at each and every some time later.... (usually is waiting for IO operation to complete or waiting for a request to occur), cq-&gt;Next() can actually receive operation completion events not only for this request, but for other requests as well.So if a new request come in while the first request is, let’s say, waiting for the transmission of reply data to finish, cq-&gt;Next() will get the event emitted by the new request, and starts the processing of the new request immediately and concurrently, instead of waiting for the first request to finish its transmission.Synchonous API, on the other hand, will always wait for the full completion of one request (from start receiving to finish replying) before even starting the receiving of another one. This meant near 0% CPU utilization while receiving request body data and sending back reply data (IO operations). Precious CPU time that could have been used to process other requests are wasted on just waiting.This is really bad since if a client with a bad internet connection (100ms round-trip) sent a request to the server, we will have to spend at least 200ms for every request from this client just on actively waiting for TCP transmission to finish. That would bring our server performance down to only ~5 requests per second.Whereas if we are using asynchonous API, we just don’t actively wait for anything. We tell gRPC: “please send this data to the client, but we will not wait for you to finish here. Instead, just put a little letter to the completion queue when you are done, and we’ll check it later.” and move on to process other requests.Related informationYou can see how a simple server is written for both synchronous APIs and asynchronous APIsBest performance practicesThe best performance practice suggested by the gRPC C++ Performance Nodes is to spawn the amount of threads equal to your CPU cores count, and use one CompletionQueue per thread." }, { "title": "[StackOverflow] C++ gRPC 异步 API 实例与优势", "url": "/posts/cn-so-difference-between-sync-and-async-grpc/", "categories": "Backend, Networking, gRPC", "tags": "C++, gRPC", "date": "2021-08-20 23:09:00 +0800", "snippet": " 转自 我的一个 StackOverflow 回答。因为答案比较长，而且感觉比较有意义，就翻译成了中文发了出来。 原问题: 我正在用 gRPC 构建一个要求高吞吐量的服务。但是我现在用 C++ 同步式 gRPC 编写的程序的吞吐量并不高。 我已经读过了 gRPC 文档，但是我并没有找到对于同步/异步 API 的区别的清晰解释。我只知道异步 API 可以控制完成队列（completion queue），而对于同步 API 来说是不可视的。 我的理解是同步 gRPC 会发送消息到 TCP 层，然后等待收到 “ack”，因此下个消息会被阻塞，而异步 API 会异步地发送消息，而不需要后面的消息等待前面的消息。TLDR: 是的，异步 API 发送消息不会造成后面消息等待，而同步 API 在发送/接收数据的时候，会把整个线程阻塞起来。gRPC 的异步操作使用 完成队列（CompletionQueue）。这个是它的官方教程: https://grpc.io/docs/languages/cpp/async/完成队列是一个事件队列（event queue）。这里的“事件”可以是数据接收的完成、或时钟 (alarm) 过期等。（简单来说，任何异步操作的完成都是完成队列中的一个事件）使用 gRPC 官方异步 API 示例作为例子，重点观察 CallData 类和 HandleRpcs()： void HandleRpcs() { // 创建一个新的 CallData 实例来服务新客户端 new CallData(&amp;service_, cq_.get()); void* tag; // 唯一地识别一个请求 bool ok; while (true) { // 阻塞读取完成队列中的下个事件。事件是通过 tag 唯一地识别的， // 在这里，使用 CallData 实例的内存地址作为 tag。 // 必须检查 Next 的返回值，这个返回值告诉我们是有事件到来 // 还是 cq_ 正在关闭。 GPR_ASSERT(cq_-&gt;Next(&amp;tag, &amp;ok)); GPR_ASSERT(ok); static_cast&lt;CallData*&gt;(tag)-&gt;Proceed(); } }HandleRpcs() 是服务器的主循环。它使用 cq-&gt;Next()，不断地从完成队列中获取下一个事件，并调用对应的 Proceed() 方法（也就是我们用于处理不同状态的请求的方法）。CallData 类的实例代表了一个完整的客户端请求周期： class CallData { public: // 输入 \"service\" 实例（在这里代表一个异步服务器）以及用于与 gRPC 运行时进行异步通信的完成队列 \"cq\" CallData(Greeter::AsyncService* service, ServerCompletionQueue* cq) : service_(service), cq_(cq), responder_(&amp;ctx_), status_(CREATE) { // 马上调用服务逻辑 Proceed(); } void Proceed() { if (status_ == CREATE) { // 将这个实例的状态推进到 PROCESS 状态 status_ = PROCESS; // 作为最初 CREATE 状态的一部分，我们*请求*系统开始处理 SayHello // 请求。在这个请求的过程中，\"this\" 被作为唯一识别请求的 tag // （这样一来不同的请求可以用不同的 CallData 实例异步地被处理） service_-&gt;RequestSayHello(&amp;ctx_, &amp;request_, &amp;responder_, cq_, cq_, this); } else if (status_ == PROCESS) { // 在我们处理这个请求之前，创建一个新的 CallData 实例用于处理未来 // 的新请求。实例会在它的 FINISH 状态流程中释放自己占用的内存 new CallData(service_, cq_); // 实际请求处理流程 std::string prefix(\"Hello \"); reply_.set_message(prefix + request_.name()); // 处理完成！让 gRPC 运行时知道我们已经完成了，使用这个实例的内存 // 地址作为事件内唯一识别请求的 tag status_ = FINISH; responder_.Finish(reply_, Status::OK, this); } else { GPR_ASSERT(status_ == FINISH); // 已经到达 FINISH 状态，释放自身占用内存（CallData） delete this; } } private: // 与 gRPC 运行时就一个异步服务进行通信的方法 Greeter::AsyncService* service_; // 用于接收异步服务器通知的生产-消费者队列 ServerCompletionQueue* cq_; // RPC 的上下文信息，用于例如压缩、鉴权以及发送元数据给客户等用途。 ServerContext ctx_; // 从客户端接收到了什么 HelloRequest request_; // 向客户端发送回什么 HelloReply reply_; // 用于回复客户端的方法 ServerAsyncResponseWriter&lt;HelloReply&gt; responder_; // 实现一个带有如下状态的小状态机. enum CallStatus { CREATE, PROCESS, FINISH }; CallStatus status_; // 目前请求状态 };如你所见，一个 CallData 有三个状态： CREATE，PROCESS 和 FINISH。一个完整的请求流程如下： 启动服务时，预分配 一个 CallData 实例供未来客户端使用。 该 CallData 对象构造时，service_-&gt;RequestSayHello(&amp;ctx_, &amp;request_, &amp;responder_, cq_, cq_, this) 将被调用，通知 gRPC 开始准备接收 恰好一个 SayHello 请求。这时候我们还不知道请求会由谁发出，何时到达，我们只是告诉 gRPC 说我们已经准备好接收了，让 gRPC 在真的接收到时通知我们。提供给 RequestSayHello 的参数告诉了 gRPC 将上下文信息、请求体以及回复器放在哪里、使用哪个完成队列来通知、以及通知的时候，用于鉴别请求的 tag（在这个例子中，this 被作为 tag 使用）。 HandleRpcs() 运行到 cq-&gt;Next() 并阻塞。等待下一个事件发生。一段时间后…. 客户端发送一个 SayHello 请求到服务器，gRPC 开始接收并解码该请求（IO 操作）一段时间后…. gRPC 接收请求完成了。它将请求体放入 CallData 对象的 request_ 成员中（通过我们之前提供的指针），然后创建一个事件（使用指向 CallData 对象的指针 作为 tag），并 将该事件放到完成队列 cq_ 中. HandleRpcs() 中的循环接收到了该事件（之前阻塞住的 cq-&gt;Next() 调用此时也返回），并调用 CallData::Proceed() 来处理请求。 CallData 的 status_ 属性此时是 PROCESS，它做了如下事情： 6.1. 创建一个新的 CallData 对象，这样在这个请求后的新请求才能被新对象处理。 6.2. 生成当前请求的回复，告诉 gRPC 我们处理完成了，将该回复发送回客户端 6.3. gRPC 开始回复的传输 （IO 操作） 6.4. HandleRpcs() 中的循环迭代一次，再次阻塞在 cq-&gt;Next()，等待新事件的发生。一段时间后…. gRPC 完成了回复的传输，再次通过在完成队列里放入一个以 CallData 指针为 tag 的事件的方式通知我们。 cq-&gt;Next() 接收到该事件并返回，CallData::Proceed() 将 CallData 对象释放（使用 delete this;）。HandleRpcs() 循环并重新阻塞在 cq-&gt;Next() 上，等待新事件的发生。整个过程看似和同步 API 很相似，只是多了对完成队列的控制。然而，通过这种方式，每一个 一段时间后.... （通常是在等待 IO 操作的完成或等待一个请求出现） cq-&gt;Next() 不仅可以接收到当前处理的请求的完成事件，还可以接收到其他请求的事件。所以假设第一个请求正在等待它的回复数据传输完成时，一个新的请求到达了，cq-&gt;Next() 可以获得新请求产生的事件，并开始并行处理新请求，而不用等待第一个请求的传输完成另一方面，同步 API总是会等待请求的完全完成（从开始接收到完成回复）后才会开始另一个请求的接收。这意味着接收请求体以及发送回复数据（IO 操作）的时候，会出现接近 0% 的 CPU 利用率。因为本可以用于请求处理的宝贵的 CPU 时间都浪费在白等上了。这是我们需要避免的，因为假设一个网络状况较差的客户端（100ms 的往返延迟）对服务器发送请求，那么对于这个客户端发送的每个请求，我们都将需要花费至少 200ms 用于等待 TCP 传输的完成。这会直接将服务器性能降低到约 5 个请求每秒。假设我们使用异步 API，我们根本就不主动等待任何东西。我们直接告诉 gRPC 一声：“将这个数据发给客户端，但是我不会站在这里等你完成。你搞定后往完成队列里塞一封信就行了，我后面自己去看。”，然后马上继续去处理其他请求。相关信息你可以看 同步 API 和 异步 API 的服务器各自是怎么编写。最佳性能实践由 gRPC C++ 性能小注 提供的性能最佳实践是创建与 CPU 核心数量一样多的线程，并为每一个线程使用一个完成队列（CompletionQueue）。" }, { "title": "在 PostgreSQL 中解码 Django Session", "url": "/posts/Decoding-Django-Sessions-in-PostgreSQL/", "categories": "Backend, Database, PostgreSQL", "tags": "Chinese, translated", "date": "2021-07-12 13:21:00 +0800", "snippet": " 原文地址：Decoding Django Sessions in PostgreSQL 原文作者：Daniel Lifflander 译文出自：掘金翻译计划 本文永久链接：https://github.com/xitu/gold-miner/blob/master/article/2021/Decoding-Django-Sessions-in-PostgreSQL.md 译者：Miigon 校对者：PassionPenguin、kamly 解决将用户的会话数据与其实际的用户对象联系起来的问题时，Postgres 显得十分好用。Django 中的会话会话（session）是任何基于 HTTP 的 web 框架的重要组成部分。它使得 web 服务器可以记录重复请求的 HTTP 客户端而不需要对每一次请求重新进行认证。记录会话的方式有多种。其中的一些方法不需要你服务器保持会话数据（如 JSON Web Tokens），而另外一些则需要。Django，一个基于 Python 的热门 web 框架，自带了一个会存储会话数据的默认会话后端。存储和缓存的方案也有多种：你可以选择直接将会话存储在 SQL 数据库中，并且每次访问都查询一下、可以将他们存储在例如 Redis 或 Memcached 这样的缓存中、或者两者结合，在数据库之前设置缓存引擎。如果你使用这些最终将会话存储在 SQL 中的方案，则 django_session 表将存储你的用户会话数据。本文中的截图来自 Arctype。会话结构细读你应用程序的数据，你可能会遇到需要将你的用户的会话数据联系到实际的用户表项（auth_user 表）。我最近遇到过这一情景，当我查看会话表的结构时， user_id 没有被作为一列数据存储在其中使我感到非常吃惊。这背后是有重要的设计选择的，但是对于像我这样的 SQL’er 来说就不太方便了。session_key 是提供给客户端的 key。一般而言，发起请求的客户端会以 cookie 的形式将 session_key 附带其中。当 web 服务器收到请求时，若存在 session_key，将发起查询来检验 key 是否已知。若是，服务器将检索与其关联的 session_data 并获取有关用户及会话的原数据。这就是你可以在一个 Django 请求中访问 request.user 的原因。user_id 从解码到的 session_data 中获取，内建的 User 对象将根据存储的 user_id 被填充，在这之后在项目的视角中 User 对象就持续可用了。谷歌了一下告诉我默认的会话数据是以 JSON 的形式存储的。我此前已经知悉了 Postgre 出色的 JSON 能力（如果你还不知道，看一看这篇文章），因此我认为我们可以在 Postgre 的范畴内实现这一功能。这对于像我一样在 Postgres 上花了大量时间的人来说是个大好消息。构建请求初瞥一眼你可能在第一张图片中观察到，session_data 看起来不像是 JSON。以 JSON 存储的原数据被隐藏在了 base64 之后。幸运的是，我们可以在 Postgres 中很方便地解码 base64。从 Base64 解码已经没办法比这更可读了。我们需要将二进制数据转换成文本。编码为文本Postgres 提供的 “encode” 函数可以用来“将二进制数据编码为文本形式的表示”。现在，我们终于可以看到可以看懂的数据了。这是一个文本格式的完整的记录：11fcbb0d460fd406e83b60ae082991818a1321a4:{\"_auth_user_hash\":\"39308b9542b9305fc038d28a51088905e14246a1\",\"_auth_user_backend\":\"x.alternate_auth.Backend\",\"_auth_user_id\":\"52135\"}提取 JSON我们这里得到的是一个带有某种哈希加上一个冒号作为前缀的 JSON blob。我们只对 JSON blob 感兴趣。一个快捷的提取方法是找到第一个冒号的位置，并提取其后的所有字符。为了实现这一功能，我们可以同时使用 RIGHT 函数以及 POSITION 函数，前者返回一个 string 末尾的 n 个字符，后者返回字符串内某个字符的位置。POSITION 只会返回你的搜索目标第一次出现的位置。RIGHT 函数可接收一个负索引。负的索引指从字符串右侧提取字符直到不包括负索引指向的那个字符。继续构建我们的请求，我们使用 CTE 将其分成两部分。CTE 在你已经构造并选择了一列数据并且需要多次使用它时有帮助。如果我们仅用一个 SELECT，我们将要多次输入 encode(decode(session_data, 'base64'), 'escape')。这很快会变得混乱，并且如果你决定想要更改你解析编码数据的方式时，你将需要同时修改 2 处函数调用。这是我们能够提取 JSON 部分的新请求。完整结果示例：{\"_auth_user_hash\":\"396db3c0f4ba3d35b350a\",\"_auth_user_backend\":\"x.alternate_auth.Backend\",\"_auth_user_id\":\"52646\"}JSON 校验现在列数据可以作为 JSON 解析了。然而，在 Postgres 中如果你尝试解析一个非法 JSON 文本，Postgres 会抛出一个错误并终止你的查询。在我自己的数据库中，有一些会话数据不能被作为 JSON 解析。下面是一个确保文本看起来像可解析 JSON 的便捷方法。where substring(decoded, position(':' in decoded) + 1, 1) = '{' and right(decoded, 1) = '}'`任何不以花括号开头及结尾的字符串都将被过滤掉。这不能完全保证它可以被解析，但是对于我有几百万会话的数据库而言，它能够解决问题。你可以写一个自定义的 Postgres 函数来验证 JSON 有效性，但那样查询速度会变慢。JSON 转换使用一个 WHERE语句来排除无效的会话元数据后，是时候将我们的字符串转换成 Postgres 的 JSON 类型并从中提取 _auth_user_id key 了。取决于你的 Django 配置，这个 key 可能不同。一旦一个对象被转换为 JSON 类型，你就可以使用 object-&gt;key 语法来请求一个 JSON 值。字符串清理胜利就在眼前！当从 JSON 转换到 text 的时候，Postgres 会在其两端添加双引号。最终我们想要 user_id 的类型为 int，但 Postgres 不会将一个带有双引号的字符串转换为 int。就算是 JavaScript 也不允许这么干！带有 BOTH 的 TRIM 函数会将指定的字符从字符串的两端去除，留下可以轻松转换为整数类型的干净的字符串。最终的请求这是加上去除多余的双引号并转换为 int 的请求：现在，如样例结果所示，我们成功将 session_key 和 Django 的 auth_user id 连接起来了。这是可复制格式的完整查询语句：with step1 as ( select session_key, encode(decode(session_data, 'base64'), 'escape') :: text as decoded from django_session)select session_key, trim( both '\"' from ( right( decoded, 0 - position(':' in decoded) ) :: json -&gt; '_auth_user_id' ) :: text ) :: int as user_idfrom step1where substring(decoded, position(':' in decoded) + 1, 1) = '{' and right(decoded, 1) = '}'使用实例化视图来加快查询如果你的数据库有大量的用户，你会发现这个查询十分缓慢。创建实例化视图 (materialized view) 使得你可以从一个一致的视图中重复地请求数据，而不用重新执行 SQL 语句。当你创建实例化视图时（以及当你刷新它时），视图对应的源代码将会被执行以生成结果用于填充视图。确保你在需要最新的数据的时候刷新一下视图！create materialized view mv_django_session_user aswith step1 as (…// To refresh:refresh materialized view mv_django_session_user;总结Postgres 中的编码以及字符串操作比常见的用于 web 应用的语言（如 Python、Ruby 或 PHP）来说更加繁琐些，但是用纯 Postgres 构建出一个可以快速提取你要的数据并让你可以和其他表直接连表查询的视图，不得不说是十分愉悦的。下一次你需要从 web 框架或其他第三方提取数据时，不妨从 Postgres 寻找答案！ 如果发现译文存在错误或其他需要改进的地方，欢迎到 掘金翻译计划 对译文进行修改并 PR，也可获得相应奖励积分。文章开头的 本文永久链接 即为本文在 GitHub 上的 MarkDown 链接。 掘金翻译计划 是一个翻译优质互联网技术文章的社区，文章来源为 掘金 上的英文分享文章。内容覆盖 Android、iOS、前端、后端、区块链、产品、设计、人工智能等领域，想要查看更多优质译文请持续关注 掘金翻译计划、官方微博、知乎专栏。" }, { "title": "Node.js 与 Python：为你的项目选择哪一个？", "url": "/posts/node-js-vs-python-which-one-to-choose-for-your-project/", "categories": "Backend", "tags": "nodejs, python", "date": "2021-05-22 01:08:00 +0800", "snippet": " 原文地址：Node.js vs. Python: Which One to Choose for Your Project? 原文作者：Saul Gallegos 译文出自：掘金翻译计划 本文永久链接：https://github.com/xitu/gold-miner/blob/master/article/2021/node-js-vs-python-which-one-to-choose-for-your-project.md 译者：kamly 校对者：Kimhooo, PassionPenguin, Miigon Node.js 与 Python：为你的项目选择哪一个？Node.js 和 Python 是网络上最常被拿来做比较的两门后端技术。由于两者在 Web 应用程序开发中都非常流行，在作出二选一之前，有必要对两者进行透彻的对比。在本文中，我们将探讨这两门技术中哪一个更能适合您的开发需求，以及在哪些情景下适用。什么是 Node.js?Node.js 属于事件驱动型，它的可拓展性为项目带来出色的开发效率。它的异步式设计使其可处理并发请求而不阻塞 I/O 操作。多数开发团队为了使用 JavaScript 同时开发客户端和服务端，会偏向选择 Node.js。什么是 Python?另一方面，Python 是一门成熟的高级面向对象语言。它接近 30 年的历史也意味着它有大量的库、API 和辅助工具可用。Python 对多种编程范式友好，被广泛应用于商业应用程序。它的社区、库和支持平台十分广泛。因此，对于大多数需要为不同且通常不相关的场景开发应用程序的企业来说，它是一种理想的编程语言。Node.js 和 Python 的流行度一种后端技术的受欢迎程度对它是否会被用于你的项目起着关键作用。首先从公司的角度来看，开发团队必须确定哪种语言在市场上有相当程度的人才储备。从程序员的角度来看，他们必须思考是否愿意学习那些许多公司大量使用的语言。按网站热度来看，SimilarWeb 报告称，Node.js 享有「热度前 10,000、前 100,000 、前 1,000,000 的网站以及『所有网站』类目中使用最多的工具」之名。根据 Stack Overflow 2019 年的调查，Python 语言是最受「喜爱」的编程语言，而 Node.js 则是专业程序员最偏向使用的工具。根据行业采用情况来看，Node.js 在计算机电子与技术、生活、艺术与娱乐以及其他 16 个类别中均处于领先地位。而 Python 则是科学与教育、工程和其他一些行业的热门选择。Node.js 和 Python 性能比较当你的应用程序规模扩大时，每一点执行效率提升都有助于减少运营成本。因此，在评估一种编程语言或环境时，性能和速度是很重要的。Node.js我们已经探讨了 Node.js 的非阻塞架构。有了这个优势，它的执行过程更加简单和快速。在此基础上，还有一个优势使得 Node.js 成为一个坚实之选，那就是它可以在网络浏览器之外执行代码。这样，在浏览器上运行的应用程序处理速度更快，并因此在效率指标上表现得更好。该特性还允许应用程序使用原本在浏览器中无法使用的功能，如 TCP 套接字。PythonPython 的架构天生不支持多线程，因此在这方面也是一个问题。因为任务处理不是并行运行的，处理速度受到了降低。虽然 Python 的语法易于学习和运行，但对于需要频繁从 Web 服务器调用数据的应用程序来说，它的速度还是不够快。Python 的语法很易于学习，但它没有针对速度和性能进行优化。在这方面，Node.js 也是明显的赢家。Node.js 和 Python 的社区社区是投身技术的用户和开发者群体。由于他们的活跃性，他们会不断更新流行的库，进行调试练习，甚至增加新的功能。一般来说，最有效的社区是由是由单一、专门的组织来管理对应的语言或编程环境的社区。Node 的社区你可能会认为由于 Node.js 是一种相对较新的技术形式，从而误以为其社区规模很小。然而实际上，Node.js 社区规模之大可能会令你震惊，在全球范围内活跃着众多经验丰富的 Node.js 开发者。在 Node.js 圈子里要发现人才超级容易。Python 的社区Python 作为这两种语言中的老大哥，自然拥有更大的社区。从初级到高级的贡献者，Python 并不缺乏人才。如此壮大的社区的最大优势之一就是容易找到开发者。正是因为这些开发人员的积极参与，为 Python 带来了快速的解决方案和语言总体的改进。什么时候应该使用 Node.jsNode.js 是一款适合运行需要在服务端和浏览器端之间保持可靠连接的应用程序的运行时环境。考虑应用程序中是否需要 Node.js 的一种简单方法是对应用程序的实时特性进行评估。如果实时数据传输对应用程序的可用性至关重要，那么 Node.js 正是您所需要的。同样值得注意的是，Node.js 在服务端和客户端都可使用。这将减少部署时间，并使团队在协作中维护代码变得可行。由于其高效的处理能力，Node.js 是云服务器和客户端应用之间频繁交互的公司使用的首选平台。Netflix、LinkedIn、Medium、Trello 和 Paypal 是其中几个使用 Node.js 作为平台的知名技术公司。什么时候该使用 PythonPython 拥有充足的人才供应，部分原因是由于其不复杂的语法。因此，谷歌和 Facebook 这两个最大的技术人才消费者大量使用 Python 来构建技术也就不足为奇了。此外，一些最受欢迎的数据科学培训课程使用 Python 作为教给学生的默认语言。因此，Python 人才的供给仍在不断上升。进一步讲，Python 的流行使学习和使用它具有了重要的战略意义。它丰富的库允许你非常高效地建立和部署一个 MVP 设计模式。这也确保了开发人员的资源得到最佳利用。还有呢! 负责执行代码的 Python 解释器是超级有效的。它不需要编译，这使得 Python 成为快速部署和迭代的理想语言。总结在 Node.js 和 Python 之间进行选择有时很棘手，特别是当你想开发一个完整的产品，而不仅仅是一个附带项目时。双方的功能和优势都可能是压倒性的，可能会模糊你的判断。摆这种困境的最佳方法，是关注产品的最终用途。列出你产品的所有特性，以及你计划如何设计它们。最终对你的设计更有帮助的语言或运行时环境就是你的不二之选。如果你发现自己仍然感到困惑，或者不知道自己是否做出了正确的决定，请随时在文章下方评论区将你的问题贴上来，我们还有广大掘金社区开发者们都很乐于帮助你的呀～ 如果发现译文存在错误或其他需要改进的地方，欢迎到 掘金翻译计划 对译文进行修改并 PR，也可获得相应奖励积分。文章开头的 本文永久链接 即为本文在 GitHub 上的 MarkDown 链接。 掘金翻译计划 是一个翻译优质互联网技术文章的社区，文章来源为 掘金 上的英文分享文章。内容覆盖 Android、iOS、前端、后端、区块链、产品、设计、人工智能等领域，想要查看更多优质译文请持续关注 掘金翻译计划、官方微博、知乎专栏。" }, { "title": "Natural Sort: How to sort file names naturally", "url": "/posts/how-to-sort-file-names-naturally/", "categories": "Algorithm, Sorting", "tags": "javascript", "date": "2021-04-13 13:33:00 +0800", "snippet": "What’s the problem?When a programmer is given the task of sorting file names in a list, it might be tempting to sort using something like std::sort(). The problem with that is: std::sort() sorts alphabetically. Suppose we have a list of file like this:chapter1.txtchapter10.txtchapter2.txtchapter3p1.txtchapter3p2.txtchapter3p10.txtchapter11.txtchapter20.txtIf we sort it alphabetically, aka, using std::sort() (or Array.prototype.sort() for JavaScript), what would happen?chapter1.txtchapter10.txtchapter11.txtchapter2.txt # we expect chapter 2 to be in front of chapter10chapter20.txtchapter3p1.txtchapter3p10.txtchapter3p2.txtWe see that chapter10 and chapter11 have been placed before chapter2. This make sense for an alphabetically sorted list.Both 10 and 11 is smaller than 2 since the first digit 1 is smaller than 2. However, it does not follow our intuition.As a human, we expect 2 to be bigger than 10, not the other way around.Natural sortThis is where the algorithm of natural sort (sometimes called alphanumerical sort) comes in handy. Natural sort order is an ordering of strings in alphabetical order, except that multi-digit numbers are treated atomically, i.e., as if they were a single character. Natural sort order has been promoted as being more human-friendly (“natural”) than the machine-oriented pure alphabetical order.Pseudo-code for natural sort:# PreprocessingFor every file name in list: processedName = [] Scan for every character c in file name: If consecutive digits are found: Merge consecutive numerical characters into a single number processedName.Append(number) Else: # non-numerical character processedName.Append(c) list[current] = processedName # use the new name as name for comparisonSort list: Comparing file name x and y: For every cooresponding [xi in x] and [yi in y]: If xi == yi: Continue If both xi and yi are numbers: If xi &lt; yi: x is smaller Else: y is smaller Else If both xi and yi are strings: If xi &lt; yi: x is smaller Else: y is smaller Else: # One of xi or yi is a number and the other is string If xi is number: x is smaller Else: y is smaller# the list is now sorted naturally (or alphanumerically)!After applying the algorithm, the list should look like this:chapter1.txtchapter2.txtchapter3p1.txtchapter3p2.txtchapter3p10.txtchapter10.txtchapter11.txtchapter20.txtThe number rigit after chapter is sorted correctly as well as the second number after chapter3p.External resourcesThis blog only described a simplified version of natural sort. Some file manager might ignore leading and trailing spaces when sorting. Multiple consecutive spaces might be considered as one single space as well. However the overall algorithm did not change and all of these features are quite trivial to implement.Checkout these resources:https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/https://rosettacode.org/wiki/Natural_sortingImplementation in JavaScript// natural sort algorithm in JavaScript by Miigon.// 2021-03-30// // GitHub: https://github.com/miigon/function natSort(arr){ return arr.map(v=&gt;{ // split string into number/ascii substrings let processedName = [] let str = v for(let i=0;i&lt;str.length;i++) { let isNum = Number.isInteger(Number(str[i])); let j; for(j=i+1;j&lt;str.length;j++) { if(Number.isInteger(Number(str[j]))!=isNum) { break; } } processedName.push(isNum ? Number(str.slice(i,j)) : str.slice(i,j)); i=j-1; } // console.log(processedName); return processedName; }).sort((a,b) =&gt; { let len = Math.min(a.length,b.length); for(let i=0;i&lt;len;i++) { if(a[i]!=b[i]) { let isNumA = Number.isInteger(a[i]); let isNumB = Number.isInteger(b[i]); if(isNumA &amp;&amp; isNumB) { return a[i]-b[i]; } else if(isNumA) { return -1; } else if(isNumB) { return 1; } else { return a[i]&lt;b[i] ? -1 : 1 ; } } } // in case of one string being a prefix of the other return a.length - b.length; }).map(v =&gt; v.join(''));}let a = ['a2','a1b10z','b1a2','a1b10','a33','7','a3','a22','a1b2','abbbb','a1b1','aaaaa','a10','a1','10'];console.log(natSort(a).join('\\n'))/* 7 10 a1 a1b1 a1b2 a1b10 a1b10z a2 a3 a10 a22 a33 aaaaa abbbb b1a2*/" }, { "title": "关于 IEEE 754 浮点数一些设计细节的疑问解释", "url": "/posts/answering-some-questions-about-ieee-754/", "categories": "Misc, Discussion", "tags": "computer system, Chinese", "date": "2021-03-12 10:30:00 +0800", "snippet": "计算机系统课程上讲到的 IEEE 754 32位浮点数一些规则细节的个人理解与解释。老师在课上已经把各个细节都大致讲过了，这篇文章是给课后对这些细节还感兴趣的同学，做补充解释和扩展。 这篇文章不会采用晦涩的引用或者证明，而是尝试让同学能直观理解 IEEE 754 的一些设计选择。 2021-03-31: 最近有比较多的同学看到了这篇文章，这篇文章的本意是回答几个课上遇到的具体问题，而对 IEEE 754 本身的介绍方面的完整性和系统性可能不及其他资料，建议配合其他资料阅读。重温课上的例子课上老师讲过，现代的计算机是以 IEEE 754 的标准来存储浮点数的，以 32 位浮点数 6.625 为例子：-6.625 = -(4 + 2 + 1/2 + 1/8)，一位一位对应过来，二进制表示就是 -110.101，那么使用浮点数表示 6.625 的话，内存中实际存储的比特位是这个样子的：其实可以观察到，浮点数的存储，本质上就是二进制的科学记数法：由一个有效数字（绿色部分），乘上一个数量级（蓝色部分）来表示一个小数。为什么有效数字的整数部分要规定为 1 ？根据公式可以观察到，尾数前面的整数部分，IEEE 754 规定（当 1 ≤ e ≤ 254 的时候）固定是 1。有同学问，为什么这里非得是 1 呢？假设整数部分是 0 可不可以呢？其实 0 也是可以的，但是这样其实就浪费了一个位的精度了。我们知道浮点数在内存中的表示，其实就是二进制的科学记数法。我们先考虑我们所熟悉的十进制，十进制下科学记数法为了达到最高效地表示数字的目的，是规定不允许有效数字的整数部分是 0 的，如果整数部分是 0 的话，就通过改变数量级指数来调整，使得整数部分变成 1 到 9 之间的整数。0.365 * 10^5 =&gt; 3.65 * 10^4二进制的科学记数法也是一样的，我们为了高效简介的表达，也像十进制的科学记数法一样，规定有效数字的整数部分不能是 0（因为前导 0 是无效数字，并不能提供任何信息）。 也就是说，例如 111010它的二进制科学记数法是 1.11010 * 2^5而不是 0.111010 * 2^6，因为这种表示不是最高效简介的表示方法但是专家们很快发现：既然都规定了科学记数法有效数字的整数部分不能是 0 ，因为二进制只有 0 和 1 两种数字，那整数部分不就只剩下 1 这一种可能性了吗？于是通过规定整数部分不为 0 ，加上二进制本身的性质，我们得到一个结论：二进制数的科学记数法中，有效数字的整数部分永远是 1。知道这个结论以后，我们会发现，我们其实就没必要花内存去存有效数字（例如 1.11010）开头的这个 1 了，因为我们已经知道个位肯定是 1 了，只存有效数字的小数部分（.11010）就行了。 例如 1.11010 * 2^5，已知二进制科学记数法有效数字必然是 1. 开头的所以只需要花内存去存小数点后面的尾数 11010 就足够了这就是为什么在二进制浮点数中 仅用 23 个 bit 就能表示 24 位的精度，这多出来的 1 个 “免费的精度” 是二进制的特性所共同提供的。 （ps. 如果是 10 进制的话，因为有效数字的整数部分有 1 ~ 9 九种情况，就不能像二进制这样省略掉不存第一位）我们也可以做一个小实验，我们还是以 -110.101 作为例子，看一下如果假设有效数字个位规定为 0，会有什么效果：规定 1 为整数部分（IEEE 754 的做法，与开头的例子相同）规定 0 为整数部分（我们自己假想的做法，注意对比 N 公式的变化）（注意对比两种方案的尾数）我们会发现，我们按照 IEEE 754 专家们的规矩去存储的话，需要存储的尾数部分是 10101，但是如果按照我们假设个位是 0 去存储的话，我们的尾数就变成 110101了。这也印证了我们前面提到的，有效数字整数部分如果为 0 的话，这种表示不是最高效的。也就是对于同样的数据，假设有效数字整数部分规定是 0 的话，我们的尾数要多浪费一个位，去存储这一个我们明明知道的 1。为什么指数 e 要用移码表示？而不是带符号位的原码或补码？同学提到，为什么指数要用移码表示？也就是为什么要把它加上 127 去存储？不是无论哪一种存储方式，表示的范围不都是一样的吗？答案是为了简化浮点数的运算和大小比较。对于浮点数，我们进行大小比较的时候，其实就是比较两个科学记数法表示的数字，所以第一步肯定是先比较他们的数量级。如果数量级都不一样大，那就没有必要去比较有效数字部分的具体大小了，因为数量级大的表示的数字肯定更大。浮点数中的数量级大小由指数 e 决定，思考一下，假设现在指数不用移码，而是采用带符号位的原码表示的话，需要考虑什么？首先我们要检查符号位，要看符号是不是一样的，如果不一样的话，正数要比负数大。而符号位同正呢？同负呢？同正的话是不是就是绝对值大的数比较大？同负的话是不是绝对值小的数比较大？那就得实现两套比较逻辑，对应两种不同的情况。首先要把符号关系搞清楚（++,+-,-+,--），然后，再按符号关系执行多套不同的逻辑，这样实现起来 CPU 电路会很复杂。所以当时设计 IEEE 754 的专家为了保持简洁，就干脆不要符号位了，直接规定我们把指数加上 127 再存储。加上 127 就把指数的取值范围 “移” 到正数上来了。这样比较指数大小（数量级大小）的时候就不用考虑异号或者同号了，因为都是正数，采用同一套比较电路直接比较就行了，而正数之间的比较电路是很简单的，所以也就简化了 CPU 的设计。 为什么是 +127 而不是 +128？8bit 有符号整数的范围是 -128 ~ 127，但是将整个指数的范围移动到整数上，只需要加 127 就可以了，不用加 128因为在 IEEE 754 中，指数 = -128 被规定保留为表示特殊情况了（具体用途是用来表示很小的数字，称为 Subnormal number）实际上 指数 = 128 也被规定保留为表示特殊情况了（NaN / Infinity）我们能够取到的指数范围只剩下 -127 ~ 127所以不需要偏移 128，只需要偏移 127 就足够了" }, { "title": "问题分析：ios_base::fixed 导致输出精度丢失？", "url": "/posts/cpp-ios-base-fixed-precision-loss/", "categories": "Misc, Discussion", "tags": "C++, Chinese", "date": "2021-02-24 20:08:00 +0800", "snippet": " 这篇文章是来自我在 0xffff.one 上的一个帖子 https://0xffff.one/d/911/ 的回复。 原帖内容：百度说是这行代码的作用是使用定点输出，同时输出小数点后6位(我试了好多数，仍然表示很迷) 为什么有这行代码有时候求两个数加减乘除的结果就不对，没有这行代码就对呢 比如55.25+11.17有上面那行代码结果是66.419998，没有就是66.42；而20.5+10.5有无上面那行代码结果都正确. 这是为什么😳 呢？How to research? How to approach?我们注意到，导致输出不同的，是这样一行代码：cout.setf(ios_base::fixed,ios_base::floatfield);控制变量，这行代码干的事情就是我们的切入点。可以看到，我们使用了 setf，对 floatfield 设置了一个 fixed 的 flag，那么这些就是我们搜索的关键词。搜索 setf，我们得到：http://www.cplusplus.com/reference/ios/ios_base/setf/ …The format flags of a stream affect the way data is interpreted in certain input functions and how it is written by certain output functions. See ios_base::fmtflags for the possible values of this function’s arguments.…我们知道了 format flags 是可以改变数据被显示的方式的。继续搜索 fixed：http://www.cplusplus.com/reference/ios/fixed/ When floatfield is set to fixed, floating-point values are written using fixed-point notation: the value is represented with exactly as many digits in the decimal part as specified by the precision field (precision) and with no exponent part.precision！！！我们的问题看起来就是一个精度相关的问题！看一下参考中关于 ios_base::precision 的部分：http://www.cplusplus.com/reference/ios/ios_base/precision/ For the default locale:Using the default floating-point notation, the precision field specifies the maximum number of meaningful digits to display in total counting both those before and those after the decimal point. Notice that it is …… In both the fixed and scientific notations, the precision field specifies exactly how many digits to display after the decimal point, even if this includes trailing decimal zeros. The digits before the decimal point are not relevant for the precision in this case.啊，破案了。在默认的浮点输出模式下，precision 代表的是精确到第几位有效数字，而在 fixed （或scientific）的输出模式下，precision 代表的是精确到小数点后第几位。Solution知道了这个事实，就可以很容易猜到这是同一个浮点数，输出时的 rounding 不同造成的区别，而不是由于精度丢失造成的区别。（精度丢失依然存在，只是在这里不是问题的直接原因）int main() { using namespace std; float result = 55.25f + 11.17f; cout &lt;&lt; result &lt;&lt; endl; // 66.42 cout.setf(ios_base::fixed,ios_base::floatfield); cout &lt;&lt; result &lt;&lt; endl; // 66.419998 return 0;}我们知道 C++ 默认的浮点输出精度是 6，但是这个 6 在不同的输出模式下有不同的含义。在默认的浮点输出模式下，6 代表的是 精确到6 位有效数字，而在 fixed （或scientific）的输出模式下，6 代表的是 精确到小数点后第 6 位。猜到了吗？没错，其实对于计算机来说，由于精度不足，我们的数字是（且一直都是） 66.4199981689……，只是在默认的输出模式下，由于整数部分已经消耗掉了 6 位有效数字精度中的 2 位，只剩下 4 位有效数字给小数，因而小数点后只能精确到第四位。精确到小数点后第四位，就要看这个数字的第五位，在这个数字 66.4199__9__81689 中，第五位是个9，所以四舍五入就是 __66.4199__9 ≈ 66.42 了。而采用固定小数点后位数的输出方式，精度的含义是，精确到小数点后 6 位。66.419998__1__689…… 的第七位是1，四舍五入舍去，留下 __66.419998__1 ≈ 66.419998 。说到底，是因为不同的输出方式下，对 “精度”（ios_base::precision） 的理解不一样。我们也可以很方便地验证我们的结论，只需对普通的方法设置一个 8 的精度即可：int main() { using namespace std; float result = (55.25f+11.17f); cout.precision(6); // 6 位有效数字 cout &lt;&lt; result &lt;&lt; endl; // 66.42 cout.precision(8); // 8 位有效数字 cout &lt;&lt; result &lt;&lt; endl; // 66.419998 cout.setf(ios_base::fixed,ios_base::floatfield); cout.precision(6); // 小数点后 6 位 cout &lt;&lt; result &lt;&lt; endl; // 66.419998 return 0;}至于 double？在 double 下，55.25 + 11.17 = 66.4200000000000017053025658242404460906982421875……直到小数点后第15位才出现进度丢失，所以在两种显示方案下，无论小数点后是有 4 位精度还是 6 位精度，都会被四舍五入到 66.42。所以这个问题总结起来是：在 float 存储时精度丢失的前提下，不同输出方案导致了输出时小数点后精确位数不同，进而导致 rounding 不同。能够学到的： 能 double 就尽量不要 single float 如果应用场景不需要显示那么多位小数，就把 precision 设小点" }, { "title": "原理分析：使用 dd 跳过开头若干字节快速拷贝文件", "url": "/posts/cn-skip-bytes-while-copying-with-dd/", "categories": "Misc, Discussion", "tags": "tools, Chinese", "date": "2021-02-18 13:20:00 +0800", "snippet": " 这篇文章是来自我在 0xffff.one 上的一个帖子 https://0xffff.one/d/900 的回复。 原帖内容：在折腾一个超大的备份文件，需要把它的前 41 个字节删除掉，没有 WinHEX，想着用 dd 命令来实现 一开始这么干，发现速度奇慢，5分钟过去才复制40MB… dd if=input.bak bs=1 skip=41 &gt; result.bak google 一波发现一老哥的操作，配合 dd 和 cat 实现快速拷贝的功能，有些佩服。 { dd bs=41 skip=1 count=0; cat; } &lt; input.bak &gt; result.bak 原帖：https://unix.stackexchange.com/questions/6852/best-way-to-remove-bytes-from-the-start-of-a-file干了啥看一下一开始的指令：dd if=input.bak bs=1 skip=41 &gt; result.bak为什么这个指令会慢呢？首先一点背景知识：　　计算机中每一次向硬盘读取和写入数据，无论读多小的数据量，都至少需要花一段常数时间（称为overhead）。（就像你去超市买鸡蛋一样，无论你一次只买一个，还是一千个，你都至少要花从家走到超市，再从超市走回家的时间。）　　就好比你现在要100个鸡蛋，但是你去超市一趟只买一颗鸡蛋的话，你就要来回跑100次一样。用 dd 拷文件也是同样的道理，如果一次只跑去给硬盘要一个字节，一个文件就要来回跑特别多次，花费的时间就会特别长。　　为了解决这个问题，dd 在读文件的时候，会将文件切分成大小固定的一小块一小块 (block)，每次向硬盘要数据就一次性要一个“块”的大小（默认 512 个字节），也就是说，每次费那么大功夫跑过去，那就干脆多要一点数据，同样大小的文件不就可以少跑很多次了吗？所以为啥慢？　　前面提到的一次拿一个分块，这个分块的具体大小，是可以通过bs=参数进行人工调节的（bs = block size, 块大小）。　　我们一开始的指令的问题就在于，我们这个指令里有一个参数bs=1，也就是告诉 dd，我想要每 1byte 就当作一个 block，这样 dd 实际上就又变回了跑一次读一个字节的傻 dd 了，所以这个指令最后就慢得出奇啦。（0.26MB/s)大不就完事了？路人甲：那么既然是因为块大小 bs 设太小了，那我们改大不就行了吗？没错，理论上是这样的。划重点：理论上如果我们只是想要单纯的把文件a.txt拷贝一份到文件b.txt，那我们的确可以直接把 bs 改大就行了：# 块大小：512Bytes，速度93MB/sdd if=a.txt of=b.txt bs=512# 块大小：4MB，速度1138MB/sdd if=a.txt of=b.txt bs=4m仅仅把每次读的数据块大小改大，就得到 12 倍的速度提升！然鹅，为什么我们这个例子里不行呢？dd if=input.bak bs=1 skip=41 &gt; result.bak # 为啥不把 bs 直接改大？？？因为我们除了拷贝文件外，还有另外一个要求：跳过前41个字节！那么我们用什么方法实现跳过41个字节呢？我们一开始的指令使用了 skip=41 来实现这一要求。好，刹住，大问题来了：skip 的单位是 block！也就是说skip=41不是指跳过41个字节，是指跳过41个 block 😅 （只是我们用bs=1让每个块都刚好是 1 字节而已）也就是说，我们一开始的指令里的 bs=1 skip=41 其实是在讲，我们想要跳过 41个块 x 每个块 1 个字节 = 共41个字节但是因为 skip 只能跳整数个 block，这就意味着，我们如果想把每个 block 大小改大，最多也就是bs=41 skip=1，跳过 1个块 x 每个块 41 个字节 = 共41个字节。因为再大的话，skip 就凑不成整数个块了。那么矛盾就来了： 我们要把块大小 bs 改大，才能提高拷贝速度 但是为了实现跳过前 41 个字节，块大小最大也只能是 41 Bytes路人甲：啊这……曲线救国这时候我们进退两难，就需要用曲线救国思路，借助我们万能的朋友——管道 来解决这个问题了。看我们的解决方案：{ dd bs=41 skip=1 count=0; dd bs=4m; } &lt; input.bak &gt; result.bak看起来很复杂， 其实分开来看很简单，这里用到了一个 shell 的特性，花括号 { }。在花括号里的指令，会视为一个整体，里面每一条指令都会从输入管道读入数据后执行，但与每条指令分开执行不同的是，在花括号里的指令，需要排队分蛋糕一样读输入，也就是说 花括号内的指令按顺序依次从输入管道读取同一个输入流，每个指令读走了多少，下一个指令能读的就少掉多少。我们最后的方案中，花括号里有两条通过分号隔开的指令，dd bs=41 skip=1 count=0 还有 dd bs=4m。 第一个指令是我们的老朋友—— dd，这里是让它从输入流直接跳掉1个41字节大小的块，然后count=0表示读 0 个 block，也就是跳完啥都不用读了，直接退出，把剩下的输入交给下个指令去处理。 第二个指令还是我们的 dd，但是因为第一个 dd 已经负责跳过了前41个字节了，第二个 dd 不需要考虑跳过字节，也就不需要怕 bs 设置太大啦！于是第二个 dd 放飞自我，可以直接用 4MB 的块大小（bs=4）去拷贝，一次搬 4MB，那叫一个快啊！ 第一个小 block size 的 dd 实现跳过，然后用第二个大 block size 的 dd 来快速搬数据，双d齐心，其利断金！路人甲：妙哉!最后我们得到了这个d上加d的无敌大指令：{ dd bs=41 skip=1 count=0; dd bs=4m; }，我们只需要把文件输入到这个大指令里：&lt; input.bak，再告诉它输出到哪里：&gt; result.bak，回车一摁，哗啦一下，拷完啦！（1107MB/s，比最初的方法快4000+倍）cat? 喵喵喵？# 我们的指令{ dd bs=41 skip=1 count=0; dd bs=4m; } &lt; input.bak &gt; result.bak# StackExchange 答案里的指令{ dd bs=41 skip=1 count=0; cat; } &lt; input.bak &gt; result.bakStackExchange 上原文用到的第二个负责搬砖的指令是 cat ，实际上和我们的dd bs=4m是差不多的，都是从输入流读数据，然后原封不动送到输出流，并且都是用较大的分块读取。不同点在于 cat 会自动帮你选择合适的块大小，所以用 cat 的话我们什么参数都不用写，而用 dd 我们需要手动告诉它我们要 4MB 大小的分块。结尾最后给大家推荐一下这个小站吧：0xffff.one这个小站是华南师范大学一位师兄建立的，出于改变学校内计算机科学讨论氛围欠缺的现状，打造一个能专注思考，能碰撞思维，能沉淀思想的交流社区。和站长的认识也是算一个很难得的机会，交流到深夜，感到思维方式与看法相近，有一种很难得的归属感，在深大没曾体验过的兴奋。不知是因为是个人的圈子还是大的环境，总觉得深大的计算机科学也缺少这一种研究、沉淀的氛围，浮躁与急于求成风气比较浓。有效的讨论我认为是加深对学科理解、促进思考与发展的重要因素，在平等的表达与交流中思维碰撞，往往能够产生超出讨论双方意料之外的结果。这样 1 + 1 &gt; 2，双方都能有所收获，有所长进。在这个社区中，每个人都是老师，也都是学生。 是故弟子不必不如师，师不必贤于弟子我想把这个小站也推荐给深大的同学们。能够有更多人参与到研究和讨论中来，并有所收获就是最大的期望。 If you want to go fast, go alone.If you want to go far, go together.0xffff.one" }, { "title": "MongoDB 高性能最佳实践: 事务，读取关心程度与写入关心程度", "url": "/posts/cn-translation-performance-best-practices-transactions-and-read-write-concerns/", "categories": "Backend, Database, MongoDB", "tags": "Chinese, translated", "date": "2021-02-03 21:09:11 +0800", "snippet": " 本文章为翻译作品，原作者为 Mat Keep 与 Henrik Ingo原文章发布日期 2020年02月25日原文标题：Performance Best Practices: Transactions and Read / Write Concerns原文链接：https://www.mongodb.com/blog/post/performance-best-practices-transactions-and-read–write-concerns翻译：Miigon本文为介绍 MongoDB 最佳性能实践的系列推文的第 5 篇。本系列文章中，我们将多维度介绍在大数据量场景下实现高性能的关键技术点，包括： 数据建模与内存分配（工作集） 请求模式与性能分析 索引 数据分片（分布式） 事务，读取关心程度与写入关心程度（本文的主题） 硬件与操作系统配置 基准测试单文档原子性　　在分表式的数据库设计中，互相有关联的数据需要被抽象为分散在多个独立的父-子表中。但在 MongoDB 里，由于文档的存在，这样的数据可以被聚集在一起存储。MongoDB 的单文档操作，提供了足够满足大多数应用的原子性语义。一个操作可以修改一个或多个字段，包括更新多个子文档或数组元素。MongoDB 保证单个文档更新时的完全隔离；任何错误都会使得整个操作回滚，这就保证了用户得到的文档数据总是一致的。多文档 ACID 事务　　从 MongoDB 4.0 开始，加入了多文档 ACID 事务支持，用 MongoDB 实现各种场景下的需求变得更简单了。在 4.0 版本之后，事务的作用域限制为一个副本集 (replica set) 内，在随后的 4.2 版本，多文档事务的支持被拓宽到整个分片集群。MongoDB 的事务功能和关系型数据库的事务功能十分相似 —— 多语句，熟悉的语法，便于集成到任何程序。通过快照隔离，事务功能确保了数据一致性，提供“要么全成功要么全失败”的执行模式，并且对不涉及事务功能的其他操作的性能没有影响。你可以查看我们发布在 VLDB 会议论文上的基准测试结果获取有关事务性能的更多资料。接下来我们将讨论如何在你的项目中更好地使用事务。多文档事务的最佳实践　　创建长耗时的事务，或者在单个 ACID 事务中进行大量操作，会加大 WiredTiger 存储引擎的缓存压力。这是因为自快照创建开始，所有的写操作都需要由缓存来保存和管理状态。由于一个事务自始至终使用同一份快照，事务途中对集合进行的写操作将在缓存中堆积。这些写操作在事务提交/终止之前都没有办法写入数据库，只有在事务结束后，相关的锁才会被释放。为了维持稳定可预测的数据库性能，开发者需要注意以下几点：事务运行时限　　默认地，MongoDB 会自动终止运行超过 60 秒的多文档事务。若服务器写入能力较弱，可以灵活调整事务的运行时间。为解决事务超时问题，过大的事务应该被切分为能够在运行时限内执行完毕的多个小事务。同时为了降低查询语句耗时，确保已经使用合适的索引对查询语句进行了优化。事务中的操作数量　　一个事务中能够读取的文档数量没有硬性限制。但作为一种最佳实践，单个事务一般不应该修改超过 1000 个文档。若存在需要修改超过 1000 个文档的操作，开发者应该将事务切分为多个事务分配处理这些文档。分布式的跨分片 (multi-shard) 事务　　涉及多个数据库分片 (shard) 的事务产生的性能开销更大，因为跨分片的操作需要多个节点通过网络协同进行。“快照”读取关心等级 (“snapshot” read concern) 是在跨分片情景下唯一能够提供一致的数据快照的隔离等级 (isolation level)。当低延迟比跨分片读取一致性更加重要时，应使用默认的local 读取关心等级，该等级将在本地单机的一份快照中执行事务（忽略其他分片节点）。异常处理　　当一个事务终止时，一个异常会被返回给调用者，并且事务会被完全回滚。开发者需要实现捕捉异常并针对临时性异常（如 MVCC 写冲突、暂时性网络错误、主副本选举事件(primary replica election)）进行重试的逻辑。借助可重试写入 (retryable writes) 机制，MongoDB 调用者会对事务的提交指令进行自动重试。使用事务对写入延迟的益处　　虽然第一眼可能没那么显而易见，但使用多文档事务，由于降低了提交延迟，实际上提高了写入性能。使用 w:majority 的写入关心等级 (write concern)，假设分开执行 10 条更新指令，则每一条指令都需要等待一个分片间复制 (replication) 的往返时长。然而，如果同样的 10 条更新指令运行在同一个事务里，它们将在事务提交的时候被一次性复制 (replicated)，从而将延迟降低 10 倍！我还需要知道什么？　　你可以在MongoDB 多文档事务 参考文档里学习所有最佳实践。查阅文档中的生产环境注意事项一栏来了解性能相关的指引。选择合适的写入保证等级　　MongoDB 允许你在向数据库提交写入请求时指定一个可靠性保证等级，称为“写入关心等级” (write concern)注意：写关心等级可以对任何对服务器进行的操作生效，无论该操作是对单个文档的一般操作还是是包含在一个多文档事务中的一部分。以下选项能够在“每连接”、“每数据库”、“每集合”、甚至“每操作”的水平上设置。总共有这些选项： 写入确认 (Write Acknowledged)： 这是默认的写入关心等级。mongod 会保证写操作的执行，使得客户可以捕捉到网络异常、key重复异常、schema验证异常等异常类型。 日志确认 (Journal Acknowledged)： mongod 只有在写操作已经被写入主节点的日志 (journal) 后才会确认写入操作成功。该等级确保写操作能够在 mongod 崩溃后依然有效并且确保能够写入硬盘。 副本确认 (Replica Acknowledged)： 使用本选项能够确保等待副本集 (replica set) 其他成员已经发送写入确认后才视为写入操作成功。MongoDB 支持写入到指定数量的副本 (replica) 中。本选项同时确保写入数据被写入二级数据库的日志中。由于副本可以被部署在数据中心的不同机架甚至不同数据中心，确保写入的数据扩增到额外的副本可以提供极高的可靠性。 多数确认 (Majority)： 本写关心等级将等待写操作被应用到副本集中多数可承载数据 (data-bearing) 且可选举 (electable) 的成员上，因此在遇到主副本选举事件时，写操作将会无法成功执行。这个等级同时确保了写操作能够被记录在各个副本的日志 (journal) 里 —— 包括主副本的。选择合适的读取关心程度　　就像写入关心程度一样，读取关心程度也可以被应用于任何对数据库发起的请求，无论是对单个文档的读取，还是作为多文档事务的一部分。　　为保证隔离度与一致性，写入关心程度可以被设置为 majority (多数确认) ，该等级代表仅当数据已经被覆盖到副本集中大多数的节点时，才能被返回到应用程序。同时确保数据不会因为新主节点的选举而被回滚。　　MongoDB 支持一个“可线性化” (linearizable) 的读取关心等级。可线性化的读取关心等级确保一个节点在读取的时候仍然是副本集的主节点，并且即使后来另外一个节点被选举为新的主节点，其已经返回的数据也保证不会被回滚。使用该读取关心等级可能会对延迟造成显著影响，故需要提供一个 maxTimeMS 值来让运行时间过长的操作超时。仅在必要时使用因果一致性　　因果一致性 (causal consistency) 保证客户端会话 (session) 内的所有读操作都能看到上一次写操作的结果，不管当前请求是由哪一个副本在提供服务。仅当在需要单调读保证 (monotonic read guarantees) 的地方使用因果一致性，能够降低延迟的影响。What’s Next　　这就是本期的高性能最佳实践。本系列的下一篇：硬件与操作系统配置。 译注：没有按照原系列文章顺序翻译。" }, { "title": "Golang Project: Tic Tac Toe", "url": "/posts/golang-project-tic-tac-toe/", "categories": "Execises, Golang", "tags": "golang, project", "date": "2021-01-29 09:23:36 +0800", "snippet": "In this article, I’ll go through my process of writing a simple Tic-Tac-Toe game in Golang. Error handling, closures, iota as well as other golang features are used to create the game. Before we start: After some more research to the language, It seems that Go is really not thatdifferent from order C-family programming languages. So I decided that it wouldnot be worthwhile to document every single detail of Golang in my blog posts, sincethere’s a lot of resources readily available on the internet which go much deeperinto the Go language features than my blog will ever be able to do. So it doesn’t make much sense for me to write about everything, a quick Google search will usuallywork better. As a result, this article series will now be focused on my experiences of making experimentaland/or actual project with Golang. The goal is to show Golang’s features and neuances.Full code on GithubDefine the board and game stateThe first thing we want to do is define the board on which the game will be played on.The game Tic-Tac-Toe has a 3*3 board. Each of the squares can be one of three states:it can be a X or a O, or it can be just empty.We define these states with type alias and const definition, which is Go’s equivalanceof enum:// what kind of piece is on a certain squaretype squareState intconst (\tnone = iota\tcross = iota\tcircle = iota)iota represents successive integer constants 0,1,2,….It resets to 0 whenever the keyword const appears in the source code and increments after each const specification.In our case, none would be 0, cross would be 1 and circle would be 2.Noted that all the iota(except for the first one) can be omitted and still have the same effect.With suqareState we can represent the state of one square, now we can use a 3*3 array to represent the whole board.var board [3][3]squareStateFor each turn, we also need to know whose turn it is. So we introduce another variable to represent that:type player intvar turnPlayer playerWe don’t have to define constants again, the constants defined for the squareState before can be used. This is when we run into the discussion about whether Golang’s decision to not includeC/C++ style enum keyword had made the language difficult to use in some cases. Some have argued that the lack of compile-time type checking makes the code more prone tomistakes. const names being in the same package scope also can cause some confusion.Since it’s not always immediately obvious which enum a const name belongs to when you see one.The last thing for us to do is wrap them (the board &amp; whose turn is it) up as the current game state struct:// current state of the gametype gameState struct {\tboard [3][3]squareState\tturnPlayer player}Storing them as saparate global variables will work as well, but using struct is usually a betterpractice.It makes the code easier to understand. And also enables us to do some other cool things.(eg. an “undo” feature, which we will talk about later)Draw the boardNext, we need a way to show our board on the screen. We use fmt to draw the board to the terminal.The code is pretty standard:// define a method for struct type `gameState`func (state *gameState) drawBoard() {\tfor i, row := range state.board {\t\tfor j, square := range row {\t\t\tfmt.Print(\" \")\t\t\tswitch square {\t\t\tcase none:\t\t\t\tfmt.Print(\" \")\t\t\tcase cross:\t\t\t\tfmt.Print(\"X\")\t\t\tcase circle:\t\t\t\tfmt.Print(\"O\")\t\t\t}\t\t\tif j != len(row)-1 {\t\t\t\tfmt.Print(\" |\")\t\t\t}\t\t}\t\tif i != len(state.board)-1 {\t\t\tfmt.Print(\"\\n------------\")\t\t}\t\tfmt.Print(\"\\n\")\t}}(Notice how break is not required in switch cases in Go.)Now we test our drawBoard function with a main function like this:func main() {\tstate := gameState{}\tstate.board[0][1] = cross\tstate.board[1][1] = circle\tstate.drawBoard()}which yields:$ go run main.go | X | ------------ | O | ------------ | | Hooray! It works.Game logicNow it’s time for the game logic.The rule of tic-tac-toe is really simple. The whole game can be discribed as:for {\tdraw_the_board()\trow, column := enter_position_to_place_mark()\tplace_mark(row, column)\tif any_one_has_won_the_game() == true {\t\tbreak\t}\tnext_turn()}Placing markSo far we can already draw the board, but we don’t really have a proper way to place a mark on a square.When we are testing our drawBoard function, we modified the board field of gameState directly, but that’s not good enough for our game logic. We need it to be able to do a little bit more, eg. checking if a square has already had a mark on it.Therefore we write a function to do just that:// define error typestype markAlreadyExistError struct {\trow int\tcolumn int}type positionOutOfBoundError struct {\trow int\tcolumn int}// implement Error()func (e *markAlreadyExistError) Error() string {\treturn fmt.Sprintf(\"position (%d,%d) already has a mark on it.\", e.row, e.column)}func (e *positionOutOfBoundError) Error() string {\treturn fmt.Sprintf(\"position (%d,%d) is out of bound.\", e.row, e.column)}// place a mark at a certain positionfunc (state *gameState) placeMark(row int, column int) error {\tif row &lt; 0 || column &lt; 0 || row &gt;= len(state.board) || column &gt;= len(state.board[row]) {\t\treturn &amp;positionOutOfBoundError{row, column}\t}\tif state.board[row][column] != none {\t\treturn &amp;markAlreadyExistError{row, column}\t}\tstate.board[row][column] = squareState(state.turnPlayer) // the actual \"placing\"\treturn nil // no error}You can see that aside from error handling, the code above really does nothing more than changing state.board[row][column].However, in real projects, as the project grow more and more complex, instead of directly setting the valueeverywhere, using a function to do it allows for more flexibility and would pay off in the long run.The code above defined two new error types markAlreadyExistError and positionOutOfBoundError.For them to be considered an error type, their respective Error() function must be implemented.That’s called composition (compared to inheritance).Switching turnNow we can place a mark on a square, what do we need next?Well, the game has 2 players and they take turns to place marks. So after a mark is placed, we would like to switch whose mark will be placed on the next placeMark()We do that by adding a nextTurn() function, which sets state.turnPlayer to the other player.type gameResult intconst (\tnoWinnerYet = iota\tcrossWon\tcircleWon\tdraw)func (state *gameState) whosNext() player {\treturn state.turnPlayer}func (state *gameState) nextTurn() {\tif state.turnPlayer == cross {\t\tstate.turnPlayer = circle\t} else {\t\tstate.turnPlayer = cross\t}}whosNext() and nextTurn() are pretty straightforward and does exactly what it says to do.Checking for winnerSo for each and every single turn, we need to check if anyone has won the game by placing a mark.This is when we get into a more interesting part of this project.The naive approachYou might be tempted to do something like this:func (state *gameState) checkForWinner() gameResult {\t// check vertical\tif state.board[0][0] == state.board[0][1] &amp;&amp;\t\tstate.board[0][1] == state.board[0][2] &amp;&amp;\t\tstate.board[0][2] != none {\t\treturn gameResult(state.board[0][0])\t}\tif state.board[1][0] == state.board[1][1] &amp;&amp;\t\tstate.board[1][1] == state.board[1][2] &amp;&amp;\t\tstate.board[1][2] != none {\t\treturn gameResult(state.board[1][0])\t}\tif state.board[2][0] == state.board[2][1] &amp;&amp;\t\tstate.board[2][1] == state.board[2][2] &amp;&amp;\t\tstate.board[2][2] != none {\t\treturn gameResult(state.board[2][0])\t}\t// check horizontal\tif state.board[0][0] == state.board[1][0] &amp;&amp;\t\tstate.board[1][0] == state.board[2][0] &amp;&amp;\t\tstate.board[2][0] != none {\t\treturn gameResult(state.board[0][0])\t}\tif state.board[0][1] == state.board[1][1] &amp;&amp;\t\tstate.board[1][1] == state.board[2][1] &amp;&amp;\t\tstate.board[2][1] != none {\t\treturn gameResult(state.board[0][1])\t}\tif state.board[0][2] == state.board[1][2] &amp;&amp;\t\tstate.board[1][2] == state.board[2][2] &amp;&amp;\t\tstate.board[2][2] != none {\t\treturn gameResult(state.board[0][2])\t}\t// check diagonal\t// ...\t\treturn noWinnerYet}This DOES work, but it’s a naive way of doing it. It’s long, verbose, and hard to modify.The board can also only be 3x3 and can not be expanded easily.Using for-loopsA way better solution would be to use loops:func (state *gameState) checkForWinner() gameResult {CheckHorizontal:\tfor _, row := range state.board {\t\tvar lastSquare squareState = row[0]\t\tfor _, square := range row {\t\t\tif square != lastSquare {\t\t\t\tcontinue CheckHorizontal // continue with label, affects the outer loop instead of the inner one\t\t\t}\t\t\tlastSquare = square\t\t}\t\tif lastSquare == cross {\t\t\treturn crossWon\t\t} else if lastSquare == circle {\t\t\treturn circleWon\t\t}\t}\t// check for verticals and diagonals...\treturn noWinnerYet}// (the complete code will be about 4 times as long as the code shown)By doing it like this, we can handle board of any dimension. But due to the way the board is stored(board[row][column]), checking vertical lines using nested for-loops isn’t as intuitive as checking horizontal lines. And it gets even trickier when it comes to checking for diagonal lines.Also, the function is still repetitive since the actual “checking” part inside each loop:\t\tif lastSquare == cross {\t\t\treturn crossWon\t\t} else if lastSquare == circle {\t\t\treturn circleWon\t\t}are the same.A better approachIn order to know if anyone has won the game, we have to check for 3 horizontal lines, 3 vertical lines and 2 diagonal lines. We can think of the process of checking each line like this: for each iteration, check if next square(x + a, y + b) has the same mark as the current square(x, y) set current square position to (x + a, y + b) repeat the process until different marks between iteration was found or a border was hit.This is a generalized description of all the for-loops we discussed before. By using different delta (a, b), we can control how we move between different iterations. So the same code can be used to check for horizontals, verticals\tand diagonals.This is the implementation:First we define a lambda function for checking one line (can be either horizontal, vertical or diagonal):checkLine := func(startRow int, startColumn int, deltaRow int, deltaColumn int) gameResult {\tvar lastSquare squareState = state.board[startRow][startColumn]\trow, column := startRow+deltaRow, startColumn+deltaColumn\t// loop starts from the second square(startRow + deltaRow, startColumn + deltaColumn)\tfor row &gt;= 0 &amp;&amp; column &gt;= 0 &amp;&amp; row &lt; boardSize &amp;&amp; column &lt; boardSize {\t\t// there can't be a winner if a empty square is present within the line\t\tif state.board[row][column] == none {\t\t\treturn noWinnerYet\t\t}\t\tif lastSquare != state.board[row][column] {\t\t\treturn noWinnerYet\t\t}\t\tlastSquare = state.board[row][column]\t\trow, column = row+deltaRow, column+deltaColumn\t}\t// someone has won the game\tif lastSquare == cross {\t\treturn crossWon\t} else if lastSquare == circle {\t\treturn circleWon\t}\treturn noWinnerYet}Then we put it inside our checkForWinner() function, alongside some other code to utilize it.func (state *gameState) checkForWinner() gameResult {\tboardSize := len(state.board) // assuming the board is always square-shaped.\t// define a lambda function for checking one single line\tcheckLine := func(startRow int, startColumn int, deltaRow int, deltaColumn int) gameResult {\t\t// ...\t}\t// check horizontal rows\tfor row := 0; row &lt; boardSize; row++ {\t\tif result := checkLine(row, 0, 0, 1); result != noWinnerYet {\t\t\treturn result\t\t}\t}\t// check vertical columns\tfor column := 0; column &lt; boardSize; column++ {\t\tif result := checkLine(column, 0, 0, 1); result != noWinnerYet {\t\t\treturn result\t\t}\t}\t// check top-left to bottom-right diagonal\tif result := checkLine(0, 0, 1, 1); result != noWinnerYet {\t\treturn result\t}\t// check top-right to bottom-left diagonal\tif result := checkLine(0, boardSize-1, 1, -1); result != noWinnerYet {\t\treturn result\t}\t// check for draw\tfor _, row := range state.board {\t\tfor _, square := range row {\t\t\tif square == none {\t\t\t\treturn noWinnerYet\t\t\t}\t\t}\t}\t// if no one wins yet, but none of the squares are empty\treturn draw}The code above uses the same checkLine() routine to check for horizontals, verticals and diagonals.Putting everything togetherNow that we can draw the board, place a mark, switch turns and check for potiential winners, it’s time to put everything together.func (e player) String() string {\tswitch e {\tcase none:\t\treturn \"none\"\tcase cross:\t\treturn \"cross\"\tcase circle:\t\treturn \"circle\"\tdefault:\t\treturn fmt.Sprintf(\"%d\", int(e))\t}}func main() {\tstate := gameState{}\tstate.turnPlayer = cross // cross goes first\tvar result gameResult = noWinnerYet\t// the main game loop\tfor {\t\tfmt.Printf(\"\\nnext player to place a mark is: %v\\n\", state.whosNext())\t\t// 1. draw the board onto the screen\t\tstate.drawBoard()\t\tfmt.Printf(\"where to place a %v? (input row then column, separated by space)\\n&gt; \", state.whosNext())\t\t// 2. use a loop to take input\t\tfor {\t\t\tvar row, column int\t\t\tfmt.Scan(&amp;row, &amp;column)\t\t\te := state.placeMark(row-1, column-1) // -1 so coordinate starts at (1,1) instead of (0,0)\t\t\t// if a valid position was entered, break out from the input loop\t\t\tif e == nil {\t\t\t\tbreak\t\t\t}\t\t\t// if an invalid position was entered, prompt the player to re-enter another position\t\t\tfmt.Println(e)\t\t\tfmt.Printf(\"please re-enter a position:\\n&gt; \")\t\t}\t\t// 3. check if anyone has won the game\t\tresult = state.checkForWinner()\t\tif result != noWinnerYet {\t\t\tbreak\t\t}\t\t// 4. if no one has won in this turn, go on for next turn and continue the game loop\t\tstate.nextTurn()\t\tfmt.Println()\t}\tstate.drawBoard()\tswitch result {\tcase crossWon:\t\tfmt.Printf(\"cross won the game!\\n\")\tcase circleWon:\t\tfmt.Printf(\"circle won the game!\\n\")\tcase draw:\t\tfmt.Printf(\"the game has ended with a draw!\\n\")\t}}Noted that we defined method String() for player type at the beginning.This is done for the following line of code to work.fmt.Printf(\"\\nnext player to place a mark is: %v\\n\", state.whosNext())Defining this method makes type player a Stringer, meaning something that has a String() method and can be converted into a string.In this case, fmt.Printf uses the method internally to convert a player into a string, then prints it out.Testing the gameNow we compile and run the game:next player to place a mark is: cross | | ------------ | | ------------ | | where to place a cross? (input row then column, separated by space)&gt; 1 1next player to place a mark is: circle X | | ------------ | | ------------ | | where to place a circle? (input row then column, separated by space)&gt; 1 2...next player to place a mark is: cross X | O | X------------ O | O | X------------ | X | Owhere to place a cross? (input row then column, separated by space)&gt; 3 1 X | O | X------------ O | O | X------------ X | X | Othe game has ended with a draw!From another run:...next player to place a mark is: circle X | X | O------------ X | O | ------------ | | where to place a circle? (input row then column, separated by space)&gt; 3 1 X | X | O------------ X | O | ------------ O | | circle won the game!There you have it, a fully working tic-tac-toe in Go." }, { "title": "Announcement | 公告", "url": "/posts/first-announcement/", "categories": "Announcement, Uncategorized", "tags": "announcement", "date": "2020-12-19 16:25:00 +0800", "snippet": "What?Here’s my personal blog on which I may post stuffs like what I am learning recently, what I found fascinating and want to talk about, or just random thoughts.Why?I have been learning programming for quite some time, but I havn’t really done that much summarizing yet.That’s why I decided to start this blog.I’d like to write down everything I learned, so it’s easy for me to pickthem up later if I forgot them at some point and need to do so. I will also postsome technical problems and difficulties I encountered and try to explain my solution andmy thought process.What’s the plan?I still havn’t fully figured out what I might want to post. But most likely, it wouldbe among these categories: Data structures, Algorithms Computer Networking Object-oriented Programming and Design Patterns Operating Systems New Programming Languages and Features Tools, Techniques and TricksInspirations will most likely come from: My courses and appointments My personal projects and projects with my friends My own understanding of a bookI will not be covering the front-end side of thing, since that’s not the path I will be taking.But anything can change in the future, so who knows?I will try to keep the blog all-English to open my contents to a broader audience,and improve my English writing skills at the same time.DisclaimerThis is my personal blog, which means most of the time the articles will certainly not beup to professional standards and may even be straight up inaccurate. The main purpose ofthis blog is for me to document my learning process, the main target reader is myself.For any consequence caused by the use of my blog content, I will not bear anylegal responsibilities.If you end up losing your job or accidentally deleting your company’s databaseafter following my blog, well, that’s your fault for believing some random guy on theinternet when doing something so crucial. 😊" } ]
